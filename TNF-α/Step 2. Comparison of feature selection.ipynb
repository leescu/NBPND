{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50559d19-ab40-4880-bae7-44a3dbe180a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Others\n",
    "import random\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "import optuna\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70ff5d38-60c9-476d-a509-9e1419bea2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing packages\n",
    "#Sklearn\n",
    "from sklearn import model_selection, linear_model\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.feature_selection import RFECV,SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV,RepeatedStratifiedKFold,cross_validate\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,auc,roc_auc_score,roc_curve,classification_report\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Ridge, LinearRegression, Lasso\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6baca9e-4a5b-4e5f-9cd3-2db48c543dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc75945b-ce4f-4650-8380-f7bfb5ff1b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:/Users/A/Desktop/Bioactive/TNF-α\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12865361-291e-449c-8ca4-53ed95b92f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6175, 3228)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MW</th>\n",
       "      <th>AMW</th>\n",
       "      <th>Sp</th>\n",
       "      <th>Si</th>\n",
       "      <th>Mp</th>\n",
       "      <th>Mi</th>\n",
       "      <th>GD</th>\n",
       "      <th>nAT</th>\n",
       "      <th>nSK</th>\n",
       "      <th>nAA</th>\n",
       "      <th>...</th>\n",
       "      <th>s1_numAroBonds</th>\n",
       "      <th>s2_numAroBonds</th>\n",
       "      <th>s3_numAroBonds</th>\n",
       "      <th>s4_numAroBonds</th>\n",
       "      <th>s34_size</th>\n",
       "      <th>s34_relSize</th>\n",
       "      <th>s34_phSize</th>\n",
       "      <th>s34_phRelSize</th>\n",
       "      <th>chiralMoment</th>\n",
       "      <th>chiralPhMoment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2083077.0</th>\n",
       "      <td>425.57</td>\n",
       "      <td>8.866042</td>\n",
       "      <td>34.7672</td>\n",
       "      <td>53.2847</td>\n",
       "      <td>0.724317</td>\n",
       "      <td>1.110098</td>\n",
       "      <td>0.078818</td>\n",
       "      <td>48.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138703696.0</th>\n",
       "      <td>526.58</td>\n",
       "      <td>8.493226</td>\n",
       "      <td>42.4378</td>\n",
       "      <td>70.2759</td>\n",
       "      <td>0.684481</td>\n",
       "      <td>1.133482</td>\n",
       "      <td>0.058030</td>\n",
       "      <td>62.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>54.589376</td>\n",
       "      <td>16.822604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139316163.0</th>\n",
       "      <td>585.65</td>\n",
       "      <td>8.134028</td>\n",
       "      <td>46.5686</td>\n",
       "      <td>82.7424</td>\n",
       "      <td>0.646786</td>\n",
       "      <td>1.149200</td>\n",
       "      <td>0.053426</td>\n",
       "      <td>72.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>59.757845</td>\n",
       "      <td>16.822604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139315919.0</th>\n",
       "      <td>541.62</td>\n",
       "      <td>7.965000</td>\n",
       "      <td>43.8697</td>\n",
       "      <td>77.9875</td>\n",
       "      <td>0.645143</td>\n",
       "      <td>1.146875</td>\n",
       "      <td>0.056680</td>\n",
       "      <td>68.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>54.589376</td>\n",
       "      <td>16.822604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138703798.0</th>\n",
       "      <td>491.63</td>\n",
       "      <td>7.448939</td>\n",
       "      <td>43.0232</td>\n",
       "      <td>75.2290</td>\n",
       "      <td>0.651867</td>\n",
       "      <td>1.139833</td>\n",
       "      <td>0.063492</td>\n",
       "      <td>66.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>0.231481</td>\n",
       "      <td>49.318632</td>\n",
       "      <td>13.379088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3228 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 MW       AMW       Sp       Si        Mp        Mi        GD  \\\n",
       "cid                                                                             \n",
       "2083077.0    425.57  8.866042  34.7672  53.2847  0.724317  1.110098  0.078818   \n",
       "138703696.0  526.58  8.493226  42.4378  70.2759  0.684481  1.133482  0.058030   \n",
       "139316163.0  585.65  8.134028  46.5686  82.7424  0.646786  1.149200  0.053426   \n",
       "139315919.0  541.62  7.965000  43.8697  77.9875  0.645143  1.146875  0.056680   \n",
       "138703798.0  491.63  7.448939  43.0232  75.2290  0.651867  1.139833  0.063492   \n",
       "\n",
       "              nAT   nSK   nAA  ...  s1_numAroBonds  s2_numAroBonds  \\\n",
       "cid                            ...                                   \n",
       "2083077.0    48.0  29.0  21.0  ...             0.0             0.0   \n",
       "138703696.0  62.0  39.0  26.0  ...             0.0             0.0   \n",
       "139316163.0  72.0  42.0  15.0  ...             0.0             0.0   \n",
       "139315919.0  68.0  39.0  15.0  ...             0.0             0.0   \n",
       "138703798.0  66.0  36.0  15.0  ...             0.0             0.0   \n",
       "\n",
       "             s3_numAroBonds  s4_numAroBonds   s34_size  s34_relSize  \\\n",
       "cid                                                                   \n",
       "2083077.0               0.0             0.0   0.000000     0.000000   \n",
       "138703696.0             0.0            27.0  37.000000     0.948718   \n",
       "139316163.0             0.0            16.0  40.000000     0.952381   \n",
       "139315919.0             0.0            16.0  37.000000     0.948718   \n",
       "138703798.0             0.0            16.0  33.333333     0.925926   \n",
       "\n",
       "             s34_phSize  s34_phRelSize  chiralMoment  chiralPhMoment  \n",
       "cid                                                                   \n",
       "2083077.0      0.000000       0.000000      0.000000        0.000000  \n",
       "138703696.0   11.000000       0.282051     54.589376       16.822604  \n",
       "139316163.0   11.000000       0.261905     59.757845       16.822604  \n",
       "139315919.0   11.000000       0.282051     54.589376       16.822604  \n",
       "138703798.0    8.333333       0.231481     49.318632       13.379088  \n",
       "\n",
       "[5 rows x 3228 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading the data\n",
    "ML_data= pd.read_csv(\"./ML_data.csv\",header=0,index_col=0)\n",
    "X_NAomit_data= pd.read_csv(\"./X_NAomit_data.csv\",header=0,index_col=0)\n",
    "Raw_data = pd.read_csv('./Original_data.csv',index_col=0)\n",
    "\n",
    "#original data(descriptors= 4175）\n",
    "print(X_NAomit_data.shape)\n",
    "X_NAomit_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a68cc910-dca6-4b63-ae90-8fac100e1ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MW</th>\n",
       "      <th>AMW</th>\n",
       "      <th>Sp</th>\n",
       "      <th>Si</th>\n",
       "      <th>Mp</th>\n",
       "      <th>Mi</th>\n",
       "      <th>GD</th>\n",
       "      <th>nAT</th>\n",
       "      <th>nSK</th>\n",
       "      <th>nAA</th>\n",
       "      <th>...</th>\n",
       "      <th>s1_numAroBonds</th>\n",
       "      <th>s2_numAroBonds</th>\n",
       "      <th>s3_numAroBonds</th>\n",
       "      <th>s4_numAroBonds</th>\n",
       "      <th>s34_size</th>\n",
       "      <th>s34_relSize</th>\n",
       "      <th>s34_phSize</th>\n",
       "      <th>s34_phRelSize</th>\n",
       "      <th>chiralMoment</th>\n",
       "      <th>chiralPhMoment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2083077.0</th>\n",
       "      <td>425.57</td>\n",
       "      <td>8.866042</td>\n",
       "      <td>34.7672</td>\n",
       "      <td>53.2847</td>\n",
       "      <td>0.724317</td>\n",
       "      <td>1.110098</td>\n",
       "      <td>0.078818</td>\n",
       "      <td>48.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138703696.0</th>\n",
       "      <td>526.58</td>\n",
       "      <td>8.493226</td>\n",
       "      <td>42.4378</td>\n",
       "      <td>70.2759</td>\n",
       "      <td>0.684481</td>\n",
       "      <td>1.133482</td>\n",
       "      <td>0.058030</td>\n",
       "      <td>62.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>27.00</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>54.589376</td>\n",
       "      <td>16.822604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139316163.0</th>\n",
       "      <td>585.65</td>\n",
       "      <td>8.134028</td>\n",
       "      <td>46.5686</td>\n",
       "      <td>82.7424</td>\n",
       "      <td>0.646786</td>\n",
       "      <td>1.149200</td>\n",
       "      <td>0.053426</td>\n",
       "      <td>72.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>59.757845</td>\n",
       "      <td>16.822604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139315919.0</th>\n",
       "      <td>541.62</td>\n",
       "      <td>7.965000</td>\n",
       "      <td>43.8697</td>\n",
       "      <td>77.9875</td>\n",
       "      <td>0.645143</td>\n",
       "      <td>1.146875</td>\n",
       "      <td>0.056680</td>\n",
       "      <td>68.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>54.589376</td>\n",
       "      <td>16.822604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138703798.0</th>\n",
       "      <td>491.63</td>\n",
       "      <td>7.448939</td>\n",
       "      <td>43.0232</td>\n",
       "      <td>75.2290</td>\n",
       "      <td>0.651867</td>\n",
       "      <td>1.139833</td>\n",
       "      <td>0.063492</td>\n",
       "      <td>66.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>0.231481</td>\n",
       "      <td>49.318632</td>\n",
       "      <td>13.379088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158020.0</th>\n",
       "      <td>462.06</td>\n",
       "      <td>7.831525</td>\n",
       "      <td>39.7844</td>\n",
       "      <td>66.3847</td>\n",
       "      <td>0.674312</td>\n",
       "      <td>1.125164</td>\n",
       "      <td>0.070968</td>\n",
       "      <td>59.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9631088.0</th>\n",
       "      <td>246.31</td>\n",
       "      <td>9.122593</td>\n",
       "      <td>19.6137</td>\n",
       "      <td>29.9962</td>\n",
       "      <td>0.726433</td>\n",
       "      <td>1.110970</td>\n",
       "      <td>0.132353</td>\n",
       "      <td>27.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3247362.0</th>\n",
       "      <td>463.62</td>\n",
       "      <td>7.244062</td>\n",
       "      <td>43.8243</td>\n",
       "      <td>71.3113</td>\n",
       "      <td>0.684755</td>\n",
       "      <td>1.114239</td>\n",
       "      <td>0.065546</td>\n",
       "      <td>64.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>9.75</td>\n",
       "      <td>11.75</td>\n",
       "      <td>31.500000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>30.833138</td>\n",
       "      <td>5.196152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5428531.0</th>\n",
       "      <td>282.31</td>\n",
       "      <td>8.066000</td>\n",
       "      <td>24.1478</td>\n",
       "      <td>38.7440</td>\n",
       "      <td>0.689937</td>\n",
       "      <td>1.106971</td>\n",
       "      <td>0.109524</td>\n",
       "      <td>35.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138502092.0</th>\n",
       "      <td>477.67</td>\n",
       "      <td>8.096102</td>\n",
       "      <td>39.5173</td>\n",
       "      <td>66.7363</td>\n",
       "      <td>0.669785</td>\n",
       "      <td>1.131124</td>\n",
       "      <td>0.072581</td>\n",
       "      <td>59.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6175 rows × 3228 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 MW       AMW       Sp       Si        Mp        Mi        GD  \\\n",
       "cid                                                                             \n",
       "2083077.0    425.57  8.866042  34.7672  53.2847  0.724317  1.110098  0.078818   \n",
       "138703696.0  526.58  8.493226  42.4378  70.2759  0.684481  1.133482  0.058030   \n",
       "139316163.0  585.65  8.134028  46.5686  82.7424  0.646786  1.149200  0.053426   \n",
       "139315919.0  541.62  7.965000  43.8697  77.9875  0.645143  1.146875  0.056680   \n",
       "138703798.0  491.63  7.448939  43.0232  75.2290  0.651867  1.139833  0.063492   \n",
       "...             ...       ...      ...      ...       ...       ...       ...   \n",
       "2158020.0    462.06  7.831525  39.7844  66.3847  0.674312  1.125164  0.070968   \n",
       "9631088.0    246.31  9.122593  19.6137  29.9962  0.726433  1.110970  0.132353   \n",
       "3247362.0    463.62  7.244062  43.8243  71.3113  0.684755  1.114239  0.065546   \n",
       "5428531.0    282.31  8.066000  24.1478  38.7440  0.689937  1.106971  0.109524   \n",
       "138502092.0  477.67  8.096102  39.5173  66.7363  0.669785  1.131124  0.072581   \n",
       "\n",
       "              nAT   nSK   nAA  ...  s1_numAroBonds  s2_numAroBonds  \\\n",
       "cid                            ...                                   \n",
       "2083077.0    48.0  29.0  21.0  ...             0.0             0.0   \n",
       "138703696.0  62.0  39.0  26.0  ...             0.0             0.0   \n",
       "139316163.0  72.0  42.0  15.0  ...             0.0             0.0   \n",
       "139315919.0  68.0  39.0  15.0  ...             0.0             0.0   \n",
       "138703798.0  66.0  36.0  15.0  ...             0.0             0.0   \n",
       "...           ...   ...   ...  ...             ...             ...   \n",
       "2158020.0    59.0  31.0  15.0  ...             0.0             0.0   \n",
       "9631088.0    27.0  17.0  11.0  ...             0.0             0.0   \n",
       "3247362.0    64.0  35.0  22.0  ...             0.0             1.5   \n",
       "5428531.0    35.0  21.0  12.0  ...             0.0             0.0   \n",
       "138502092.0  59.0  32.0  11.0  ...             0.0             0.0   \n",
       "\n",
       "             s3_numAroBonds  s4_numAroBonds   s34_size  s34_relSize  \\\n",
       "cid                                                                   \n",
       "2083077.0              0.00            0.00   0.000000     0.000000   \n",
       "138703696.0            0.00           27.00  37.000000     0.948718   \n",
       "139316163.0            0.00           16.00  40.000000     0.952381   \n",
       "139315919.0            0.00           16.00  37.000000     0.948718   \n",
       "138703798.0            0.00           16.00  33.333333     0.925926   \n",
       "...                     ...             ...        ...          ...   \n",
       "2158020.0              0.00            0.00   0.000000     0.000000   \n",
       "9631088.0              0.00            0.00   0.000000     0.000000   \n",
       "3247362.0              9.75           11.75  31.500000     0.900000   \n",
       "5428531.0              0.00            0.00   0.000000     0.000000   \n",
       "138502092.0            0.00            0.00   0.000000     0.000000   \n",
       "\n",
       "             s34_phSize  s34_phRelSize  chiralMoment  chiralPhMoment  \n",
       "cid                                                                   \n",
       "2083077.0      0.000000       0.000000      0.000000        0.000000  \n",
       "138703696.0   11.000000       0.282051     54.589376       16.822604  \n",
       "139316163.0   11.000000       0.261905     59.757845       16.822604  \n",
       "139315919.0   11.000000       0.282051     54.589376       16.822604  \n",
       "138703798.0    8.333333       0.231481     49.318632       13.379088  \n",
       "...                 ...            ...           ...             ...  \n",
       "2158020.0      0.000000       0.000000      0.000000        0.000000  \n",
       "9631088.0      0.000000       0.000000      0.000000        0.000000  \n",
       "3247362.0      5.000000       0.142857     30.833138        5.196152  \n",
       "5428531.0      0.000000       0.000000      0.000000        0.000000  \n",
       "138502092.0    0.000000       0.000000      0.000000        0.000000  \n",
       "\n",
       "[6175 rows x 3228 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_NAomit_data= pd.read_csv(\"./X_NAomit_data.csv\",header=0,index_col=0)\n",
    "X_NAomit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1c89e32-9d35-4229-a7fd-6dc4bc6e3b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_data= pd.read_csv(\"./X_NAomit_data.csv\",header=0,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c075c2f5-c839-4ab6-8f1d-ee4e808c3238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MW</th>\n",
       "      <th>AMW</th>\n",
       "      <th>Sp</th>\n",
       "      <th>Si</th>\n",
       "      <th>Mp</th>\n",
       "      <th>Mi</th>\n",
       "      <th>GD</th>\n",
       "      <th>nAT</th>\n",
       "      <th>nSK</th>\n",
       "      <th>nAA</th>\n",
       "      <th>...</th>\n",
       "      <th>s1_numAroBonds</th>\n",
       "      <th>s2_numAroBonds</th>\n",
       "      <th>s3_numAroBonds</th>\n",
       "      <th>s4_numAroBonds</th>\n",
       "      <th>s34_size</th>\n",
       "      <th>s34_relSize</th>\n",
       "      <th>s34_phSize</th>\n",
       "      <th>s34_phRelSize</th>\n",
       "      <th>chiralMoment</th>\n",
       "      <th>chiralPhMoment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2083077.0</th>\n",
       "      <td>425.57</td>\n",
       "      <td>8.866042</td>\n",
       "      <td>34.7672</td>\n",
       "      <td>53.2847</td>\n",
       "      <td>0.724317</td>\n",
       "      <td>1.110098</td>\n",
       "      <td>0.078818</td>\n",
       "      <td>48.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138703696.0</th>\n",
       "      <td>526.58</td>\n",
       "      <td>8.493226</td>\n",
       "      <td>42.4378</td>\n",
       "      <td>70.2759</td>\n",
       "      <td>0.684481</td>\n",
       "      <td>1.133482</td>\n",
       "      <td>0.058030</td>\n",
       "      <td>62.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>27.00</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>54.589376</td>\n",
       "      <td>16.822604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139316163.0</th>\n",
       "      <td>585.65</td>\n",
       "      <td>8.134028</td>\n",
       "      <td>46.5686</td>\n",
       "      <td>82.7424</td>\n",
       "      <td>0.646786</td>\n",
       "      <td>1.149200</td>\n",
       "      <td>0.053426</td>\n",
       "      <td>72.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>59.757845</td>\n",
       "      <td>16.822604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139315919.0</th>\n",
       "      <td>541.62</td>\n",
       "      <td>7.965000</td>\n",
       "      <td>43.8697</td>\n",
       "      <td>77.9875</td>\n",
       "      <td>0.645143</td>\n",
       "      <td>1.146875</td>\n",
       "      <td>0.056680</td>\n",
       "      <td>68.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>54.589376</td>\n",
       "      <td>16.822604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138703798.0</th>\n",
       "      <td>491.63</td>\n",
       "      <td>7.448939</td>\n",
       "      <td>43.0232</td>\n",
       "      <td>75.2290</td>\n",
       "      <td>0.651867</td>\n",
       "      <td>1.139833</td>\n",
       "      <td>0.063492</td>\n",
       "      <td>66.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>0.231481</td>\n",
       "      <td>49.318632</td>\n",
       "      <td>13.379088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158020.0</th>\n",
       "      <td>462.06</td>\n",
       "      <td>7.831525</td>\n",
       "      <td>39.7844</td>\n",
       "      <td>66.3847</td>\n",
       "      <td>0.674312</td>\n",
       "      <td>1.125164</td>\n",
       "      <td>0.070968</td>\n",
       "      <td>59.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9631088.0</th>\n",
       "      <td>246.31</td>\n",
       "      <td>9.122593</td>\n",
       "      <td>19.6137</td>\n",
       "      <td>29.9962</td>\n",
       "      <td>0.726433</td>\n",
       "      <td>1.110970</td>\n",
       "      <td>0.132353</td>\n",
       "      <td>27.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3247362.0</th>\n",
       "      <td>463.62</td>\n",
       "      <td>7.244062</td>\n",
       "      <td>43.8243</td>\n",
       "      <td>71.3113</td>\n",
       "      <td>0.684755</td>\n",
       "      <td>1.114239</td>\n",
       "      <td>0.065546</td>\n",
       "      <td>64.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>9.75</td>\n",
       "      <td>11.75</td>\n",
       "      <td>31.500000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>30.833138</td>\n",
       "      <td>5.196152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5428531.0</th>\n",
       "      <td>282.31</td>\n",
       "      <td>8.066000</td>\n",
       "      <td>24.1478</td>\n",
       "      <td>38.7440</td>\n",
       "      <td>0.689937</td>\n",
       "      <td>1.106971</td>\n",
       "      <td>0.109524</td>\n",
       "      <td>35.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138502092.0</th>\n",
       "      <td>477.67</td>\n",
       "      <td>8.096102</td>\n",
       "      <td>39.5173</td>\n",
       "      <td>66.7363</td>\n",
       "      <td>0.669785</td>\n",
       "      <td>1.131124</td>\n",
       "      <td>0.072581</td>\n",
       "      <td>59.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6175 rows × 3228 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 MW       AMW       Sp       Si        Mp        Mi        GD  \\\n",
       "cid                                                                             \n",
       "2083077.0    425.57  8.866042  34.7672  53.2847  0.724317  1.110098  0.078818   \n",
       "138703696.0  526.58  8.493226  42.4378  70.2759  0.684481  1.133482  0.058030   \n",
       "139316163.0  585.65  8.134028  46.5686  82.7424  0.646786  1.149200  0.053426   \n",
       "139315919.0  541.62  7.965000  43.8697  77.9875  0.645143  1.146875  0.056680   \n",
       "138703798.0  491.63  7.448939  43.0232  75.2290  0.651867  1.139833  0.063492   \n",
       "...             ...       ...      ...      ...       ...       ...       ...   \n",
       "2158020.0    462.06  7.831525  39.7844  66.3847  0.674312  1.125164  0.070968   \n",
       "9631088.0    246.31  9.122593  19.6137  29.9962  0.726433  1.110970  0.132353   \n",
       "3247362.0    463.62  7.244062  43.8243  71.3113  0.684755  1.114239  0.065546   \n",
       "5428531.0    282.31  8.066000  24.1478  38.7440  0.689937  1.106971  0.109524   \n",
       "138502092.0  477.67  8.096102  39.5173  66.7363  0.669785  1.131124  0.072581   \n",
       "\n",
       "              nAT   nSK   nAA  ...  s1_numAroBonds  s2_numAroBonds  \\\n",
       "cid                            ...                                   \n",
       "2083077.0    48.0  29.0  21.0  ...             0.0             0.0   \n",
       "138703696.0  62.0  39.0  26.0  ...             0.0             0.0   \n",
       "139316163.0  72.0  42.0  15.0  ...             0.0             0.0   \n",
       "139315919.0  68.0  39.0  15.0  ...             0.0             0.0   \n",
       "138703798.0  66.0  36.0  15.0  ...             0.0             0.0   \n",
       "...           ...   ...   ...  ...             ...             ...   \n",
       "2158020.0    59.0  31.0  15.0  ...             0.0             0.0   \n",
       "9631088.0    27.0  17.0  11.0  ...             0.0             0.0   \n",
       "3247362.0    64.0  35.0  22.0  ...             0.0             1.5   \n",
       "5428531.0    35.0  21.0  12.0  ...             0.0             0.0   \n",
       "138502092.0  59.0  32.0  11.0  ...             0.0             0.0   \n",
       "\n",
       "             s3_numAroBonds  s4_numAroBonds   s34_size  s34_relSize  \\\n",
       "cid                                                                   \n",
       "2083077.0              0.00            0.00   0.000000     0.000000   \n",
       "138703696.0            0.00           27.00  37.000000     0.948718   \n",
       "139316163.0            0.00           16.00  40.000000     0.952381   \n",
       "139315919.0            0.00           16.00  37.000000     0.948718   \n",
       "138703798.0            0.00           16.00  33.333333     0.925926   \n",
       "...                     ...             ...        ...          ...   \n",
       "2158020.0              0.00            0.00   0.000000     0.000000   \n",
       "9631088.0              0.00            0.00   0.000000     0.000000   \n",
       "3247362.0              9.75           11.75  31.500000     0.900000   \n",
       "5428531.0              0.00            0.00   0.000000     0.000000   \n",
       "138502092.0            0.00            0.00   0.000000     0.000000   \n",
       "\n",
       "             s34_phSize  s34_phRelSize  chiralMoment  chiralPhMoment  \n",
       "cid                                                                   \n",
       "2083077.0      0.000000       0.000000      0.000000        0.000000  \n",
       "138703696.0   11.000000       0.282051     54.589376       16.822604  \n",
       "139316163.0   11.000000       0.261905     59.757845       16.822604  \n",
       "139315919.0   11.000000       0.282051     54.589376       16.822604  \n",
       "138703798.0    8.333333       0.231481     49.318632       13.379088  \n",
       "...                 ...            ...           ...             ...  \n",
       "2158020.0      0.000000       0.000000      0.000000        0.000000  \n",
       "9631088.0      0.000000       0.000000      0.000000        0.000000  \n",
       "3247362.0      5.000000       0.142857     30.833138        5.196152  \n",
       "5428531.0      0.000000       0.000000      0.000000        0.000000  \n",
       "138502092.0    0.000000       0.000000      0.000000        0.000000  \n",
       "\n",
       "[6175 rows x 3228 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_NAomit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8129de55-e5e1-44df-bea5-1de439f32345",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.99  # 如果列中 0 的比例超过 90%，则移除该列\n",
    "non_zero_threshold = X_NAomit_data.shape[0] * (1 - threshold)\n",
    "X_NAomit_data =X_NAomit_data.loc[:, (X_NAomit_data != 0).sum(axis=0) > non_zero_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25db298a-25a8-4f41-ba0a-23d279862cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MW</th>\n",
       "      <th>AMW</th>\n",
       "      <th>Sp</th>\n",
       "      <th>Si</th>\n",
       "      <th>Mp</th>\n",
       "      <th>Mi</th>\n",
       "      <th>GD</th>\n",
       "      <th>nAT</th>\n",
       "      <th>nSK</th>\n",
       "      <th>nAA</th>\n",
       "      <th>...</th>\n",
       "      <th>s4_numRotBonds</th>\n",
       "      <th>s2_numAroBonds</th>\n",
       "      <th>s3_numAroBonds</th>\n",
       "      <th>s4_numAroBonds</th>\n",
       "      <th>s34_size</th>\n",
       "      <th>s34_relSize</th>\n",
       "      <th>s34_phSize</th>\n",
       "      <th>s34_phRelSize</th>\n",
       "      <th>chiralMoment</th>\n",
       "      <th>chiralPhMoment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2083077.0</th>\n",
       "      <td>425.57</td>\n",
       "      <td>8.866042</td>\n",
       "      <td>34.7672</td>\n",
       "      <td>53.2847</td>\n",
       "      <td>0.724317</td>\n",
       "      <td>1.110098</td>\n",
       "      <td>0.078818</td>\n",
       "      <td>48.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138703696.0</th>\n",
       "      <td>526.58</td>\n",
       "      <td>8.493226</td>\n",
       "      <td>42.4378</td>\n",
       "      <td>70.2759</td>\n",
       "      <td>0.684481</td>\n",
       "      <td>1.133482</td>\n",
       "      <td>0.058030</td>\n",
       "      <td>62.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>27.00</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>54.589376</td>\n",
       "      <td>16.822604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139316163.0</th>\n",
       "      <td>585.65</td>\n",
       "      <td>8.134028</td>\n",
       "      <td>46.5686</td>\n",
       "      <td>82.7424</td>\n",
       "      <td>0.646786</td>\n",
       "      <td>1.149200</td>\n",
       "      <td>0.053426</td>\n",
       "      <td>72.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>59.757845</td>\n",
       "      <td>16.822604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139315919.0</th>\n",
       "      <td>541.62</td>\n",
       "      <td>7.965000</td>\n",
       "      <td>43.8697</td>\n",
       "      <td>77.9875</td>\n",
       "      <td>0.645143</td>\n",
       "      <td>1.146875</td>\n",
       "      <td>0.056680</td>\n",
       "      <td>68.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>54.589376</td>\n",
       "      <td>16.822604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138703798.0</th>\n",
       "      <td>491.63</td>\n",
       "      <td>7.448939</td>\n",
       "      <td>43.0232</td>\n",
       "      <td>75.2290</td>\n",
       "      <td>0.651867</td>\n",
       "      <td>1.139833</td>\n",
       "      <td>0.063492</td>\n",
       "      <td>66.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>0.231481</td>\n",
       "      <td>49.318632</td>\n",
       "      <td>13.379088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158020.0</th>\n",
       "      <td>462.06</td>\n",
       "      <td>7.831525</td>\n",
       "      <td>39.7844</td>\n",
       "      <td>66.3847</td>\n",
       "      <td>0.674312</td>\n",
       "      <td>1.125164</td>\n",
       "      <td>0.070968</td>\n",
       "      <td>59.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9631088.0</th>\n",
       "      <td>246.31</td>\n",
       "      <td>9.122593</td>\n",
       "      <td>19.6137</td>\n",
       "      <td>29.9962</td>\n",
       "      <td>0.726433</td>\n",
       "      <td>1.110970</td>\n",
       "      <td>0.132353</td>\n",
       "      <td>27.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3247362.0</th>\n",
       "      <td>463.62</td>\n",
       "      <td>7.244062</td>\n",
       "      <td>43.8243</td>\n",
       "      <td>71.3113</td>\n",
       "      <td>0.684755</td>\n",
       "      <td>1.114239</td>\n",
       "      <td>0.065546</td>\n",
       "      <td>64.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.5</td>\n",
       "      <td>9.75</td>\n",
       "      <td>11.75</td>\n",
       "      <td>31.500000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>30.833138</td>\n",
       "      <td>5.196152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5428531.0</th>\n",
       "      <td>282.31</td>\n",
       "      <td>8.066000</td>\n",
       "      <td>24.1478</td>\n",
       "      <td>38.7440</td>\n",
       "      <td>0.689937</td>\n",
       "      <td>1.106971</td>\n",
       "      <td>0.109524</td>\n",
       "      <td>35.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138502092.0</th>\n",
       "      <td>477.67</td>\n",
       "      <td>8.096102</td>\n",
       "      <td>39.5173</td>\n",
       "      <td>66.7363</td>\n",
       "      <td>0.669785</td>\n",
       "      <td>1.131124</td>\n",
       "      <td>0.072581</td>\n",
       "      <td>59.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6175 rows × 1585 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 MW       AMW       Sp       Si        Mp        Mi        GD  \\\n",
       "cid                                                                             \n",
       "2083077.0    425.57  8.866042  34.7672  53.2847  0.724317  1.110098  0.078818   \n",
       "138703696.0  526.58  8.493226  42.4378  70.2759  0.684481  1.133482  0.058030   \n",
       "139316163.0  585.65  8.134028  46.5686  82.7424  0.646786  1.149200  0.053426   \n",
       "139315919.0  541.62  7.965000  43.8697  77.9875  0.645143  1.146875  0.056680   \n",
       "138703798.0  491.63  7.448939  43.0232  75.2290  0.651867  1.139833  0.063492   \n",
       "...             ...       ...      ...      ...       ...       ...       ...   \n",
       "2158020.0    462.06  7.831525  39.7844  66.3847  0.674312  1.125164  0.070968   \n",
       "9631088.0    246.31  9.122593  19.6137  29.9962  0.726433  1.110970  0.132353   \n",
       "3247362.0    463.62  7.244062  43.8243  71.3113  0.684755  1.114239  0.065546   \n",
       "5428531.0    282.31  8.066000  24.1478  38.7440  0.689937  1.106971  0.109524   \n",
       "138502092.0  477.67  8.096102  39.5173  66.7363  0.669785  1.131124  0.072581   \n",
       "\n",
       "              nAT   nSK   nAA  ...  s4_numRotBonds  s2_numAroBonds  \\\n",
       "cid                            ...                                   \n",
       "2083077.0    48.0  29.0  21.0  ...        0.000000             0.0   \n",
       "138703696.0  62.0  39.0  26.0  ...        6.000000             0.0   \n",
       "139316163.0  72.0  42.0  15.0  ...        8.000000             0.0   \n",
       "139315919.0  68.0  39.0  15.0  ...        8.000000             0.0   \n",
       "138703798.0  66.0  36.0  15.0  ...        6.333333             0.0   \n",
       "...           ...   ...   ...  ...             ...             ...   \n",
       "2158020.0    59.0  31.0  15.0  ...        0.000000             0.0   \n",
       "9631088.0    27.0  17.0  11.0  ...        0.000000             0.0   \n",
       "3247362.0    64.0  35.0  22.0  ...        3.500000             1.5   \n",
       "5428531.0    35.0  21.0  12.0  ...        0.000000             0.0   \n",
       "138502092.0  59.0  32.0  11.0  ...        0.000000             0.0   \n",
       "\n",
       "             s3_numAroBonds  s4_numAroBonds   s34_size  s34_relSize  \\\n",
       "cid                                                                   \n",
       "2083077.0              0.00            0.00   0.000000     0.000000   \n",
       "138703696.0            0.00           27.00  37.000000     0.948718   \n",
       "139316163.0            0.00           16.00  40.000000     0.952381   \n",
       "139315919.0            0.00           16.00  37.000000     0.948718   \n",
       "138703798.0            0.00           16.00  33.333333     0.925926   \n",
       "...                     ...             ...        ...          ...   \n",
       "2158020.0              0.00            0.00   0.000000     0.000000   \n",
       "9631088.0              0.00            0.00   0.000000     0.000000   \n",
       "3247362.0              9.75           11.75  31.500000     0.900000   \n",
       "5428531.0              0.00            0.00   0.000000     0.000000   \n",
       "138502092.0            0.00            0.00   0.000000     0.000000   \n",
       "\n",
       "             s34_phSize  s34_phRelSize  chiralMoment  chiralPhMoment  \n",
       "cid                                                                   \n",
       "2083077.0      0.000000       0.000000      0.000000        0.000000  \n",
       "138703696.0   11.000000       0.282051     54.589376       16.822604  \n",
       "139316163.0   11.000000       0.261905     59.757845       16.822604  \n",
       "139315919.0   11.000000       0.282051     54.589376       16.822604  \n",
       "138703798.0    8.333333       0.231481     49.318632       13.379088  \n",
       "...                 ...            ...           ...             ...  \n",
       "2158020.0      0.000000       0.000000      0.000000        0.000000  \n",
       "9631088.0      0.000000       0.000000      0.000000        0.000000  \n",
       "3247362.0      5.000000       0.142857     30.833138        5.196152  \n",
       "5428531.0      0.000000       0.000000      0.000000        0.000000  \n",
       "138502092.0    0.000000       0.000000      0.000000        0.000000  \n",
       "\n",
       "[6175 rows x 1585 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_NAomit_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cf34dcf-3f11-4e44-83ca-67542efeea48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.25570000e+02, 8.86604167e+00, 3.47672000e+01, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [5.26580000e+02, 8.49322581e+00, 4.24378000e+01, ...,\n",
       "        2.82051282e-01, 5.45893763e+01, 1.68226038e+01],\n",
       "       [5.85650000e+02, 8.13402778e+00, 4.65686000e+01, ...,\n",
       "        2.61904762e-01, 5.97578447e+01, 1.68226038e+01],\n",
       "       ...,\n",
       "       [4.63620000e+02, 7.24406250e+00, 4.38243000e+01, ...,\n",
       "        1.42857143e-01, 3.08331381e+01, 5.19615242e+00],\n",
       "       [2.82310000e+02, 8.06600000e+00, 2.41478000e+01, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [4.77670000e+02, 8.09610169e+00, 3.95173000e+01, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=np.array(X_NAomit_data)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd096547-56ce-48a2-9a6f-210d1e31be3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MW</th>\n",
       "      <th>AMW</th>\n",
       "      <th>Sp</th>\n",
       "      <th>Si</th>\n",
       "      <th>Mp</th>\n",
       "      <th>Mi</th>\n",
       "      <th>GD</th>\n",
       "      <th>nAT</th>\n",
       "      <th>nSK</th>\n",
       "      <th>nAA</th>\n",
       "      <th>...</th>\n",
       "      <th>s4_numRotBonds</th>\n",
       "      <th>s2_numAroBonds</th>\n",
       "      <th>s3_numAroBonds</th>\n",
       "      <th>s4_numAroBonds</th>\n",
       "      <th>s34_size</th>\n",
       "      <th>s34_relSize</th>\n",
       "      <th>s34_phSize</th>\n",
       "      <th>s34_phRelSize</th>\n",
       "      <th>chiralMoment</th>\n",
       "      <th>chiralPhMoment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2083077.0</th>\n",
       "      <td>425.57</td>\n",
       "      <td>8.866042</td>\n",
       "      <td>34.7672</td>\n",
       "      <td>53.2847</td>\n",
       "      <td>0.724317</td>\n",
       "      <td>1.110098</td>\n",
       "      <td>0.078818</td>\n",
       "      <td>48.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138703696.0</th>\n",
       "      <td>526.58</td>\n",
       "      <td>8.493226</td>\n",
       "      <td>42.4378</td>\n",
       "      <td>70.2759</td>\n",
       "      <td>0.684481</td>\n",
       "      <td>1.133482</td>\n",
       "      <td>0.058030</td>\n",
       "      <td>62.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>27.00</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>54.589376</td>\n",
       "      <td>16.822604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139316163.0</th>\n",
       "      <td>585.65</td>\n",
       "      <td>8.134028</td>\n",
       "      <td>46.5686</td>\n",
       "      <td>82.7424</td>\n",
       "      <td>0.646786</td>\n",
       "      <td>1.149200</td>\n",
       "      <td>0.053426</td>\n",
       "      <td>72.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>59.757845</td>\n",
       "      <td>16.822604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139315919.0</th>\n",
       "      <td>541.62</td>\n",
       "      <td>7.965000</td>\n",
       "      <td>43.8697</td>\n",
       "      <td>77.9875</td>\n",
       "      <td>0.645143</td>\n",
       "      <td>1.146875</td>\n",
       "      <td>0.056680</td>\n",
       "      <td>68.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>54.589376</td>\n",
       "      <td>16.822604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138703798.0</th>\n",
       "      <td>491.63</td>\n",
       "      <td>7.448939</td>\n",
       "      <td>43.0232</td>\n",
       "      <td>75.2290</td>\n",
       "      <td>0.651867</td>\n",
       "      <td>1.139833</td>\n",
       "      <td>0.063492</td>\n",
       "      <td>66.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>0.231481</td>\n",
       "      <td>49.318632</td>\n",
       "      <td>13.379088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158020.0</th>\n",
       "      <td>462.06</td>\n",
       "      <td>7.831525</td>\n",
       "      <td>39.7844</td>\n",
       "      <td>66.3847</td>\n",
       "      <td>0.674312</td>\n",
       "      <td>1.125164</td>\n",
       "      <td>0.070968</td>\n",
       "      <td>59.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9631088.0</th>\n",
       "      <td>246.31</td>\n",
       "      <td>9.122593</td>\n",
       "      <td>19.6137</td>\n",
       "      <td>29.9962</td>\n",
       "      <td>0.726433</td>\n",
       "      <td>1.110970</td>\n",
       "      <td>0.132353</td>\n",
       "      <td>27.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3247362.0</th>\n",
       "      <td>463.62</td>\n",
       "      <td>7.244062</td>\n",
       "      <td>43.8243</td>\n",
       "      <td>71.3113</td>\n",
       "      <td>0.684755</td>\n",
       "      <td>1.114239</td>\n",
       "      <td>0.065546</td>\n",
       "      <td>64.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.5</td>\n",
       "      <td>9.75</td>\n",
       "      <td>11.75</td>\n",
       "      <td>31.500000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>30.833138</td>\n",
       "      <td>5.196152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5428531.0</th>\n",
       "      <td>282.31</td>\n",
       "      <td>8.066000</td>\n",
       "      <td>24.1478</td>\n",
       "      <td>38.7440</td>\n",
       "      <td>0.689937</td>\n",
       "      <td>1.106971</td>\n",
       "      <td>0.109524</td>\n",
       "      <td>35.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138502092.0</th>\n",
       "      <td>477.67</td>\n",
       "      <td>8.096102</td>\n",
       "      <td>39.5173</td>\n",
       "      <td>66.7363</td>\n",
       "      <td>0.669785</td>\n",
       "      <td>1.131124</td>\n",
       "      <td>0.072581</td>\n",
       "      <td>59.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6175 rows × 1585 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 MW       AMW       Sp       Si        Mp        Mi        GD  \\\n",
       "cid                                                                             \n",
       "2083077.0    425.57  8.866042  34.7672  53.2847  0.724317  1.110098  0.078818   \n",
       "138703696.0  526.58  8.493226  42.4378  70.2759  0.684481  1.133482  0.058030   \n",
       "139316163.0  585.65  8.134028  46.5686  82.7424  0.646786  1.149200  0.053426   \n",
       "139315919.0  541.62  7.965000  43.8697  77.9875  0.645143  1.146875  0.056680   \n",
       "138703798.0  491.63  7.448939  43.0232  75.2290  0.651867  1.139833  0.063492   \n",
       "...             ...       ...      ...      ...       ...       ...       ...   \n",
       "2158020.0    462.06  7.831525  39.7844  66.3847  0.674312  1.125164  0.070968   \n",
       "9631088.0    246.31  9.122593  19.6137  29.9962  0.726433  1.110970  0.132353   \n",
       "3247362.0    463.62  7.244062  43.8243  71.3113  0.684755  1.114239  0.065546   \n",
       "5428531.0    282.31  8.066000  24.1478  38.7440  0.689937  1.106971  0.109524   \n",
       "138502092.0  477.67  8.096102  39.5173  66.7363  0.669785  1.131124  0.072581   \n",
       "\n",
       "              nAT   nSK   nAA  ...  s4_numRotBonds  s2_numAroBonds  \\\n",
       "cid                            ...                                   \n",
       "2083077.0    48.0  29.0  21.0  ...        0.000000             0.0   \n",
       "138703696.0  62.0  39.0  26.0  ...        6.000000             0.0   \n",
       "139316163.0  72.0  42.0  15.0  ...        8.000000             0.0   \n",
       "139315919.0  68.0  39.0  15.0  ...        8.000000             0.0   \n",
       "138703798.0  66.0  36.0  15.0  ...        6.333333             0.0   \n",
       "...           ...   ...   ...  ...             ...             ...   \n",
       "2158020.0    59.0  31.0  15.0  ...        0.000000             0.0   \n",
       "9631088.0    27.0  17.0  11.0  ...        0.000000             0.0   \n",
       "3247362.0    64.0  35.0  22.0  ...        3.500000             1.5   \n",
       "5428531.0    35.0  21.0  12.0  ...        0.000000             0.0   \n",
       "138502092.0  59.0  32.0  11.0  ...        0.000000             0.0   \n",
       "\n",
       "             s3_numAroBonds  s4_numAroBonds   s34_size  s34_relSize  \\\n",
       "cid                                                                   \n",
       "2083077.0              0.00            0.00   0.000000     0.000000   \n",
       "138703696.0            0.00           27.00  37.000000     0.948718   \n",
       "139316163.0            0.00           16.00  40.000000     0.952381   \n",
       "139315919.0            0.00           16.00  37.000000     0.948718   \n",
       "138703798.0            0.00           16.00  33.333333     0.925926   \n",
       "...                     ...             ...        ...          ...   \n",
       "2158020.0              0.00            0.00   0.000000     0.000000   \n",
       "9631088.0              0.00            0.00   0.000000     0.000000   \n",
       "3247362.0              9.75           11.75  31.500000     0.900000   \n",
       "5428531.0              0.00            0.00   0.000000     0.000000   \n",
       "138502092.0            0.00            0.00   0.000000     0.000000   \n",
       "\n",
       "             s34_phSize  s34_phRelSize  chiralMoment  chiralPhMoment  \n",
       "cid                                                                   \n",
       "2083077.0      0.000000       0.000000      0.000000        0.000000  \n",
       "138703696.0   11.000000       0.282051     54.589376       16.822604  \n",
       "139316163.0   11.000000       0.261905     59.757845       16.822604  \n",
       "139315919.0   11.000000       0.282051     54.589376       16.822604  \n",
       "138703798.0    8.333333       0.231481     49.318632       13.379088  \n",
       "...                 ...            ...           ...             ...  \n",
       "2158020.0      0.000000       0.000000      0.000000        0.000000  \n",
       "9631088.0      0.000000       0.000000      0.000000        0.000000  \n",
       "3247362.0      5.000000       0.142857     30.833138        5.196152  \n",
       "5428531.0      0.000000       0.000000      0.000000        0.000000  \n",
       "138502092.0    0.000000       0.000000      0.000000        0.000000  \n",
       "\n",
       "[6175 rows x 1585 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_NAomit_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f561878b-84e9-41a4-aecf-49bd8626f377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IsomericSMILES</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Canonical_smiles</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2083077.0</th>\n",
       "      <td>CC1=CC(=C(N1C2=CC=C(C=C2)OC)C)C(=O)CSC3=NN=C(O...</td>\n",
       "      <td>1</td>\n",
       "      <td>COc1ccc(-n2c(C)cc(C(=O)CSc3nnc(-c4cccs4)o3)c2C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138703696.0</th>\n",
       "      <td>CC(C)([C@@H](CNC(=O)C1=CN=C(C=C1NC2=CN=C(C=C2)...</td>\n",
       "      <td>1</td>\n",
       "      <td>CC(C)(O)[C@H](F)CNC(=O)c1cnc(-c2ccc3cc(C#N)cnn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139316163.0</th>\n",
       "      <td>CC(C)([C@@H](CNC(=O)C1=CN=C(C=C1NC2CC(C2)COC(=...</td>\n",
       "      <td>1</td>\n",
       "      <td>CC(C)(O)[C@H](F)CNC(=O)c1cnc(-c2ccc3cc(C#N)cnn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139315919.0</th>\n",
       "      <td>CC(C)([C@@H](CNC(=O)C1=CN=C(C=C1NC2CC(C2)(CNC(...</td>\n",
       "      <td>1</td>\n",
       "      <td>COC(=O)NCC1(F)CC(Nc2cc(-c3ccc4cc(C#N)cnn34)ncc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138703798.0</th>\n",
       "      <td>CC(C)([C@@H](CNC(=O)C1=CN=C(C=C1NC2C[C@@H]3CNC...</td>\n",
       "      <td>1</td>\n",
       "      <td>CC(C)(O)[C@H](F)CNC(=O)c1cnc(-c2ccc3cc(C#N)cnn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158020.0</th>\n",
       "      <td>CC1=C(C2=C(N1)C=CC(=C2)Cl)CCN(CC3=CC(=C(C(=C3)...</td>\n",
       "      <td>0</td>\n",
       "      <td>CNC(=S)N(CCc1c(C)[nH]c2ccc(Cl)cc12)Cc1cc(OC)c(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9631088.0</th>\n",
       "      <td>C1=CC(=CC(=C1)N/N=C/C2=CC=CS2)C(=O)O</td>\n",
       "      <td>0</td>\n",
       "      <td>O=C(O)c1cccc(N/N=C/c2cccs2)c1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3247362.0</th>\n",
       "      <td>C[C@H]([C@@H]1C[C@@]1(C)[C@H](C2=CC=CC=C2)NC(=...</td>\n",
       "      <td>0</td>\n",
       "      <td>C[C@@H](C(=O)Nc1ccc2ccccc2c1)[C@@H]1C[C@@]1(C)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5428531.0</th>\n",
       "      <td>CC1=C(C(=O)OC2=CC(=CC(=C12)O)O)CC3=CC=CC=C3</td>\n",
       "      <td>0</td>\n",
       "      <td>Cc1c(Cc2ccccc2)c(=O)oc2cc(O)cc(O)c12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138502092.0</th>\n",
       "      <td>CCCN(C1CCOCC1)C2=NC(=NC3=C2S(=O)CC3)N4CC(C4)OC...</td>\n",
       "      <td>0</td>\n",
       "      <td>CCCN(c1nc(N2CC(OC(=O)c3cscn3)C2)nc2c1S(=O)CC2)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6175 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                IsomericSMILES  Activity  \\\n",
       "cid                                                                        \n",
       "2083077.0    CC1=CC(=C(N1C2=CC=C(C=C2)OC)C)C(=O)CSC3=NN=C(O...         1   \n",
       "138703696.0  CC(C)([C@@H](CNC(=O)C1=CN=C(C=C1NC2=CN=C(C=C2)...         1   \n",
       "139316163.0  CC(C)([C@@H](CNC(=O)C1=CN=C(C=C1NC2CC(C2)COC(=...         1   \n",
       "139315919.0  CC(C)([C@@H](CNC(=O)C1=CN=C(C=C1NC2CC(C2)(CNC(...         1   \n",
       "138703798.0  CC(C)([C@@H](CNC(=O)C1=CN=C(C=C1NC2C[C@@H]3CNC...         1   \n",
       "...                                                        ...       ...   \n",
       "2158020.0    CC1=C(C2=C(N1)C=CC(=C2)Cl)CCN(CC3=CC(=C(C(=C3)...         0   \n",
       "9631088.0                 C1=CC(=CC(=C1)N/N=C/C2=CC=CS2)C(=O)O         0   \n",
       "3247362.0    C[C@H]([C@@H]1C[C@@]1(C)[C@H](C2=CC=CC=C2)NC(=...         0   \n",
       "5428531.0          CC1=C(C(=O)OC2=CC(=CC(=C12)O)O)CC3=CC=CC=C3         0   \n",
       "138502092.0  CCCN(C1CCOCC1)C2=NC(=NC3=C2S(=O)CC3)N4CC(C4)OC...         0   \n",
       "\n",
       "                                              Canonical_smiles  \n",
       "cid                                                             \n",
       "2083077.0    COc1ccc(-n2c(C)cc(C(=O)CSc3nnc(-c4cccs4)o3)c2C...  \n",
       "138703696.0  CC(C)(O)[C@H](F)CNC(=O)c1cnc(-c2ccc3cc(C#N)cnn...  \n",
       "139316163.0  CC(C)(O)[C@H](F)CNC(=O)c1cnc(-c2ccc3cc(C#N)cnn...  \n",
       "139315919.0  COC(=O)NCC1(F)CC(Nc2cc(-c3ccc4cc(C#N)cnn34)ncc...  \n",
       "138703798.0  CC(C)(O)[C@H](F)CNC(=O)c1cnc(-c2ccc3cc(C#N)cnn...  \n",
       "...                                                        ...  \n",
       "2158020.0    CNC(=S)N(CCc1c(C)[nH]c2ccc(Cl)cc12)Cc1cc(OC)c(...  \n",
       "9631088.0                        O=C(O)c1cccc(N/N=C/c2cccs2)c1  \n",
       "3247362.0    C[C@@H](C(=O)Nc1ccc2ccccc2c1)[C@@H]1C[C@@]1(C)...  \n",
       "5428531.0                 Cc1c(Cc2ccccc2)c(=O)oc2cc(O)cc(O)c12  \n",
       "138502092.0  CCCN(c1nc(N2CC(OC(=O)c3cscn3)C2)nc2c1S(=O)CC2)...  \n",
       "\n",
       "[6175 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "066559ff-aefe-4de1-98aa-04e6e2d7d8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=Raw_data['Activity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f04d536-af36-4f80-8c82-b43e30cbf6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cv_model= RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf835fc-df29-4f95-ac06-6cc88fa619f2",
   "metadata": {},
   "source": [
    "# 1. LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07038cde-c52d-4ba2-b851-445a650f8dc7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.15518048070794066, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.13472865983499105, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.37079100200207904, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.13596818835151225, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.35911352487892145, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6812515119765976, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8806562738894854, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0574691444396649, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1645509603677624, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9958676192017037, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7814924827783329, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.007993621929245, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5477225376428692, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1788773198435365, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4016235084130813, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.7125568673400267, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.525256719970685, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.934073566840084, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.8644133673630563, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.961611571399942, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.1361253266040876, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.19342507072076387, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.16871303822699701, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.37319572623698605, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8719613633567747, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8045811302148422, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5213107283857426, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4810469835801996, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5468391885437427, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5526118951396484, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4617282703487149, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7697824604262564, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1729549789152998, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.2655847230259951, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0619619895412598, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3335021773466451, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.2865031283398594, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.6057815695353383, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.967317083204648, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.6451845615305274, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.603683165624375, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.729526387455735, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.3966050820934583, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.7550149230999637, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.1795060177303185, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.19335366339407756, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.14467226087435847, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.28610128230934606, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2960043378645878, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.18957442251473822, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3614482288091949, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4848098170399453, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.43730471994979325, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3143242650218667, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.770367521092794, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0092949731318868, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0302070429662535, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1970385929836311, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3284547282535186, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.286241688730911, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.0131863865426567, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8330977316812778, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.1744419617392623, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.9333280341344903, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.12602279863983767, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.21440623044782114, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2939597582122815, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.1496681538218354, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.16246263977018316, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.1776367499388698, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.17688856131337616, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2650670496885823, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.30731421819655225, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6351388402129032, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3136857025887707, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7769480111544453, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5371184061983172, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3611738400509239, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3106196035974449, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7448183237364105, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9229005039308049, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.2867141561296762, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.2668998554132713, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.264423154073313, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.17120244443333377, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.20320698579428154, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.23493898710967187, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.615741273488311, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3671412699155212, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5439815263519563, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6657148900913512, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.2323995919318804, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0923743886381772, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8156545571911806, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8172451610818712, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7765046342685764, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9251100828528251, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4620081122533861, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.0934770327246497, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8378451799576396, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.35025907109809395, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7394112958473897, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4319647243377176, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6438625520785308, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4862808994230363, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8017553623992626, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9177532223957314, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9925682668011291, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0026861846891961, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3469869970433592, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.2112833321931475, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.6925307385118344, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.7776007185279923, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.7503785652572788, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.5713279456584814, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.016898979721077, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.686766168826807, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.5898348478148137, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.9761882469347825, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.22112303028222868, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.13764481111320492, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.1653618990854966, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.1893366500622733, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.36472154003593005, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4651110398934861, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.22221828844124047, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.130287480765503, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2930940430754845, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5296079038567996, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.956548120436878, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.41197511387105124, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.23689180236709717, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.721463708574106, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0159924889454715, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.190942237967647, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.477914603554268, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.6738895185928868, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.6579146644564844, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.1249480365678437, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.16550697070601927, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2920729503869097, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5103092034535166, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3791781456358194, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.21555672546367077, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.19172157151251668, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.31371370544616184, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.32983326058183593, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.28952533585436413, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3095198703889537, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4428628812685247, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5082860284310868, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6992480517932904, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.45020795674520286, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.28293550254738875, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9021794161451453, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.493826742322426, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8361340878010424, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.5094345580972686, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.5557715961800227, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.9804808148103348, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8798671123380473, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.17583570060730835, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.22537922680305655, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3012491419873413, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2525001663142348, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2179545452978573, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.41839405054520284, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.31302311514627945, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3622835310715118, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3329427971000314, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.43050881786081163, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5660398273154215, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.725972974614649, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5572406752585266, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3813865695285017, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7257987896755935, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3557248194852036, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7116562569779603, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.5230158091623025, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.419194143989273, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.13303781021738814, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.1996296127314281, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.34195072142745175, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7393613113616766, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.552893904793109, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.48334614641828466, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.44341234563069065, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6970939554904589, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7705125203033276, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0295941882060902, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3460485404945075, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.2940955811979507, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7130266197044648, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7793803659044443, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5917718226261286, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.045872659486406, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.109740996725236, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.0761741346266263, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.52888143498609, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.1287558732461775, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.0720668820972605, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.797250375174372, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.1306917647955288, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.19379570328999307, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3046597554678101, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2443506769080841, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3024762421310925, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3569403904913315, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2175717104260002, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5233398004124865, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9901558693240986, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.2069390631525039, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0760912957146616, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5525388800067503, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7422233833844984, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3488184678924426, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1265025871660441, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.0063649153522647, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.8160097482792708, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.132590104202222, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.29804372180206, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.465494235717074, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.18736691407980288, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.12997734096836666, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.16898566841859974, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3271850956961089, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.375261504923742, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.35550886397703607, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.24816096572095603, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3482465470818852, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5277795535465657, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8851567278860557, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.551856481936511, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6521411489734419, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6929532734200166, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7111011446759221, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.065482562749196, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8171095354068143, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.6752544758044792, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4604450499704171, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.0153650482116063, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.1809590407614223, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.854086264988041, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.0989565901992364, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2520989640620428, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3229641890743551, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.153072024874632, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.300978079568722, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.23379852785456023, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.33493671791370616, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3770976009718652, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9551936035342123, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.290684763749823, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3413166979603375, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1151736060022586, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3999202714476269, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7974744570949497, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.566795094367535, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1291928464462444, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8414165028316347, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.4319997583645545, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.438607484426882, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.222823265621173, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8575389490225689, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8731201215354645, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.264889832122435, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.20564343757189363, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.13350833893400704, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2503301064881498, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3009757081791804, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.355438068047647, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.29659354916259417, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.18082031395010745, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.16157715977914222, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.19502085548595005, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.43920559255093394, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.49233951913129204, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.067091317477093, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.628358843786998, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9593308762661081, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0634917482451556, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4205736329344063, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.2400819972380077, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8241173797938472, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.6445335533864522, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.4606508454947402, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.907997830248405, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2066670078019115, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3045182658556769, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.37116139641329937, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.1582875318722472, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.248206715244919, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2825592594393811, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5221937030783579, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6417012899189558, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0011283534277595, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7443684502296719, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4954131419018495, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6354706826017491, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7451882749181777, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5747541147467814, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.0907811742609965, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7353174150129007, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4666854005650976, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.9496504565022406, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.642779629949075, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.8094543257423084, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.37870958792211695, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5030135395790012, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4473761853762426, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3577829470123106, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2876353181200102, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.49865760315879015, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.42676172864179307, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.1611800401523169, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5418647093226809, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5226136816676217, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5056768476787283, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8117966247117465, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9229122498977631, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5261602156283516, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7873447405228262, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0509735984069266, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9288239802934868, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3198989356156972, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1914603892880677, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.165616666259666, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3670879305059316, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.914611604195784, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.13954634871618055, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.22903063445107819, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4681051841137105, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3803338000383292, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2877303894911165, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3369823407280137, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2558668637308301, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.43109360818829146, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.26872555050067604, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3912974549829187, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.35477842633383716, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4145065696679353, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.656579914962208, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6452214902996047, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7362052174721043, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1010773382508319, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.2415250722380051, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.783749895318806, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5316956018521068, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.128475159623008, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.23037214817350105, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5043997449910194, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4482775355850208, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.40958186952104825, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3125788361190871, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4867376086862123, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.31754363360050775, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3582465974747606, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7345499333481484, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.031883512944205, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1822609307847642, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.1290988078416717, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.4970577423350733, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.0028833513638347, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.3526235415546353, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.9663324918253693, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4178534594826715, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.041729856942254, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.532773403089493, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.18319188665361708, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.22720251292992089, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2664843127151926, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2342990543736505, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3540492226787819, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3704714990725506, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5187811772365194, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8592596353691988, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7890216691366163, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5810343205982349, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.803020378645499, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8628839583457193, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5862629540563375, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7384659180546578, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3476277799191507, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.401790011720209, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.2327029554852516, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7686603289590153, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.218278292260379, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.15948474210017594, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3499010381692642, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5541681744797415, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7219184897331843, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9210772590786291, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5454261016491273, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5151630643040335, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.23028387392821514, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.23414124625537625, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6337720463960181, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8828477311388951, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8455796755100096, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8875734189311402, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.487109659186217, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.5681831046734374, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.762596704600128, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.136251488896278, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.9218606872461805, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.495350883582887, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.58895676485821, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.9165029591437133, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.052822230255288, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.12274768822487658, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.29324239209097414, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2186627973234181, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.21612620096334467, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2822814898014485, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.42066153779649085, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5421797528880461, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4554220541969016, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.28488932089453556, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8909811718066294, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.656910612890158, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3886784423475547, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.6914184816495208, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.722389193908839, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.804746925794916, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.617898864031531, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.6853207556038114, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.09176964163953, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.785142919316911, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.149559447228512, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.25143225879480724, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3314658160188628, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.422388658774139, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3855096645421554, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3824018743578108, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.315166269494739, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4601210950560244, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.43959444645099666, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5956031540032427, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7562140265639528, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0651811588134024, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8442699158549658, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6231973992559006, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5863897927515609, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5008795119611023, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7225184117762069, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1343365361594806, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0776951792389013, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8581275398806838, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.577508233949061, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5587841130289917, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8567874594170348, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.741888521654232, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.13655867917384512, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.36912022282092494, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7255382065593494, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4716236418406652, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.45031809747968055, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.49695840540971403, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5337068547005401, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.784771329394971, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6428594858161318, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.47134223842135725, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6966532088931103, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.44725400646177604, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7588286906163262, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5108406420075653, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7463712176480612, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.180540021708339, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.70661728931924, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.7946470261852596, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.6199491357145916, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.4620309388968167, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.417963291512933, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.13826021113936804, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.22745963035265504, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.20421490279943555, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.239656941906901, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.26523364418335404, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2421039115321264, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.35841464846282634, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3918554162960959, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4275397092231401, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4436549243643526, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.24906106363113167, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7151095498743416, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.6065903367770034, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8966613345144197, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.845973103470783, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5025206464058556, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.242956827275293, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.6311107942283343, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.0587860018581523, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.7374826618842008, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.7502241309050532, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.14058616801969492, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.399388689279931, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5394381062986895, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3389998265852796, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7969789472630282, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.2841112338319363, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.436600263253581, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.543815369608751, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5113512178420478, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4772814370523975, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.539780002326097, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4332267009141333, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5509959053648004, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4623325105375784, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4451585716191175, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.16574809983012528, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2118412010894417, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3254247095384244, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.41059572458777893, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.46137970530890016, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6599038218334954, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6923452936641183, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5669552848531794, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5909278687803976, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7469288995720262, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6879663299022241, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8660137659618954, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7521717335140465, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8125117253242706, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.2747606892875183, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.702461014885614, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4617097933753485, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.108479754413736, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9183664118932029, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0930053089501826, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.034696361136241, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7120615083124449, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8025273604409904, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.1409500145621223, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.15497328786068465, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4816465757534161, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.39678064764194687, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.36265884104727775, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.39879764937899154, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.010919585027068, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1257116445581232, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0878194445979261, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4332992943803902, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.9700233720063807, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.5222967021361455, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.3057487910556347, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8804873192166838, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.8575645513089967, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.2773260101912456, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.3321226984095347, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.6782532527601006, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.13076269402790786, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.14920596521210427, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3145473922372162, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4229904754818108, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.37020968303295376, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3989890707235304, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7673958737857447, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9979908155748944, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0485740772079453, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1030648876819669, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.143684044415636, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.0196413496539094, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.1545787040031428, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.621667262495862, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7981665223196615, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.3921198708470683, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.2885518679846086, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.9514175400885279, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.0186921199481276, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.4894387453101103, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.13434052601985513, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.21402865417189787, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3488416366153615, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4639721276947171, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3283255713249673, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.42798893809037963, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.38043711116409895, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4289609129285168, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7742683641293979, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9910827077025033, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7654161242710984, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4670543483524625, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.48568045428726236, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5746273778131581, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6744963414424774, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8197898248856177, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5196920642803207, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.03818619414119, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.1499869317682396, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.365824991812076, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.584140385837486, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.16831213892783126, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.18600044600134424, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.21974153819803632, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3300681557607277, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.12340628336426107, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2971580327638321, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3945644149478085, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3013327335251006, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.24042714192165704, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7528187257292984, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.844635961179165, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.24417731067200066, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4196798832059585, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0179069864873327, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1603794643513083, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1172358776451574, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1319223533466243, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8019488767897656, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5989241658238598, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6929643408900006, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.412355986661737, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.4851780354801463, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.13569975282143787, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.28692403301465674, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.23637639058335935, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.28690577529278016, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3936493164064814, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.31706844168923, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.29770865717216566, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6695439994136905, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9045186762303956, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9762500509655183, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.923348696213452, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.213972406443588, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.395365054714489, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.2937381248739825, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.913820667226332, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.561701386764412, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.6498260752998704, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.3196877131728115, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.17653768045499874, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.26927512052805014, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3606388600711625, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.48033518965542044, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6671307081285249, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.33253916347257473, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.41394239542682953, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.49254433674104803, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6677618271204437, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8175896086144689, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.35498665008890384, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3623874997264238, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.266232726803878, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.46344431591842294, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0595611008900505, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0802934227474452, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.178454474335524, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.7794785748076833, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.9364890292571317, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.003945355057795, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.368793773482253, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4975298773447321, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.37571895155150514, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6006799894590245, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.34828806641473875, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.48426538128148877, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6151128124860179, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5783421978940169, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7781877876085446, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3947263273871044, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.161773226314722, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8082078350926167, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.133346912502418, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7740174290870527, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.236258107544586, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.7962038460138388, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.906084339147128, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.1479486578406295, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.17468496336425687, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.21426084804090806, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.15812647334905705, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.26603817661066387, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.30502825409985235, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.551818990836523, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5615899427406248, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.77333793071125, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7442693035519596, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8744887246839426, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9518532820401902, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0344696095837094, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.6967911735167718, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.2684797682853173, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.7762737580458747, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.538276626057609, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.3794355614872984, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.1479709553959765, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.517247784607548, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.9808584345947793, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2318276388805316, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2615752951888908, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2487007641400396, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.29283832390797215, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.41379049086640407, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4028519918853135, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.24534192221096873, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.600921109790761, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7026346761816171, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6432474898382452, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6355399163441007, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7807176087287644, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8943070762491061, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1059557983294326, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1001354593221322, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0375760462987955, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0651465125459936, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.2924765981601922, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1918354715892292, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.0767108347401972, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.547288507010194, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.13594137862901334, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.24213897739514323, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2380194372386768, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.24956154110975604, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3481351056694848, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.35748387713863394, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.22685743132342395, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.253485359975798, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.20306210348576315, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8207553529458664, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.222740489875946, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4120420545286834, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8388359883824705, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4534763163542266, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1952033542964955, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3286771164516722, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.2168465905238008, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7829419220909699, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1261688982506257, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.970121548160023, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.396768061858495, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.161658322653693, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.1442694369588935, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2426202692017796, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2611551030307737, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2792589663926037, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4603094859659791, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.42347762130992805, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4684755043916766, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.805495491874467, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1742670479202388, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0285171195612293, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0770575272653105, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8869100024724048, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9462795008964235, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.359903336237096, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8749836684003185, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.6787312140428412, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1572752309672296, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.061660280612955, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.085195663512195, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.939427461706373, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8823230624332155, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.1878151923783662, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.31279859915719044, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.19392248636540899, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2920783292690885, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4014168209075706, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.47086739225198926, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.735951867225026, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7543076836696514, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5450141988937389, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1182044021995807, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.6094613649668759, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5376230103938724, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.2703879218556722, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5102250792195377, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3968165967316963, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.9386914356866214, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.511696362221528, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.6412235277277887, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.650459286055479, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.14591903072778223, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4431544926148945, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3021176231463869, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5947647772114806, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7135550157447597, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5189265165445249, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.502700634289738, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.34505173562939717, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.46859534071768394, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.42325717895789694, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4004603324535765, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5348761262070525, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3084300882028401, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.324781659569851, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3632859350956323, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4066624458753267, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.372789076727713, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5424776118246086, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.4087304557147036, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.1036103276661606, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.493785972699413, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.4153492079302055, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.361560836432432, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.23786673193421848, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2910725346739582, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.39172276542456075, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6326030471290096, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3569371128530179, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4273820927767247, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5202129728655791, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.36503703175185365, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6560784923806864, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4327436450356572, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.436214865026443, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5109499461503901, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6304861406629243, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9084560180415338, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.2274131823777452, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.322518368747751, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9716040787689622, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.004561490900869, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0796266346942502, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4791848115463608, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.26278366139456466, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3947012175336795, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4102495126951453, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.34381457777942614, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.23674209450427952, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.17951555529032248, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.1869539627924155, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.39842182423802797, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.65677164693534, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1377480927748138, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9352182666444833, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9223683305643817, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.048950700281864, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9522677718268824, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8814248551832975, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4999302930837075, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.671391951961482, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.320899136429773, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.7175722746172823, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.7825573172726763, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.1963401641748987, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.1944278156324799, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.28593445980982324, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.28872416728790995, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.15699469570017754, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2340789138117998, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.29170688012095525, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8179423456274435, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.871376017890725, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.547231890556759, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6321653936662415, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6037228474635867, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1473241315162568, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8034606668111905, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.2970355463406804, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.880762393296777, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.8739706946961974, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.29408772874001, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.36477042174306, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.20890731070630864, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5104509957520236, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3085660861018482, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4028490956143287, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.1539221522830303, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4529356251881609, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3541597291143717, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.292434181842907, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.2549516571779122, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7308350000030259, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8311173589116834, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1002614891835378, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.0369210751552487, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.033888592868266, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.8554780832375855, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.389949642149702, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.490970401760592, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.130998296065286, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.20834586794285315, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4451018620934519, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.575776480289278, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7086402580689537, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5402101305732003, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6032364063947284, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7092295006294194, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9335418044880726, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.043873268586708, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0240777326903299, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5329207637611262, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8273426392277088, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.98166576231597, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.537347998667542, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.0032708995997837, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.5042955503899975, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.534173778829427, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.49993003861448, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.702808473484197, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.297524468001143, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.14949409289965843, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.13456722719718073, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3066438353372405, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.44010791914826086, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6274074845094901, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.36297265390351185, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5157380387850026, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.824282794289843, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0388686725991647, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.033596302271917, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.779700857547823, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0605357980348344, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.2515904829550664, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8930319878891169, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5049182483324444, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7345610784165046, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6795761827909246, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5788790429195387, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0728076904262025, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5516510295562966, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.15906743904452, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.1435369619582616, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.18253863203040055, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2913046358420388, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.43425242583577983, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.33325739492181583, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7747080103005146, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.794309635958939, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5469883830995741, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8100701604261076, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1444452635570883, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3901220989918102, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.9010717743632313, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.0278968665152206, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3585697604278835, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8504058974341433, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.818742577621606, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.398769153447347, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8734316007758025, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.815869876974432, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5219337032350495, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.1908953594521563, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.22962401972108637, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6582329427965874, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2701562506306914, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6812308458271445, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.28370583781241976, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4637236421395414, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5406992717030334, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.18639139014158, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1603455223230412, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.6115553134537208, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8045121038887828, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.1218044357974577, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.0490041011540256, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.2673002044920167, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.6857553819131113, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.9380024851722055, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.782096998896918, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.6263615434700682, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.16274836432705797, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.21307935025618008, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.21077677260439032, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.18872733027990307, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.20226199903140696, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.18902593503213438, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.15292128349472023, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.30266967308409676, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.686612000079549, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7578945220539026, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7543896248112105, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.371796792372038, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3546012682677429, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.9373846790285256, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.6359069205719834, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.341573644457071, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.149052123030799, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.0164334510291155, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.588321334861007, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.053624640746989, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.13084436480835393, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.22357869248241968, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4280156448346588, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.46869909693612044, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.37315847200977714, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4095334141023841, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3072453116548104, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2418082262589678, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4020056502318994, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.842410382236892, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8023209127575228, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0120849245041654, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.6894006242217756, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.808925973305236, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.0131420968515386, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.760591360007197, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.353528365989632, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.086529857287644, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.074612465922087, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.13086271221925472, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.15822512967872626, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.17813237988400488, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.16483898079718529, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.20705117851377963, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3157611984429991, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.537578368363711, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6552798884185904, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7257138636568357, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6344659198387035, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.609256294315287, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7058003032717579, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.2735970428111045, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.90298601718996, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.337568361219951, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.6490820263115324, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.702266038053949, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.6891754214237267, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.009760016055452, tolerance: 0.12219400809716599\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    }
   ],
   "source": [
    "model = make_pipeline(StandardScaler(), LassoCV(cv=Cv_model)).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "402b4353-a400-45cc-ba23-bddd8b9e5d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1ad12a46610>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAG6CAYAAAD07mc1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB5F0lEQVR4nO3dd3xTVf8H8E92mu696KSMQpktW5aycYALRUXGoyIi64co8oiIKD64QBERARFEBBRUhgNkbyiblt3SQksnHUmafX5/VIO1gARa0obP+/XKi+Tk3Hu/93KbfHPuuedIhBACRERERC5C6uwAiIiIiKoSkxsiIiJyKUxuiIiIyKUwuSEiIiKXwuSGiIiIXAqTGyIiInIpTG6IiIjIpTC5ISIiIpfC5IaIiIhcCpMbIiIicilOTW62bduGBx54AGFhYZBIJPjxxx//dZmtW7ciMTERarUasbGxmDt3bvUHSkRERLWGU5MbnU6HZs2aYfbs2TdVPy0tDX369EHHjh1x6NAhvP766xg1ahR++OGHao6UiIiIagtJTZk4UyKRYPXq1ejXr99167z66qv4+eefkZqaai8bPnw4jhw5gt27d9+BKImIiKimkzs7AEfs3r0bPXr0qFDWs2dPLFiwAGazGQqFotIyRqMRRqPR/tpms6GwsBD+/v6QSCTVHjMRERHdPiEESktLERYWBqn0xheealVyc/nyZQQHB1coCw4OhsViQX5+PkJDQystM336dLz11lt3KkQiIiKqRpmZmahTp84N69Sq5AZApdaWv66qXa8VZuLEiRg3bpz9dXFxMSIjI5GZmQkvL6/qC5ToFuh0OoSFhQEAsrKy4O7u7uSIiIhqhpKSEkRERMDT0/Nf69aq5CYkJASXL1+uUJabmwu5XA5/f/9rLqNSqaBSqSqVe3l5MbmhGkcmk9mfe3l5MbkhIvqHm+lSUqvGuWnXrh02bNhQoez3339HUlLSNfvbEBER0d3HqS03Wq0WZ8+etb9OS0vD4cOH4efnh8jISEycOBGXLl3C4sWLAZTfGTV79myMGzcOzz33HHbv3o0FCxZg2bJlztoFoioll8vx7LPP2p8TEZHjnHor+JYtW9C1a9dK5c8++ywWLVqEwYMHIz09HVu2bLG/t3XrVowdOxYnTpxAWFgYXn31VQwfPvymt1lSUgJvb28UFxfzshQR3TSbzQaTyeTsMIhcmlKpvO6dUI58f9eYcW7uFCY3ROQok8mEtLQ02Gw2Z4dC5NKkUiliYmKgVCorvefI9zfbvYlqECEE9Ho9AECj0XAsphpACIHs7GzIZDJERET86/gaRHRrbDYbsrKykJ2djcjIyNv6/GNyQ1SD6PV6eHh4ACjvk8a7pZzPYrFAr9cjLCwMGo3G2eEQubTAwEBkZWXBYrHc1o1C/AlCRHQDVqsVAK7ZTE5EVeuvv7O//u5uFZMbIqKbwEuERNWvqv7OmNwQERGRS2FyQ0RERC6FyQ0REVENFx0djZkzZ1b5enfu3IkmTZpAoVCgX79+N7VMly5dMGbMmBvWqa54bxaTGyIiF7Zr1y7IZDL06tXL2aHQTVi0aBF8fHzu2PbGjRuH5s2bIy0tDYsWLbpj261uTG6IahCZTIZHH30Ujz76aIVJNIlu1cKFC/Hyyy9jx44dyMjIqNZtWa1Wlxvo0Gw2OzuEanXu3Dnce++9qFOnzh1NqqobkxuiGkStVmPlypVYuXIl1Gq1s8OhWk6n02HFihV48cUXcf/991f4Zd6uXTu89tprFern5eVBoVBg8+bNAMpHZp4wYQLCw8Ph7u6ONm3aVJgO569WhrVr16JRo0ZQqVS4cOEC9u/fj+7duyMgIADe3t7o3LkzDh48WGFbJ0+exD333AO1Wo1GjRph48aNkEgk+PHHH+11Ll26hAEDBsDX1xf+/v546KGHkJ6eft39tVqtGDZsGGJiYuDm5oYGDRpg1qxZleotXLgQjRs3hkqlQmhoKEaOHGl/TyKRYO7cuXjooYfg7u6OadOmAQA+//xz1K1bF0qlEg0aNMCSJUsqrHPKlCmIjIyESqVCWFgYRo0aZX9vzpw5qFevHtRqNYKDg/Hoo49eM/4tW7ZgyJAhKC4uhkQigUQiwZQpU+zv6/V6DB06FJ6enoiMjMS8efMqLO/I8UpPT4dEIkFBQQGGDh0KiURiPz+2bt2K1q1b24/Pa6+9BovFct3jnpubiwceeABubm6IiYnB0qVLK9W50fGpFuIuU1xcLACI4uJiZ4dCRLVAWVmZSElJEWVlZfayxMREER4efscfiYmJDsW+YMECkZSUJIQQYs2aNSI6OlrYbDYhhBCffvqpiIyMtL/+qyw8PFxYrVYhhBADBw4U7du3F9u2bRNnz54V77//vlCpVOL06dNCCCG++uoroVAoRPv27cXOnTvFyZMnhVarFX/88YdYsmSJSElJESkpKWLYsGEiODhYlJSUCCGEsFqtokGDBqJ79+7i8OHDYvv27aJ169YCgFi9erUQQgidTifq1asnhg4dKo4ePSpSUlLEwIEDRYMGDYTRaLzm/ppMJjF58mSxb98+cf78efHNN98IjUYjli9fbq8zZ84coVarxcyZM8WpU6fEvn37xMcff2x/H4AICgoSCxYsEOfOnRPp6eli1apVQqFQiM8++0ycOnVKfPjhh0Imk4lNmzYJIYRYuXKl8PLyEuvXrxcXLlwQe/fuFfPmzRNCCLF//34hk8nEt99+K9LT08XBgwfFrFmzrhm/0WgUM2fOFF5eXiI7O1tkZ2eL0tJSIYQQUVFRws/PT3z22WfizJkzYvr06UIqlYrU1NRbOl4Wi0VkZ2cLLy8vMXPmTJGdnS30er24ePGi0Gg0YsSIESI1NVWsXr1aBAQEiDfffNO+bOfOncXo0aPtr3v37i0SEhLErl27xIEDB0T79u2Fm5ub/bje6Pj807X+3v7iyPc3kxsiohu41odteHi4AHDHH+Hh4Q7F3r59ezFz5kwhhBBms1kEBASIDRs2CCGEyM3NFXK5XGzbts1ev127duKVV14RQghx9uxZIZFIxKVLlyqs87777hMTJ04UQpQnNwDE4cOHbxiHxWIRnp6eYs2aNUIIIX755Rchl8tFdna2vc6GDRsqJDcLFiwQDRo0qJB8GY1G4ebmJn777bebPgYjRowQjzzyiP11WFiYmDRp0nXrAxBjxoypUNa+fXvx3HPPVSh77LHHRJ8+fYQQQnz44Yeifv36wmQyVVrfDz/8ILy8vOyJ3b/56quvhLe3d6XyqKgo8fTTT9tf22w2ERQUJD7//HMhxK0fL29vb/HVV1/ZX7/++uuV1vPZZ58JDw8Pe9L79+Tm1KlTAoDYs2ePvX5qaqoAYE9ubnR8/qmqkhtOv0BUg+h0Ok6/UAuEhITU+O2eOnUK+/btw6pVqwAAcrkcAwYMwMKFC9GtWzcEBgaie/fuWLp0KTp27Ii0tDTs3r0bn3/+OQDg4MGDEEKgfv36FdZrNBrh7+9vf61UKtG0adMKdXJzczF58mRs2rQJOTk5sFqt0Ov19j4/p06dQkRERIX9ad26dYV1JCcn4+zZs/D09KxQbjAYcO7cuevu99y5czF//nxcuHABZWVlMJlMaN68uT2urKws3HfffTc8dklJSRVep6am4vnnn69Q1qFDB/slr8ceewwzZ85EbGwsevXqhT59+uCBBx6AXC5H9+7dERUVZX+vV69e6N+//y1N5fH34yyRSBASEoLc3FwAt368/ik1NRXt2rWrMJhehw4doNVqcfHiRURGRlaqL5fLKxyzhg0bVui/c6PjU12Y3BAROejAgQPODuFfLViwABaLBeHh4fYyIQQUCgWuXLkCX19fPPXUUxg9ejQ+/fRTfPvtt2jcuDGaNWsGoHwSQ5lMhuTk5Eqd2/9KwAHAzc2t0qiygwcPRl5eHmbOnImoqCioVCq0a9cOJpPJHse/jURrs9mQmJh4zf4bgYGB11xmxYoVGDt2LD788EO0a9cOnp6eeP/997F37157rDfjWj8q/hnv3/chIiICp06dwoYNG7Bx40aMGDEC77//PrZu3QpPT08cPHgQW7Zswe+//47JkydjypQp2L9/v8MdeP8515JEIrF34L6V43Ut1/q/EULYt3et+td77y83Oj63M3/UjbBDMRGRi7FYLFi8eDE+/PBDHD582P44cuQIoqKi7F+A/fr1g8FgwK+//opvv/0WTz/9tH0dLVq0gNVqRW5uLuLi4io8/q0Fafv27Rg1ahT69Olj77ibn59vf79hw4bIyMhATk6OvWz//v0V1tGyZUucOXMGQUFBlbbv7e193e22b98eI0aMQIsWLRAXF1eh1cLT0xPR0dH4448/bv5gAoiPj8eOHTsqlO3atQvx8fH2125ubnjwwQfxySefYMuWLdi9ezeOHTsGoLzVrFu3bpgxYwaOHj2K9PR0bNq06ZrbUiqVtzSv0q0cr2tp1KgRdu3aZU9a/tpXT0/PConyX+Lj42GxWCok/KdOnUJRUVGFejc6PtWByQ0RkYtZu3Ytrly5gmHDhiEhIaHC49FHH8WCBQsAlLdQPPTQQ3jjjTeQmpqKgQMH2tdRv359PPXUUxg0aBBWrVqFtLQ07N+/H//73/+wfv36G24/Li4OS5YsQWpqKvbu3YunnnqqQqtJ9+7dUbduXTz77LM4evQodu7ciUmTJgG42gLw1FNPISAgAA899BC2b9+OtLQ0bN26FaNHj8bFixevu90DBw7gt99+w+nTp/HGG29USpqmTJmCDz/8EJ988gnOnDmDgwcP4tNPP73h/rzyyitYtGgR5s6dizNnzuCjjz7CqlWrMH78eADld40tWLAAx48fx/nz57FkyRK4ubkhKioKa9euxSeffILDhw/jwoULWLx4MWw2Gxo0aHDNbUVHR0Or1eKPP/5Afn4+9Hr9DWP7y60cr2sZMWIEMjMz8fLLL+PkyZP46aef8Oabb2LcuHGQSiunDA0aNECvXr3w3HPPYe/evUhOTsZ//vOfCv/fNzo+1eZfe+W4GHYopppMq9XaO49qtVpnh0Pixh0ca6r777/f3tn1n5KTkwUAkZycLIQQYt26dQKA6NSpU6W6f919FB0dLRQKhQgJCRH9+/cXR48eFUJcv/PrwYMHRVJSklCpVKJevXpi5cqVIioqqsJdSampqaJDhw5CqVSKhg0bijVr1ggA4tdff7XXyc7OFoMGDRIBAQFCpVKJ2NhY8dxzz13389tgMIjBgwcLb29v4ePjI1588UXx2muviWbNmlWoN3fuXNGgQQOhUChEaGioePnll+3v4W+dmv9uzpw5IjY2VigUClG/fn2xePFi+3urV68Wbdq0EV5eXsLd3V20bdtWbNy4UQghxPbt20Xnzp2Fr6+vcHNzE02bNq1w99a1DB8+XPj7+wsA9ruU/nn8hBCiWbNmFe5icvR4CVG5Q7EQQmzZskW0atVKKJVKERISIl599VVhNpvt7//zbqns7GzRt29foVKpRGRkpFi8eHGFeG90fP6pqjoUS4T4W9vTXaCkpATe3t4oLi6Gl5eXs8MhqoAdimseg8GAtLQ0xMTEcOyharRz507cc889OHv2LOrWrevscMhJbvT35sj3NzsUExHRHbd69Wp4eHigXr16OHv2LEaPHo0OHTowsaEqweSGqAaRyWTo06eP/TmRqyotLcWECROQmZmJgIAAdOvWDR9++KGzwyIXweSGqAZRq9VYt26ds8MgqnaDBg3CoEGDnB0GuSjeLUVEREQuhckNERERuRQmN0Q1iE6ng7u7O9zd3aHT6ZwdDhFRrcQ+N0Q1zM0O2kVERNfGlhsiIiJyKUxuiIiIyKUwuSEiugulp6dDIpHg8OHDN73MokWLHJ7J+m6xZcsWSCSSShNGknMwuSEiohrp0KFDeOyxxxAcHAy1Wo369evjueeew+nTp5GcnAyJRFJptu6/9OzZEw8++GC1xNWlSxeMGTOmQln79u2RnZ3t0Azct4JJ1M1hckNERDXO2rVr0bZtWxiNRixduhSpqalYsmQJvL298cYbbyAxMRHNmjXDV199VWnZzMxMbNy4EcOGDbtj8SqVSoSEhNhnNSfnYnJDVINIpVJ07twZnTt3hlTKP8+aTKfTXfdhMBhuum5ZWdlN1XXUr7/+invuuQc+Pj7w9/fH/fffj3Pnzl23/l8tAuvWrUOzZs2gVqvRpk0bHDt2rFLd3377DfHx8fDw8ECvXr2QnZ1tf2///v3o3r07AgIC4O3tjc6dO+PgwYMOxa7X6zFkyBD06dMHP//8M7p164aYmBi0adMGH3zwAb744gsAwLBhw7BixYpKx2fRokUIDAxE3759r7uNXbt2oVOnTnBzc0NERARGjRpVYT1z5sxBvXr1oFarERwcjEcffRQAMHjwYGzduhWzZs2CRCKBRCJBenp6pRaVvy7hrV27Fg0aNIBGo8Gjjz4KnU6Hr7/+GtHR0fD19cXLL78Mq9Vq3+4333yDpKQkeHp6IiQkBAMHDkRubi6A8kuJXbt2BQD4+vpCIpFg8ODBAAAhBGbMmIHY2Fi4ubmhWbNm+P777x067i7lX+cNdzGOTJlORFRWViZSUlJEWVlZhXIA13306dOnQl2NRnPdup07d65QNyAg4Jr1HPX999+LH374QZw+fVocOnRIPPDAA6JJkybCarUKIYRIS0sTAMShQ4eEEEJs3rxZABDx8fHi999/F0ePHhX333+/iI6OFiaTSQghxFdffSUUCoXo1q2b2L9/v0hOThbx8fFi4MCB9u3+8ccfYsmSJSIlJUWkpKSIYcOGieDgYFFSUmKv8+yzz1ba779btWqVACB27dp1w30sKCgQKpVKfPXVV/Yym80mYmNjxYQJE6673NGjR4WHh4f4+OOPxenTp8XOnTtFixYtxODBg4UQQuzfv1/IZDLx7bffivT0dHHw4EExa9YsIYQQRUVFol27duK5554T2dnZIjs7W1gsFvvxu3LlSoVj1b17d3Hw4EGxdetW4e/vL3r06CEef/xxceLECbFmzRqhVCrFd999Z49twYIFYv369eLcuXNi9+7dom3btqJ3795CCCEsFov44YcfBABx6tQpkZ2dLYqKioQQQrz++uuiYcOG4tdffxXnzp0TX331lVCpVGLLli03PIY1zfX+3oRw7PubyQ0R0Q3U1uTmn3JzcwUAcezYMSHE9ZObv3/RFhQUCDc3N7F8+XIhRPkXNgBx9uxZe53PPvtMBAcHX3e7FotFeHp6ijVr1tjLXnvtNfHMM89cd5n//e9/AoAoLCz81/0aMGCA6NSpk/31pk2bBABx8uTJ6y7zzDPPiOeff75C2fbt24VUKhVlZWXihx9+EF5eXhUSsr/r3LmzGD16dIWyayU3/zxWL7zwgtBoNKK0tNRe1rNnT/HCCy9cN9Z9+/YJAPZl/rkdIYTQarVCrVZXSgaHDRsmnnzyyeuuuyaqquSGg/gREd0CrVZ73ff+OaP7X5cVruWflx/T09NvK66/nDt3Dm+88Qb27NmD/Px82Gw2AEBGRgYSEhKuu1y7du3sz/38/NCgQQOkpqbayzQaDerWrWt/HRoaWmH/cnNzMXnyZGzatAk5OTmwWq3Q6/XIyMiw15k+ffoNYxdC3PR+Dhs2DD169MDZs2cRFxeHhQsXokOHDmjQoMF1l0lOTsbZs2exdOnSCtu02WxIS0tD9+7dERUVhdjYWPTq1Qu9evVC//79odFobjouoPKxCg4ORnR0NDw8PCqU/f34HTp0CFOmTMHhw4dRWFhY4f+tUaNG19xOSkoKDAYDunfvXqHcZDKhRYsWDsXsKpjcENUgOp0O0dHRAMq/5Nzd3Z0bEF2XI/831VX3Rh544AFERETgyy+/RFhYGGw2GxISEmAymRxe1987ySoUikrv/T0ZGTx4MPLy8jBz5kxERUVBpVKhXbt2Dm23fv36AICTJ09WSLaupVu3boiKisKiRYswYcIErFq1CrNnz77hMjabDS+88AJGjRpV6b3IyEgolUocPHgQW7Zswe+//47JkydjypQp2L9/v0O3wl/rWF2r7K8ERqfToUePHujRowe++eYbBAYGIiMjAz179rzh8ftr+XXr1iE8PLzCeyqV6qbjdSVMbohqmPz8fGeHQLVcQUEBUlNT8cUXX6Bjx44AcN1bpv9pz549iIyMBABcuXIFp0+fRsOGDW9629u3b8ecOXPQp08fAOV3Ljl6Tvfo0QMBAQGYMWMGVq9eXen9oqIie5IhkUgwZMgQzJ8/H3Xq1IFUKsXjjz9+w/W3bNkSJ06cQFxc3HXryOVydOvWDd26dcObb74JHx8fbNq0CQ8//DCUSmWFTsBV5eTJk8jPz8d7772HiIgIAMCBAwcq1FEqlQBQYfuNGjWCSqVCRkYGOnfuXOVx1UZMboiIXIyvry/8/f0xb948hIaGIiMjA6+99tpNLTt16lT4+/sjODgYkyZNQkBAAPr163fT246Li8OSJUuQlJSEkpISvPLKK3Bzc6tQZ+LEibh06RIWL158zXW4u7tj/vz5eOyxx/Dggw9i1KhRiIuLQ35+PlasWIGMjAx899139vpDhgzB1KlT8frrr+OJJ57419avV199FW3btsVLL72E5557Du7u7khNTcWGDRvw6aefYu3atTh//jw6deoEX19frF+/HjabzX6pKzo6Gnv37kV6ejo8PDzg5+d308fnRv5qNfr0008xfPhwHD9+HG+//XaFOlFRUZBIJFi7di369OkDNzc3eHp6Yvz48Rg7dixsNhvuuecelJSUYNeuXfDw8MCzzz5bJfHVJrzXlIjIxUilUnz33XdITk5GQkICxo4di/fff/+mln3vvfcwevRoJCYmIjs7Gz///LO9teBmLFy4EFeuXEGLFi3wzDPPYNSoUQgKCqpQJzs7u0IfnGt56KGHsGvXLigUCgwcOBANGzbEk08+ieLiYkybNq1C3cjISHTr1g1XrlzB0KFD/zXGpk2bYuvWrThz5gw6duyIFi1a4I033kBoaCgAwMfHB6tWrcK9996L+Ph4zJ07F8uWLUPjxo0BAOPHj4dMJkOjRo3sl46qQmBgIBYtWoSVK1eiUaNGeO+99/DBBx9UqBMeHo633noLr732GoKDgzFy5EgAwNtvv43Jkydj+vTpiI+PR8+ePbFmzRrExMRUSWy1jUQ40nPLBZSUlMDb2xvFxcXw8vJydjhEFeh0OntnQ61Wyz43NYDBYEBaWhpiYmKgVqudHU612bJlC7p27YorV65wigVymhv9vTny/c2WGyIiInIpTG6IiIjIpbBDMVENIpVKkZSUZH9OdKd06dLFofFliGoyJjdENYibmxv279/v7DCIiGo1/jQkIroJbNUgqn5V9XfG5IaI6Ab+mkrhVkb2JSLH/PV39s8pTBzFy1JENYher7fPH5OSkuLwXDZU9eRyOTQaDfLy8qBQKNgXiqia2Gw25OXlQaPRQC6/vfSEyQ1RDSKEwIULF+zPyfkkEglCQ0ORlpZm/78houohlUoRGRlZYT6zW8HkhojoXyiVStSrV4+XpoiqmVKprJLWUSY3REQ3QSqVuvQIxUSuhBePiYiIyKUwuSEiIiKXwuSGiIiIXAr73BDVIBKJxH4r+O3eLUBEdLdickNUg2g0Gpw4ccLZYRAR1Wq8LEVEREQuhckNERERuRQmN0Q1iF6vR+PGjdG4cWPo9Xpnh0NEVCuxzw1RDSKEQEpKiv05ERE5ji03RERE5FKY3BAREZFLYXJDRERELoXJDREREbkUJjdERETkUpye3MyZMwcxMTFQq9VITEzE9u3bb1h/6dKlaNasGTQaDUJDQzFkyBAUFBTcoWiJqpdEIkFUVBSioqI4/QIR0S1yanKzfPlyjBkzBpMmTcKhQ4fQsWNH9O7dGxkZGdesv2PHDgwaNAjDhg3DiRMnsHLlSuzfvx//+c9/7nDkRNVDo9EgPT0d6enp0Gg0zg6HiKhWcmpy89FHH2HYsGH4z3/+g/j4eMycORMRERH4/PPPr1l/z549iI6OxqhRoxATE4N77rkHL7zwAg4cOHDdbRiNRpSUlFR4EBERketyWnJjMpmQnJyMHj16VCjv0aMHdu3adc1l2rdvj4sXL2L9+vUQQiAnJwfff/89+vbte93tTJ8+Hd7e3vZHREREle4HERER1SxOS27y8/NhtVoRHBxcoTw4OBiXL1++5jLt27fH0qVLMWDAACiVSoSEhMDHxweffvrpdbczceJEFBcX2x+ZmZlVuh9EVamsrAytWrVCq1atUFZW5uxwiIhqJad3KP5np0khxHU7UqakpGDUqFGYPHkykpOT8euvvyItLQ3Dhw+/7vpVKhW8vLwqPIhqKpvNhgMHDuDAgQOw2WzODoeIqFZy2txSAQEBkMlklVppcnNzK7Xm/GX69Ono0KEDXnnlFQBA06ZN4e7ujo4dO2LatGkIDQ2t9riJiIioZnNay41SqURiYiI2bNhQoXzDhg1o3779NZfR6/WQSiuGLJPJAHCSQSIiIirn1MtS48aNw/z587Fw4UKkpqZi7NixyMjIsF9mmjhxIgYNGmSv/8ADD2DVqlX4/PPPcf78eezcuROjRo1C69atERYW5qzdICIiohrEaZelAGDAgAEoKCjA1KlTkZ2djYSEBKxfvx5RUVEAgOzs7Apj3gwePBilpaWYPXs2/u///g8+Pj6499578b///c9Zu0BEREQ1jETcZddzSkpK4O3tjeLiYnYuphpHp9PBw8MDAKDVauHu7u7kiIiIagZHvr+d2nJDRJUFBAQ4OwQiolqNyQ1RDeLu7o68vDxnh0FEVKs5fZwbIiIioqrE5IaIiIhcCpMbohqkrKwMXbp0QZcuXTj9AhHRLWKfG6IaxGazYevWrfbnRETkOLbcEBERkUthckNEREQuhckNERERuRQmN0RERORSmNwQERGRS+HdUkQ1jEajcXYIRES1GpMbohrE3d0dOp3O2WEQEdVqvCxFRERELoXJDREREbkUJjdENYjBYEDfvn3Rt29fGAwGZ4dDRFQrsc8NUQ1itVqxfv16+3MiInIcW26IiIjIpTC5ISIiIpfC5IaIiIhcCpMbIiIicilMboiIiMilMLkhIiIil8JbwYlqEHd3dwghnB0GEVGtxpYbIiIicilMboiIiMilMLkhqkEMBgMee+wxPPbYY5x+gYjoFknEXXaBv6SkBN7e3iguLoaXl5ezwyGqQKfTwcPDAwCg1Wrh7u7u5IiIiGoGR76/2XJDRERELoXJDREREbkUJjdERETkUpjcEBERkUthckNEREQuhckNERERuRROv0BUg2g0Gmi1WvtzIiJyHJMbohpEIpFwbBsiotvEy1JERETkUpjcENUgRqMRgwcPxuDBg2E0Gp0dDhFRrcTpF4hqEE6/QER0bZx+gYiIiO5aTG6IiIjIpTiU3JjNZnTt2hWnT5+urniIiIiIbotDyY1CocDx48chkUiqKx4iIiKi2+LwZalBgwZhwYIF1RELERER0W1zeBA/k8mE+fPnY8OGDUhKSqp0N8dHH31UZcEREREROcrh5Ob48eNo2bIlAFTqe8PLVUS3R6PRIDc31/6ciIgc53Bys3nz5uqIg4hQ/gMhMDDQ2WEQEdVqt3Ur+MWLF3Hp0qWqioWIiIjotjmc3NhsNkydOhXe3t6IiopCZGQkfHx88Pbbb8Nms1VHjER3DaPRiJdeegkvvfQSp18gIrpFDl+WmjRpEhYsWID33nsPHTp0gBACO3fuxJQpU2AwGPDOO+9UR5xEdwWLxYI5c+YAAGbMmAGVSuXkiIiIah+Hk5uvv/4a8+fPx4MPPmgva9asGcLDwzFixAgmN0RERORUDl+WKiwsRMOGDSuVN2zYEIWFhVUSFBEREdGtcji5adasGWbPnl2pfPbs2WjWrFmVBEVERER0qxy+LDVjxgz07dsXGzduRLt27SCRSLBr1y5kZmZi/fr11REjERER0U1zuOWmc+fOOH36NPr374+ioiIUFhbi4YcfxqlTp9CxY8fqiJGIiIjopjnUcmM2m9GjRw988cUX7DhMRERENZJDyQ1nBSeqXm5ubkhLS7M/JyIix3FWcKIaRCqVIjo6GtHR0ZBKb2sAcSKiuxZnBSciIiKXwlnBiWoQk8mESZMmAQDeeecdKJVKJ0dERFT7SIQQ4mYrW61W7NixA02aNIGfn191xlVtSkpK4O3tjeLiYnh5eTk7HKIKdDodPDw8AABarbZSyygR0d3Kke9vhy7qy2Qy9OzZE8XFxbcVIBEREVF1cbjHYpMmTXD+/PnqiIWIiIjotjmc3LzzzjsYP3481q5di+zsbJSUlFR4OGrOnDmIiYmBWq1GYmIitm/ffsP6RqMRkyZNQlRUFFQqFerWrYuFCxc6vF0iIiJyTQ53KO7VqxcA4MEHH6zQgVgIAYlEAqvVetPrWr58OcaMGYM5c+agQ4cO+OKLL9C7d2+kpKQgMjLymss8/vjjyMnJwYIFCxAXF4fc3FxYLBZHd4OIiIhclEMdigFg69atN3y/c+fON72uNm3aoGXLlvj888/tZfHx8ejXrx+mT59eqf6vv/6KJ554AufPn7/pDs1GoxFGo9H+uqSkBBEREexQTDUSOxQTEV2bIx2KHW65cSR5uRGTyYTk5GS89tprFcp79OiBXbt2XXOZn3/+GUlJSZgxYwaWLFkCd3d3PPjgg3j77bevO5rr9OnT8dZbb1VJzERERFTz3dIQqNu3b8fTTz+N9u3b49KlSwCAJUuWYMeOHTe9jvz8fFitVgQHB1coDw4OxuXLl6+5zPnz57Fjxw4cP34cq1evxsyZM/H999/jpZdeuu52Jk6ciOLiYvsjMzPzpmMkutPc3Nxw/PhxHD9+nNMvEBHdIoeTmx9++AE9e/aEm5sbDh48aL/kU1painfffdfhAP458N9ffXeuxWazQSKRYOnSpWjdujX69OmDjz76CIsWLUJZWdk1l1GpVPDy8qrwIKqppFIpGjdujMaNG3P6BSKiW+Twp+e0adMwd+5cfPnll1AoFPby9u3b4+DBgze9noCAAMhkskqtNLm5uZVac/4SGhqK8PBweHt728vi4+MhhMDFixcd3BMiIiJyRQ4nN6dOnUKnTp0qlXt5eaGoqOim16NUKpGYmIgNGzZUKN+wYQPat29/zWU6dOiArKwsaLVae9np06chlUpRp06dm942UU1lMpkwZcoUTJkyBSaTydnhEBHVSg4nN6GhoTh79myl8h07diA2NtahdY0bNw7z58/HwoULkZqairFjxyIjIwPDhw8HUN5fZtCgQfb6AwcOhL+/P4YMGYKUlBRs27YNr7zyCoYOHcr+CeQSzGYz3nrrLbz11lswm83ODoeIqFZy+G6pF154AaNHj8bChQshkUiQlZWF3bt3Y/z48Zg8ebJD6xowYAAKCgowdepUZGdnIyEhAevXr0dUVBQAIDs7GxkZGfb6Hh4e2LBhA15++WUkJSXB398fjz/+OKZNm+bobhAREZGLcnicGwCYNGkSPv74YxgMBgDlnXbHjx+Pt99+u8oDrGqcOJNqMo5zQ0R0bY58f99ScgMAer0eKSkpsNlsaNSokf0DuaZjckM1GZMbIqJrq9ZB/P6i0WiQlJR0q4sTERERVQsOpEFEREQuhckNERERuZRbvixFRFVPrVZj37599udEROQ4JjdENYhMJkOrVq2cHQYRUa12S5ellixZgg4dOiAsLAwXLlwAAMycORM//fRTlQZHRERE5CiHk5vPP/8c48aNQ58+fVBUVASr1QoA8PHxwcyZM6s6PqK7islkwvvvv4/333+f0y8QEd0ih8e5adSoEd59913069cPnp6eOHLkCGJjY3H8+HF06dIF+fn51RVrleA4N1STcZwbIqJrc+T72+GWm7S0NLRo0aJSuUqlgk6nc3R1RERERFXK4eQmJiYGhw8frlT+yy+/oFGjRlURExEREdEtc/huqVdeeQUvvfQSDAYDhBDYt28fli1bhunTp2P+/PnVESMRERHRTXM4uRkyZAgsFgsmTJgAvV6PgQMHIjw8HLNmzcITTzxRHTESERER3TSHkhuLxYKlS5figQcewHPPPYf8/HzYbDYEBQVVV3xEREREDnGoz41cLseLL74Io9EIAAgICGBiQ0RERDWKw5el2rRpg0OHDiEqKqo64iG6q6nVamzevNn+nIiIHOdwcjNixAj83//9Hy5evIjExMRK43A0bdq0yoIjutvIZDJ06dLF2WEQEdVqDg/iJ5VWvpIlkUgghIBEIrGPWFxTcRA/IiKi2seR72+HW27S0tJuOTAiujGz2Yx58+YBAJ5//nkoFAonR0REVPs43HJT27HlhmoyTr9ARHRt1dpy85eUlBRkZGRUmtzvwQcfvNVVEhEREd02h5Ob8+fPo3///jh27Ji9rw1Q3u8GQI3vc0NERESuzeG5pUaPHo2YmBjk5ORAo9HgxIkT2LZtG5KSkrBly5ZqCJGIiIjo5jnccrN7925s2rQJgYGBkEqlkEqluOeeezB9+nSMGjUKhw4dqo44iYiIiG6Kwy03VqvV3uExICAAWVlZAICoqCicOnWqaqMjIiIicpDDLTcJCQk4evQoYmNj0aZNG8yYMQNKpRLz5s1DbGxsdcRIREREdNMcTm7++9//QqfTAQCmTZuG+++/Hx07doS/vz+WL19e5QES3U1UKhXWrl1rf05ERI6rknFuCgsL4evra79jqibjODdERES1zx0Z5+bv/Pz8qmI1RERERLfN4eSma9euN2yh2bRp020FRHQ3M5vNWLp0KQDgqaee4vQLRES3wOHkpnnz5hVem81mHD58GMePH8ezzz5bVXER3ZVMJhOGDBkCAHjssceY3BAR3QKHk5uPP/74muVTpkyBVqu97YCIiIiIbofD49xcz9NPP42FCxdW1eqIiIiIbkmVJTe7d++GWq2uqtURERER3RKHL0s9/PDDFV4LIZCdnY0DBw7gjTfeqLLAiIiIiG6Fw8mNt7d3hddSqRQNGjTA1KlT0aNHjyoLjIiIiOhWOJzcfPXVV9URBxEREVGVqJJB/IioaqhUKqxYscL+nIiIHOdwcuPINAuFhYUOB0R0N5PL5XjsscecHQYRUa3mcHLzxhtvYNq0aejZsyfatWsHoPxOqd9++w1vvPEGp2IgIiIip3J44sxHHnkEXbt2xciRIyuUz549Gxs3bsSPP/5YlfFVOU6cSTWZxWLB6tWrAQD9+/eHXM4rx0REgGPf3w4nNx4eHjh8+DDi4uIqlJ85cwYtWrSo8aMUM7mhmkyn08HDwwMAoNVq4e7u7uSIiIhqBke+vx0exM/f39/+y/LvfvzxR/j7+zu6OiIiIqIq5XCb91tvvYVhw4Zhy5Yt9j43e/bswa+//or58+dXeYBEREREjnA4uRk8eDDi4+PxySefYNWqVRBCoFGjRti5cyfatGlTHTESERER3bRb6q3Ypk0bLF26tKpjISIiIrptDve5OXjwII4dO2Z//dNPP6Ffv354/fXXYTKZqjQ4IiIiIkc5nNy88MILOH36NADg/PnzGDBgADQaDVauXIkJEyZUeYBEREREjnD4stTp06fRvHlzAMDKlSvRuXNnfPvtt9i5cyeeeOIJzJw5s4pDJLp7KJVK+/xtSqXSydEQEdVODic3QgjYbDYAwMaNG3H//fcDACIiIpCfn1+10RHdZRQKBQYPHuzsMIiIajWHL0slJSVh2rRpWLJkCbZu3Yq+ffsCANLS0hAcHFzlARIRERE5wuHkZubMmTh48CBGjhyJSZMm2Ucq/v7779G+ffsqD5DobmKxWLBu3TqsW7cOFovF2eEQEdVKDk+/cD0GgwEymQwKhaIqVldtOP0C1WScfoGI6Noc+f6usln51Gp1Va2KiIiI6JY5fFmKiIiIqCZjckNEREQuhckNERERuRQmN0RERORSHO5QbLVasWjRIvzxxx/Izc21D+j3l02bNlVZcERERESOcji5GT16NBYtWoS+ffsiISEBEomkOuIiuisplUrMnj3b/pyIiBzn8Dg3AQEBWLx4Mfr06VNdMVUrjnNDRERU+zjy/e1wnxulUmkflZiIiIiopnE4ufm///s/zJo1C1U0sDER/Y3VasWWLVuwZcsWWK1WZ4dDRFQrOdznZseOHdi8eTN++eUXNG7cuNJ0C6tWrXJofXPmzMH777+P7OxsNG7cGDNnzkTHjh3/dbmdO3eic+fOSEhIwOHDhx3aJlFNZTAY0LVrVwCcfoGI6FY5nNz4+Pigf//+VbLx5cuXY8yYMZgzZw46dOiAL774Ar1790ZKSgoiIyOvu1xxcTEGDRqE++67Dzk5OVUSCxEREbmGKps481a0adMGLVu2xOeff24vi4+PR79+/TB9+vTrLvfEE0+gXr16kMlk+PHHH2/YcmM0GmE0Gu2vS0pKEBERwQ7FVCNx4kwiomur1g7FVcVkMiE5ORk9evSoUN6jRw/s2rXrust99dVXOHfuHN58882b2s706dPh7e1tf0RERNxW3ERERFSz3dKs4N9//z1WrFiBjIwMmEymCu8dPHjwptaRn58Pq9WK4ODgCuXBwcG4fPnyNZc5c+YMXnvtNWzfvh1y+c2FPnHiRIwbN87++q+WGyIiInJNDrfcfPLJJxgyZAiCgoJw6NAhtG7dGv7+/jh//jx69+7tcAD/HARQCHHNgQGtVisGDhyIt956C/Xr17/p9atUKnh5eVV4EBERketyuOVmzpw5mDdvHp588kl8/fXXmDBhAmJjYzF58mQUFhbe9HoCAgIgk8kqtdLk5uZWas0BgNLSUhw4cACHDh3CyJEjAQA2mw1CCMjlcvz++++49957Hd0dIiIicjEOJzcZGRlo3749AMDNzQ2lpaUAgGeeeQZt27a1Dx3/b5RKJRITE7Fhw4YKd19t2LABDz30UKX6Xl5eOHbsWIWyOXPmYNOmTfj+++8RExPj6K4Q1TgKhQIzZsywPyciIsc5nNyEhISgoKAAUVFRiIqKwp49e9CsWTOkpaU5PLDfuHHj8MwzzyApKQnt2rXDvHnzkJGRgeHDhwMo7y9z6dIlLF68GFKpFAkJCRWWDwoKglqtrlROVFsplUq88sorzg6DiKhWczi5uffee7FmzRq0bNkSw4YNw9ixY/H999/jwIEDePjhhx1a14ABA1BQUICpU6ciOzsbCQkJWL9+PaKiogAA2dnZyMjIcDREIiIiuos5PM6NzWaDzWaz3620YsUK7NixA3FxcRg+fHiNn8mYE2dSTWa1Wu13HLZs2RIymczJERER1QyOfH87dRA/Z2ByQzUZB/EjIrq2ah/Eb/v27Xj66afRrl07XLp0CQCwZMkS7Nix41ZWR0RERFRlHE5ufvjhB/Ts2RNubm44dOiQfWqD0tJSvPvuu1UeIBEREZEjHE5upk2bhrlz5+LLL7+scKtq+/btb3p0YiIiIqLq4nByc+rUKXTq1KlSuZeXF4qKiqoiJiIiIqJb5nByExoairNnz1Yq37FjB2JjY6skKCIiIqJb5XBy88ILL2D06NHYu3cvJBIJsrKysHTpUowfPx4jRoyojhiJiIiIbprDg/hNmDABxcXF6Nq1KwwGAzp16gSVSoXx48fb53wiolujUCjw5ptv2p8TEZHjbnmcG71ej5SUFNhsNjRq1Mg+NkdNx3FuiIiIah9Hvr8dbrn5i0ajQVJS0q0uTkRERFQtbjq5GTp06E3VW7hw4S0HQ3S3s9lsSE1NBQDEx8dDKr2lcTaJiO5qN53cLFq0CFFRUWjRooXDs38T0c0pKyuzz3LP6ReIiG7NTSc3w4cPx3fffYfz589j6NChePrpp+Hn51edsRERERE57KbbvOfMmYPs7Gy8+uqrWLNmDSIiIvD444/jt99+Y0sOERER1RgOXdBXqVR48sknsWHDBqSkpKBx48YYMWIEoqKioNVqqytGIiIiopt2y70VJRIJJBIJhBCw2WxVGRMRERHRLXMouTEajVi2bBm6d++OBg0a4NixY5g9ezYyMjJqzTg3RERE5NpuukPxiBEj8N133yEyMhJDhgzBd999B39//+qMjYiIiMhhN53czJ07F5GRkYiJicHWrVuxdevWa9ZbtWpVlQVHdLdRKBQYP368/TkRETnuppObQYMGQSKRVGcsRHc9pVKJ999/39lhEBHVag4N4kdERERU093y3FJEVPVsNhsyMjIAAJGRkZx+gYjoFjC5IapBysrKEBMTA4DTLxAR3Sr+LCQiIiKXwuSGiIiIXAqTGyIiInIpTG6IiIjIpTC5ISIiIpfC5IaIiIhcCm8FJ6pB5HI5RowYYX9ORESO46cnUQ2iUqnw2WefOTsMIqJajZeliIiIyKWw5YaoBhFCID8/HwAQEBDAyWqJiG4BkxuiGkSv1yMoKAgAp18gIrpVvCxFRERELoXJDREREbkUJjdERETkUpjcEBERkUthckNEREQuhckNERERuRTeCk5Ug8jlcjz77LP250RE5Dh+ehLVICqVCosWLXJ2GEREtRovSxEREZFLYcsNUQ0ihIBerwcAaDQaTr9ARHQL2HJDVIPo9Xp4eHjAw8PDnuQQEZFjmNwQERGRS2FyQ0RERFVKCOHU7TO5ISIioiphNBrxWP9+ePPNN50aB5MbIiIium2rFs5HRPN4fP/jT3j77bfx008/OS0W3i1FREREt2X37t14dswYaEt1AACZTIaSkhKnxcOWGyIiIrolxkIdvvjiC3Tu3Nme2Hj5+uLjF57BM88847S42HJDVIPIZDI8+uij9udERDWREAJr3l2IN9fMwOG9p+3lraMa4ZOB/4fECQOdGB0gEc7u0nyHlZSUwNvbG8XFxfDy8nJ2OERERLXO8SPH0KVvVxRcKrCXjRkzBu9NeQdKTzdIpFU/AKkj39+8LEVEREQ3RQiBhQsXonW7NvbERqZS4cEnHsfHH38MlbemWhIbR/GyFBEREf2r08kn8OTgB3Hw+Hl7WUxwFN55fBCe/GSqEyOrjC03RDWITqeDRCKBRCKBTqdzdjhERACAqePHo2WveyokNs8//zySd+2ucYkNwJYbIiIiug6dTofXXnsNs2fPtpcp3NT4ZtHXePzxx50Y2Y2x5YaIiIgqmfrWf9Ggfr0KiU1U/XpY982iGp3YAGy5ISIior/JSj+LAQOexo59e+1lbm5ueGfqNIweNwZSac1vF2FyQ0RERBBC4Ouvv8b4ceNQcOWKvTwyMhQbNmxB/fr1nRidY5jcEBER3cUsWhNmzZiEmYu/w8ULF+3lCjcVOnZti99//qPWDSrK5IaIiOgulbLtAMa8PB4bjm6tUP744+Xj1oSFhTkpstvD5IaoBpHJZOjTp4/9ORFRVRI2AWupEVdMpXjnnXcwZ84cmM1m+/s+gd5Y/s0K9OjRw4lR3j6n9wqaM2cOYmJioFarkZiYiO3bt1+37qpVq9C9e3cEBgbCy8sL7dq1w2+//XYHoyWqXmq1GuvWrcO6deugVqudHQ4RuZCyc3nYPfYbPNL7cYRF1MGsWbPsiY2buxq9+rRCxumMWp/YAE5ObpYvX44xY8Zg0qRJOHToEDp27IjevXsjIyPjmvW3bduG7t27Y/369UhOTkbXrl3xwAMP4NChQ3c4ciIioppNWGywak0AgDNnzmDk1FfR5bNh+Hn3GliM5eUajQb//e9/kX3pMn5Ztw+ePq4x56JTJ85s06YNWrZsic8//9xeFh8fj379+mH69Ok3tY7GjRtjwIABmDx58jXfNxqNMBqN9tclJSWIiIjgxJlEROSySpMvoGDVafx+aRve2/8TLpw8AZvNZn9fqlQgsXUcfly+sdb0q3Fk4kyn9bkxmUxITk7Ga6+9VqG8R48e2LVr102tw2azobS0FH5+ftetM336dLz11lu3FSvRnaLT6RAUFAQAyM3Nhbu7u5MjIqKaTtgETJmlkHkrIPdxg9FoxHeb12LmwllIuXymQl0vLy90btcCPRu2xPD3/geZWuGkqKuX05Kb/Px8WK1WBAcHVygPDg7G5cuXb2odH374IXQ63Q1HSpw4cSLGjRtnf/1Xyw1RTaXX650dAhHVIrnf7oP5uAnbSnZi1uF1SD93Grm5uRXquHtr8PqESXjppZfg7e3tpEjvHKffLSWRVJwaXQhRqexali1bhilTpuCnn36y/9K9FpVKBZVKddtxEhEROZOwCZRuuwjD6QJ4PB4MjU8o9Ho9Vp/ehC+XLcPBjGOVlgkL90fHxGh0i7kXw16bCIn0379fXYHTkpuAgADIZLJKrTS5ubmVWnP+afny5Rg2bBhWrlyJbt26VWeYREREd5wwW2HKLIXNYIVbI38AgEQqQdHOVJivyPDJ0GVYfu4XZJ87D51OV2FZqUyKRx95FKNGjUL79u1h1JZB7alxxm44jdOSG6VSicTERGzYsAH9+/e3l2/YsAEPPfTQdZdbtmwZhg4dimXLlqFv3753IlQiIqJqZdWaIJFKINWU94ExppUgf+FxWJUlKFRvR4Mxk7Fx40Ys3fop1mzfhTJD5cvXfsG+aN6qPrq6+WHSsu/srTR3W2IDOPmy1Lhx4/DMM88gKSkJ7dq1w7x585CRkYHhw4cDKO8vc+nSJSxevBhAeWIzaNAgzJo1C23btrW3+ri5ud0V1xCJiMj1XFl1Brp9l+HWXAGf3vGQeXtDGemJLGMGfjp0CKvOb8aZKR/CarZUWtbNTYmBA5/B4MGDEVc3Hsd//AVdhjx+11x+uh6nJjcDBgxAQUEBpk6diuzsbCQkJGD9+vWIiooCAGRnZ1cY8+aLL76AxWLBSy+9hJdeesle/uyzz2LRokV3OnwiIqKbZjNZUfTzOZgvahE4ohmkyvJRyCXe5YnI+bMH8cuLv0LRRIU1a9Zgz57duNZgLR4eHohqHI6WUUFIUgZh1Pz59vdCXnz6juxLTefUcW6cwZH75InutLKyMvTu3RsA8Msvv8DNzc3JERHRrTBl61B2OBcyLyU8OoQDKL9hJmvqTogygfzSz+A/8m341AnBxvW/44tZ07Ev9SKuFOVcc31yXx90io3D2Clv4r777sPRPSnI270XXYcMgHuo/53cNaepFePcEFFlbm5u2LJli7PDICIH6A7kwJRRAo9OdaAIKP9BYsnVo3TrRVit2chZtxV13/0MEokE+hYX8P2PR7D7hAq7+/ZC8cXMCnM7/V3jxo3RrUcPiJIjCC9VITCkGe6//34AQJuuiUDXxDu2j7UNkxsiIqJrEDYBCAGJrHymInOODiV/ZECilMHv0fr2eroDl2FKL0HBhi8Q8p8n4ZGYCGWUJ4p89uOk3oDfT6lx9P6+cNPpsWfPHhgMhmtuTyqVwqtJAzRrEIS3nvwvOvcrvxt474bd8LJI0KBbq+rfaRfB5IaIiO5aNoMFxvQSCLMNmiYB9vKCZSdRdjwffo/Vh6Z5+VhqwiJQdjQfwlaGgj/eRNycbyGRSCCPlyPTuBknfeVYN+t7tG29B4cPJmPT7z/gwqWSG27fN9ALTz72NHr27IlmDeth3bQJMFmVkOqs9jpturernp13YUxuiGoQnU6H6OhoAEB6ejqnXyC6BX8lLLDY4JZwNWEp/j0dhlNX4Nklwp7IWAoNKFh0AlJ3OWSaYqjq1i2vLAVgFcj44iMYY0PR7JXRkAe4oSRiCy5Y9diXkYDtw1/A/fGNcOTIEfzx2zpczCmAsNmwbMn1Y4uOjkanTp1g1aYgys8TwXo1Rn40C1JV+ddx///ORVBkkMtOi3CnMLmpJmXH8yFxk0MZ4WnvEU90M/Lz850dApFTWQrKYC01Qe7nBpmXEgBgLTZCuzsbkEng3T3KXvfK6jMoO1EA794xcE8sHwDWWmS0Jyyq+l6QKv9cxxUjzJe0yJ/3Dby6RsLn4Ych81YB7nrodBdxdv58mB95GfclNYdntwjsM0zC5mN1cfLwacS9PgmmkmIk71uLYykl0OmuAAD+uMF+KBQKhNQJR2BDH8QFeaC11Q//t+QnAMCpE+ew+YMZCIpuApPOCPWfyU1o/fCqPpx3JSY3VchiteGdVXsx+dG2KFxxCsJkw/Km7nAP88TzneJgPFkI/eFcyON84dUq2D7NhKXYCJm7AhK51Ml7QER084RNQFhskMgk9n4pNoMFpotaSKSAKtbHXle7LxvmLB00LYOgiiy/08WUpUXBN6mQauQIHtnCXrdozXkYThbC9+F6cG8dYl9v6ZZMSBQCQncI3g89BIlEAmGywaY1I2/OAug6hCJo6BBIPRSQeFlhyDmP5JfmQTJqPJKaNIZH+zCcyfoQuWoFMvcGoH/nQighsCdsMdb94o2sQjmO7RqFRC9vnDt3DufOnobJfPXy0I3IZDL4h/ggqH4g6nppMHLIO7inZxeo1Wosfmkw8vLzEewXb6/foHFdNPjqi9v9L6DrYHJThS4VlkJ3JR/CbIMy2hvZGYWYezQb4uhFPNbED/LMUpQdzcf+jHxM+DEZQxL9MfHhNsj54ACE2YaNnQLhGeyB/i3CYU4vhvF8MZRRXlDX83X2rhFRLSds5aN+/DW4m7XUBGN6CSRKKdwa+NnrlW67CHOuHh7tw6AM8wAAGDNKULT6LGQ+KgQ829heN3/+MRjPF8PvyYbQNAsEAJhz9MiffwwyHyUChkRD8ed0OoaUQhhOFsJ4/hA8u9SFe8uWkMgksBYaYMkrw+mBzyJ60UIolTLIfFSwmUtw8aP5yKkrR6d3XoPUQwlpRClK0s8hc28+TquUGNS7D7zui0Rq5ru4HBqKvFN10Sc7FxEhgchssRJr17sh60okkqe9h6Htk5CVlYU9mzfhfL4nCst+xKiZQ2E1Vb5Tae2/HEtPbx94R3ohvI4n6pvkGPnuPCQkJKDw0B78NOd9GCwyKAusUKvVAIAer74P+RUT/BuF3tp/HjmMyU0V8lbL0DNKDalShsChCZj45keoJysfOVmj0UAar0Bu9kWsTi+G2QYUFeRCGK34a6ihd7edA6QCPep7w3amCKWbM3EiWIm3TTq8fG8cHk+KwOX/7Qfc5NjR3BfeAW7o3igYlgIDrMVGKALcyptYicjlmTJLYS01QRnlBZl7ef8M08VSaHdnQ+ajqnDpJnfOYZgySxEwNMH+Y8l0SYvCpamQegmYW5bBq1dPAEBZSgFM6SUo/WM5fPq1g3fPboBNwJytgyH9ErLXzESDb+dCqZQBf7Y2Z76/ECXxvmg16XlI3eSQaqwwXbqIlAmbIXnrSTSNjoJb0wDkH/wVxkIFtizPxePNW0Dup4al5RGcTytFnsc9mPXxInz26jD49ovD7vz/wxH3LsjWqpD+w2o0r1cXh22LsfVsCPIMchw/MQ+nduxEYWEhjuzbhswrPigyL8OYuVdg1ZfBYqk4mu+YFd84dHxVKhViY2MBlCIgxgsRbip0bDkEjz7/JAICArB2ZF+cypNAo1ShdevWAABNh3sRe1SPAI034rs1s68rJDIQiHTs/5duD5ObKqRRyOBRVgSgfLCmx3adg1WqxPkGCqiUSkgj3XAp9Rd0vJCJgOh66BnXGVK1HOFvd8Dg/y1ERIkcEomASqWCtY4nLDFy/HIxDxfNEqSknoStUSisRUagyIjXs7OhUslx/K2e0Cfn/JkIqfCGvgRju9fHE60iUPjtSVjUMsy2lkHjocTEPvGwFBshDBacLTPhisWK2EAPhHirnXvgiO4yVp0Zljw9JHIplHU87eUlmzJgKTTAq0sE5H+Ol1KWUoArq85AWccTAYOvtpoUfn8alhw9/J6Mhbq+H6RubrBqzdAn50CgBNl7VqLepHGQSaWAVAIIIO3dJShuEYy2owaU92VRlcKca0PGeh2K406gQ1xjuCeFoOBQMi57t8GhjXl4vpMJmmB3WGMO4uSFUKTEPIAPZi7F1xMGwX9gQ/zx3mgkh3XCxUI1RMpZtG4Uh9ym3+KXrY2RL/PH7v99gZlD+kGr1eI3429IO5CEdMVlfPLkU+gaF4P007/h5AlPXFYYUWrOx45ln6CkuAT5uVnQla2FEDbg42sfx3fX/HTL/wcKjQrePmo0jG6Kho0aICA4FPmn18LT0wtBBjkmLPkdUqkUpRcz8ON/ByO3TINgkwwBAeUdkaOGz4Hk2w0IqlsfwiogkZW3iPV+8f5bjomqDpObKmQ2GBAa1wAAIAwGZEf3hE7mAZV+A6Sy8k7FPvJQWD3ao3FmOjxLc8vr2mx4cuM+aD19cbqhOxRyOZSN/XFq7T5IrRb0VUjQyD0OUrUMQaNa4PVvf0dIoR5SiwQmkwkSlQw2d+BAfj5yrQIGoxE2vQVlx8o7pn6HEqhVMkzsEw/driyUbr2Is4EKvJxXgAm9GuDFznWRv/A4LCoZBl7IgtpDhXUv3wNbkRE2vRnJV3Q4rzeiZaQv4kPLr5ULIWC1Cchl7CdErk/YRIW5ekzZOtj0ZijreNjvcjFd0qLsWD5kfip4tL56+SFv/jGYc/UIGNzYfpnHcPoKriw/BVWcDwL/08ReV38kD5YcPcx+RqiahcPH3w+QADatGUUn0pH86h/o+PZwaJRKKELcobtwETu+uIxk30KMefM/UARpYNak4nxRXZwoa4ZZXyzA5y8+B78n6uG3d2bjpHtTpJ0DAi5dQWyoNzKivsLmzY/CYCrDuneW44NnukCr1WKDYTkuXaiPi6oc/PKfn5AUGYOMM2tx5mQYCjVaFItidPv9a5SVGZCVcQqFptUwWAyYN7cMwmSp1GrSZu70v73aYH+2+xrH+uht/D/JFDK4e6oRERKDsPBQuHn6QFt6HH6eCgTqJXj0lY8QHh6Oo6uWoejiUVwplaFuVAc88vYYAMC6KWU4mZoKhVQOYbYBKik860TCu81oBBUb0OBvg+Y1SYhCk3f/cxvRUnViclOF1J4a/HrmOwxt2Ag60yl4NzwC9yvhCGnuBYtFD7lcg9LYUFgL3OAZGAbPUBsAwFp8BZej+0In90DIlY3Anx2NNdkCdQ0tILPkI7xFGSQyKRQhGjQ7kI7wwEiUeqRBoVBA2SUC2/esgV6rwACJBTidB0mrGPj0q4slmw4iBFoEwwyr1QJIJbDJbTiTfxk+MoEgdzlseguMZ4oAAJkwQmMyQyqVoGRvNrRbLyLH3YxJujJM6NUADYM9kfNxMqxKKXpcyobKXYn9k7rBfL4IpgulOGA2YafFhPsaBqF9XACM54tglUpwxGyGp0aBRqFekFhtEFYByCSQKngn2d9JpVIkJSXZnzuLMFshLAISuRQSRXkcwiZgLTUBApD7XL38adWaYDNYIXWT2y+PCKuAOUcHCEAR5n6183yRAdYSE2ReSsh9ylsMhRAwX9QCABRhHvZfwJYiA6xFRsg8lPZWDAAwnLkCCEAV422PzZyrhzlbC5mv2t5ZFQC0u7MgTFa4tw6F1K38485wtghlR/KgCPeAR9urSUj+V8dh1ZrhP7Ah5P7l29MduIwrq89C3dAPAc80stctWHQC1mIjgkY2t7e8mHP1KN2SCRHjhYxYTzQKKE9krKUm2EpMWDd3LSwd4tC/W0vIPBQQSjPOncjEvJdPI+KxcAzq1BYebUORumQX9q8y4tCaDLwz9T4ERHvjiu/P2JfeGwaRgPemrsbmaQPg83g0fjqyDJfyGmJ/SQnUi5YhKkCDQ7mf4/jBJOgVWTh35DKe2bUNRUVFOJOSjAKDDHqTFl/OL4O5zPTnJfHv7ft136K3/3YW/G5/ttH+7Lj92R84cb3T57bIpFL4+fvD29sbZcYiqHyU8JbLEBvZDjH1IiFRqpGVsh4ajRpBBjMenjQXfn5+2LloHqxFKSjWyhAW1gmPTB1dHufYh3A4ywqlD9C1UxdI5FI0mDgFC/8zFApjPuS4est1m9GToV62CeH1Yysksw+++FC17CtVHyY3VWTTpk2Y9clHyLqUg71rcxAdLYVW9xPcpcH4bKcfhpXVRbduHVEW8Dlie+thPn8f1pakIR6PoESkQN1kK5RXYnAl4hxKy7LgpQmHIU4HyQkbPLxlMAfsAjAAZXnnYA6oB6XZHQ01+2CxlECh8IZHnhxhurrQyI3wV6VCqpbDvXUINAuseEIWCIvXWUilMnj3jMauvRvhmR+MEfIyWJPXQdJsOHwHNMC67/fhYZkVJmkuLGYzJHIpLAobTgPw1wiEyssvaVnyygAABgAwmSGTSlB6ugjabReRIddjgcWCAA8V2sX4I2/eMQDA8yiBFsD56X1QsuUiSv/IwPEgJUYXFmLUffXwUtc4ZE0tnyRuSoCAWSPHvGeSYD6cB+2uS8gOUuN3HynaxvqjU/1AFCw7CWG2YWesBhJPJXo1DoHtQgnKjuZB66tCVqQ7Qr3dEOmvQckfGbAZrbgU5wl4KNAg2BO2y3qUnchHmacCBVEe8NEoEe7jBu3uLNgMFmSGaSA8FKgX7AFpvgH6Y/mweCpgbOgLd5Uc3m4KlG6/CGupCR6tQ+1fvqYsLXR7syH3U8Ozc4T9/Cj+JQ3mvDJ43Rth/zI0XSxF8fo0yPzU9tFO3dzc8PukZTBd0kJy0QDUu7rewuWnIPNWIXBogn29hd+fhvFsEbz7xEDT9K8OnTrkzTsGqYcCIWOv/tIsXHkaZcfz4d0nBh5tyr/ULflluPxRMiRKGcKnXB0o7Mrqs9AfzIV372j7fti0Jlyevg+QAnXe7WivW7IxA7o92fC8L9Lez0OYrMj95BAAIHxaB0Be/kWh3ZUN7baL8OgYDp++seUrsAG5nx0GAIRNbguJpvzLRn8gByUbM+DeNhS+/eLs28tfdAKwCoRObG3vY2ZILUTxL2kV7sQBgOLf0iEMVuTV0QA+KsT6e8CSp4du/2UUZKmwQ1+AXk2iUTfQA+ZsHawlJsz4cid0dbzx1pOJ5T80rAJnj13ClPFn0XFAUwxoFQ25vxqlBVfw5Xu7sNXNgC/f6gO3YA3K5Ok4daIOzh7Kx5sNirBy7CPweSQWGz5Yj5yiUGz9pQDtE/UIr+eLy8HzcCDnOWgsAgtXHkRLPw/kG7OxRr8cV/I8kaU4jzGjlsLP3Q9nT/yMC+mboJdnoUSvR+AXI3HlyhVYrVfv5Fmz9O+fSIftz87hIKqTSqUCpIBco4KbXIo6IdHw9PKEXl8Cg0QHlUqOoDITmvd4HF6enjiy+QeofNwQBBMiEp5Guz7t4OXlhV9nT4RGKoHGUIanvlgPANj61SLk71+MiyXuqBPdDo9MGwMA2Ph/qThy0QClUoIWTZpDopAiavI0LH7uKZi0pRDGqy1HUaO/QPo770Gt8YHNaIFMXn5b+ID/zYRapYLUXWmvG+DriftGMJFxBUxuqsiJEyfw80/rAAAHDhz42zt5AICt2GYvUSgkUKgOQaVWY1lES3h6CgBnoFS644xQ4+TxD1A3KgrZF7+BKugTZOS3xfI9uQj0Ow6z8RhUTRfBuywOa6T5SCzNRl0/bxia7IH7eROUVg0O+m3CgxiHkqxkmD3MQJkEprDjuFx8AqE+CVDiCkymWLhDihzf1ZAqX4K6sSdM86SIEhq4hZ6G0VIE7+5ROPDHRrjnxWCkVA3dibWQtH0Fgc83wfZZuzHV6oZM93MwGw1QRnigxNME/3wlnpUA7kVnICzRkAe6oTRPh+YmOTLUZgibDfjzro2yHB3UNgGrsEAIAZu+/APpQEYprkgEFDIJDKVGmLN0yMwuxhyhh4BAp/qBMKQWQJhs+F9KOrIgcOKtnrBd1kG39zJOS60YZtNhSPtovPlg4/KERWvGy9u0OA8bDr7RHaosLUo3ZSJDZsNAqxZPJNbBe481Q+n2S7AWGvA6dDgCK7ZP6Ar/yzqU/pGBPHcZ+v94CPcnhGD204nQHciBJUePCfvSsFtY8MOI9ogoMkG39zL0vko8dTgd7esF4LU+8TCcL4Y5sxTrYUKqhwwjutRFoMEC4/li2EqM+GXvBUT4atCpfiAsuXqYM0uRcakERjcpYgPdITfbYMnRw2ay4lyeFmqFDOE+brBpzbAWGZFfoIchpxThPm5QWAVsOjMEBM5n5MPNyx2hPm7lrTFGK3SFJTDkquHl5w2pBOX/H1YrdOfToaoTDrlSUd5HA4A5OwemDAFlZGT5F71MAkAgfcMf8EtqBS9fr/LWE4UEmSkncUpxEc3uaQuFFJB6KmAyGLBo4QokdGqPVg0jIdXIIdwl2HL4GA7nnMBzD3dDmKcaMh8VSkq1eHbmj6hfPxJvPNoGUo0CVg3w48FzWHPsJMY+0QYd6wVCEeKOy9kFeO+dP3DZR47vX+8Bma8KBg8j9uxLx28709CyXwxe7NoAbk0CcGbXSWz96BA2qsz45YNeUEZ4okhxFudOR0ObUooPT13AnBf7wueROGyZtQ3+Rd7Yn1MAXT8zPBP8cWLNtziS8xjqCncs+SkZA1pFw3doDH6bvAVWbSTkxlwcPrAX3l4e2GmegxPnu0O4n0Lubi3eM57BlSt5OHjodxTrzdDbCtHhVxXcJB7Iy0lHUekPsP55CafJ7H/7lMm8zU8pQKaQQ+6ugbtShrqRcfD28caV4jyUSQTc5HLUsRjQ6sGB8NS4Yd/aFfB2VyJSfgXRfSYjpmkUDDbgwIK34aMwwcMq8PBn6yGXy/Ht+JHwwUkYDVZ4NhyHbiMfgBACv42+FxeK1ZDYJHh+evmlqawH7sfemS/gvDYIUVYJWrUqn1IgIzQOR4+kwCBVQJhtkCik6DxkMBbs3gGL+TKkf2tJiR49G+emToXSzRM2mw0ySCGRStF2xBsQWj3qxMfY68ZFhyDuy5mVjoW7PydOdmVMbqpIYWHhTdc1mwXMZgP0WgOu5B/62zs6AMDsP2b+Y4mfAQBrX23yt7LfIZFLseq/v8HP1wcSSQ7U6mXQQQPrb3KcWvIkpJIClJUdgAQeOHJZjuxiJRqE1Mf5nG9hFb/CqA3CyZMn0HbfPhiKjyHX+xCELhr7pClokH0I7WO6wxp0AiiKgVVVii3eP+Bp+auwBetwUVoMnTEIEp90nLuyDw2bdELu9weQUdYKUXIgQzsLUlUvBI1tjj9GrkNbmyfu80pFQdEZBHRvgEPJO5GdEYqXJCooL60E8F+EvJKE79/agudMalzyyoPFbIKmaSDOnzqL4mPAKKggs6UDiIfPg3HYseIQHtIpcUSjgww2yCM8YYiWovCkGf1sShituQAaw6NtKI5vPIkEgxQShQUSAPIgDRCjQN4ZPVrZZNCZCwAAmqYBOL39FIINEoTJbFDKpZAHuEHRQI2LqVr4QYKSPzuNuycGI2XjUVwok6AUNsilEsgD3eDe2hf79+XiKMqgUZoBxMOzUx0cXL0H36VokQIbnmoTibBgd3g/UAdr1p7FpNW5aBqiQqf63eDVMxq7lm3Gh7+W4jgEVrzQDkmhXvB7Kg7ffXcIUz/ciob+Cvz6Sg94943B0ZxUfPhbKU78JrB4aGt0jPFH0MvNsGDOFvxvTiGivWTY8nov+NxfF8k5xzF7awmObj2LuU8nomd8MEJeS8JnH63Hx/OKEex2Anvf7Pvn3Sq78dVhf6QevIxpT0rQv0UEQqe2xOy31mLNr57Qbt2KPdMegE/fWOzI/B2bT0XiTHopHpGfwbP3NETAK/FY+N/fcPKoP746ewTbZkTCq0sEdh1YjrPnm6M4W+Brz314/bHO8BsZg7UT9qJtgT/W5hcCjwIe7cOwd+0n0Bf2QHOpDN+u3Y6OYx+G3/MxWD8mE/HCA2kWE6xWCzRNA3Fp2bfI0T6NuhKBTZuO4MWuDaDs6YlD2zIAkwY+0kL8sR3w9wjCzpL5OJvZHcLzNM7uM+BLeRaKirNxION3aE1WlEkK0f8JXwR7xiDz9B/IK/4VNmkedAaByMVjodWWoqSkGFZreaK+7NO//73usT/btnL9TX8u3CyZuzt85XJE1q0LP28PpOfnAz5+CBNG9LzvQQSEBaGk8DIuHNiNULUZflI5Hv5gMby9vfHDy0+jsc8pGEwSuN83FwkPtITZasOB0YkoNMqgs8bg8UmTAABHYsNx7qdZSNdFwrvIZL8jyPS9FEcyFTDJZZDLy79CHv7vVHw/9ink6NVoEJINAJBIJFC0Gwbtj99BKVPZO92GNWmKAt9uUJjOQxMeZN+vJkMnoGDuQrh7eZePDvynR6dOg9VmhYff1dvV4yJDEDd/TqVj0yCxUaUyujsxuaki48ePhyrQDb+W5GOIrgHMUZcwZ+1OpPnUgUlrRPNTqQiIqYMz53bgsk6OYoMMUm0xrPoy+63gjhIWG0pLilFaUvy30vIJ2b5L/u5vZeUjaX7800fXXM/GKW0qlS2evAgSiQQqpQRy5WwImRrSb2U49G5TyOUmmA15MAof6BVS/LBxCxpH1kd+2kaUWSKglcmhvaCFLHM6bNZcnLmcgjJjIDKLC7H/8zPontAdKdlzcTnnPpg0Bpw9vRvNjzwAm60QF0y7Uaarizz//Vi9pxA9mj6Jy8VfId/0FKRqPc4VfAagN1TN3ZHz3Ul4WJuisc9hnLmoQpO6vZCrWIlcw4OoJ7WisPB9AF3hcW8oTv++Eg0MLRHhdRRFOX7wje2AywG/4+Khe9AFChhKZgG4F149o3Bi+09opmuKJr5pEFdOQRnVDEXR+5GxtyGGQQ2D+SsA98GzUx2c/fUnPFgcD7jnQ1mcDkVcAsoiDiFnQygmQI0LxZsB3AtNkwBc/i4NvUvj0VpZBlnuecjCW0DveQRarRueF0ocKzwHvb49GnVuCn1RCf7zxCL4uCmgv3AG0pg2KFMfh7FUisdsMhyRlCfTikANSrRn0EbbCv5yC4rOnYakfgfoJCchKVbjWSHFblE+t43MSwlD8RncU9IGIXIrLh07CklCD2j1x6HM98IYmxS/WMrPR4lcCmRcQNeiCMTKrTi2Yw/6t4iANvs4VAXe6G9VYA0EzFYTFDIllOfPI1ofB7nEhM0bd+CBRkEoyNiPojwtfAwmSLR6bN+3E2qJEiePbUdmkRlaRREO/1qEOJGDooIz2HnuPKzSElhVpRg5cS38lQE4tX8V8koOQLhdQkEq8NSBH6DT5ePC2VMwwQCLMKDerx7wkPvhSu45aE0rYbOVwWQSkE8fWOGyDQCsrNA6ssv+bMOi1df5K9v75795fyvTX6fuzfPy8oKvry905lKYfMOhcnNDY0MpEnv2ga+7Ent37QJ8AxBrK0TfR8chJqkhzGVXkDL3DVglAMoUeHzBmvJ9GjUEan0KcgyeiI9MRIehvQEAW0a0RXJBAOQKKYKCypOI3q9Nxa9TX0BWmReanD+FBLSEQiZFcewDOLn3INwVGnuMzR54DNvXbYLAJdg0V7MN/4HvQfHZB5B5XE021D5+COjxAtR5l9GwY3t7eceHH0XDFi2h9vC096UCgP988Pe+PeVCQgLwxJQJlcq9Q0Ju8SjT3UwibvWbtZYqKSmBt7c3iouL4eVVtc2SG39ch2PLFuKRF99AZJfmWPT7Whw5thslnhr0OSTwyOf/hdlsxsglY7G/Ti/E557ADM+H4dbRFxM//xDrghtCXybw1NaNaPNQb+SkHcWKk3txQl4f/kVpaJCvgleYAvsPpyDfTQFTmRkeV/KglCugLS2CvsxUpftTE0kkgFyugFwuA2CGkCggk0gg5DL4efrAYsiHRaghJFJAKkVoUAikUgm0RZdgsHpASAWESoJ6oVHQFqRCX+YPs0QCg0qC5jExkEolyMlIhd7ghzIVAI0JrWMa4crFXSjMrQuL3IRcXym6xcVALlfg3PH9MJaGoNjDBqu3DvfGNUd++kZczmgEm8yA9GAZ+tWLhlyhwKmDW1FWHIUiTxvMPiXo1SAR+em/4+LZBrBJzTgbKkPfyCB88UX5qKWdEu6H1lsCg08JetZviYLMLcg8EwEhMeJCINAlMhwatRvOHtkEfXEEyjRGlHrq0TG6GYouJyPrvA8gL0OOtw2NAoLhofZA5snd0OuCYVbrUKy2oFlwPWivnENOpgwShQ4lKiu8VZ7wVHki/9IpGI3usCp00EsAf5UfjIZiaEuMsMEEi7DCapUBNsBkMsJmc/GPEokEEjc3eEnlCAwJgodKhgsmAbO3H8KMOnRs2Qq+gX44cngTcnzqw0MuRbviLPR9ZTK8lQIbZn4ApZ8a/lY9uj7zASI6J6AwNxsH3noYp4t9oZHIMHRJecKy/r9jkJdxBPlGd7TtPRIdBvcCAGwe3RUHL7vDR6nEsCWrAACWknwsfnkwrhiApvcNRffnHwYA/LF0IQ7/vAoqjT9GfvV1+T4IgXlvfwhzwWW0fehhJN5bnohkZefg8OZN8A0IQLse3e27rL1SnkS7eXpCJudcR+Rcjnx/M7mpQtnpp/Dtq/+HnsNHI6Frd6Rc0eLK5K7YkxsMtVKGl/6cU2TG2/+Husf3IEfigXb9X0SLAf2w8UI6Mj96HrlevgjLMuGZBeW/JMd9PBSlvhEIvJKHIf59UW9QX6w+sR2zzmUiTxWIV/9YhGfeXQLIJBi0YBx+C3sMTbJ2Y0yOL6IeSMKXq1dijZsHis1q9Dv2M1p1eAxWYcKSHSuQomkCP+0lNLlkRmjjcBw4cAgXPBQoM0kRmn8B3p4hsBh1uFiYhTKbDDAaIQxWmC2un0RRLSIB3NRuUKlUMNhMMLv7QS6XIdwGhEbXgcyqx1GzHGWevogtyUOnZq3g4eeJ7Qc24WJ4M3hIgPvyL6PT4Gch117C0u3JMPoEonHxBTz95ASEtojDlqUfIS8jC1aLGUFlZjwx/3sI/RX88J8BKJVbcMWoxv2D30T93m1QqtXh0LgO2F9cB/5KGQb/+Xd/YM50HN/zOwqM7mjf92W0G1Q+aN6v/x2CE2fy4KX2xHNfLyvfJ5Mec156AWaDCQn9XsB9j9wLANi++xh2/rgOfmFheH70IPsh2LLzECQ2CxIaxcHfv3yQPmErvxtT4sS77oiqkiPf37wsVYXchQ3xXjnwCy+/u6SRjzuyNVk4KvOFUnF1duexrX2xJk2gTKeA3L/8bo976kQiV3UaK08noFh+tfn2ifAI5G74FadKAlHYr/zyUouYVpg55zn8kR+HQkn5LdUSiQSJxUCrwx/istkTHg2bo2nTpng+MgzST2cg3SMcDdz9Mfz5YZCqFDhquwxbSFuE52fgpSIDevx3PI4VZmDglpPI8w3Cy+s/xH8mzIFXoCeemfkKNjR7Cg0vpWJs6gU8OO1VbDiyE68dSkOmZwjGbP8MbXtNgG+wGq8veRv7IvsiOP8CBpxLR3z/+3FkxyasMplRKPfGfWlbENv4AcilRqw5+AsK3CPgpi9CQrEF3gkJuHQwGeeUehglSgQW5cE/pAksViNOXzoFq1IJidkMT70M6qBAlGZloURuhAUSyA0mKFW+sNksKNGXQEgBm8UGqQUQUimsZjOswoa7K5W/PVKpFJAAErkcUgmgkaugdtfAbNRBr1BDyBXwNesRFBoFhVKB9IJslHqFwE2Y0cgqRWBcFLQF6TggD4RJ7Y7m+WeRmNgZak8Nfju0E2kRzeFr1uP+gmI0ebAHSjMOYY7eG8W+Qeh9Zhue7DMEXnX8MWf5QvzRvDfC9IUYeXQf+r79DsxpuzBx93lc8QtEp9M7MHLINHjHh2PBjPHItFpgLjEi4Uopnpz7Daz557FqwgjkmwUMphAMeG40wto1wpl9vyJ7wevYXxSGoFAPPPHEE4DVgoBDT+BUfgq0FhUU1vJLOv1ffgf7X0zC7pJwmNTlf7MSjS96jn8dyz96FwJS5Ot1qA/A08Md2ubPQLlrF/RedezHM2n4BCw/L4O+zIx2Da/eSRc6dDqO7UuFf0Tw1YOv1OCxj+bDXSWD29+GS+jYrgk6tvt737tyXTq0qFTGpIbuZkxuqpDa0x+NijwQVr9heYEQsBYmoK/uMiT3jrDXM8kikKDTIVqhQN2ktuXLyqSAvg5ay3Nhiu5sr9ssNAbJJgti5AYENSyfdC1ao0aWmwJhylKY3CLsY4iM7tgEv6Vsh1UbDPWfHfUaefvjFcPPWH48DlKJBFJVedPyW23icWjhNJwsCYKyR/mImnW962DBocexNjsOZr0Wmj9HLn6yYVNE7ViIKyVy2Ly9IZFIcE+T9mi/YyuOWiXQGv3QMjEevkG+6HSuP85pouHl74PgkgIMGDAADzzcD3/8+AuKAqIRuN6Gwc89h3r1I1DybR0sC+0In/wsdNuyGsM/+xIGiwHdlq/G2bB4tNv4DUY/OQL1EmIxZvEH+C6iG3yL8vH65m/x9CcLYS67gp7f/YSU6OZ4ZMsyvP7wcwhvFodXvnobS6IfgKe2GG//9iWemP0NrMXZ6LviZxyq2xr9dqzApK4DEJjUABMXv41vw3pDZdBj8to5eOj9L2EsyMDgNb/gaFQr3Hf4F4xK6IGgNg3w8Y8fYHlQb0itFkxY/xl6vfkJrMXZGPX7Lzhepw06nNqK5yPaIqRDI3z18/v4LvRBQAiM2jAXPV/9AEJXiFc2rEFKeBu0Tt+HwX5NENm1GVb/+jEWBPYFALywcS7e+7m8n0ersc/ibEwXJGYk41l1LOr2aYvdf3yKd717QMhkGLVtPvqO/xASiQTjf1iAozGd0fLSQbxgC0X8gO5I3z4Hzyk6wqR0w/itc/D4xM8hlUkxZvFn2NmwO1pkH8XYEgVaj3gahr1forOhOUrdffHa7zPx8rSvIXdT4rmPp2NN895oeukYXj2fjfsmT4B1/1dofjkCeR4BGP7Lp5g06TPIvdUY88k0fNfkfsRcPofXkvfjwQ/eA85tQs892bikDkD7rT/i7VenQRPuh8Zf/Q8bLCrYcvW4LzsTj44YARRlwHv6ZBzXWRCs0OCexNbwi49AmJcUI+aORnJxOHz9glCnTh0g7BH896cnsflQCUw2OYr1WngDGDjqHRwa3gq7yyJh9S7vEyILiMW93XpjxZr1sEGKAosJYQDqte6F9O8WQVFcCq2bf/kfnEyOTu/OQfIrr8JokaAssPzXoUIuQ2a/T7Fr3XaER0fhrzYTz5ZdcLijBXqLDfe3bW7/u41/fDi0rR5HlP/VPiyQyvDejFcg+9tdPwDQLDYEzWIr9ysJ9ORUKkS3islNFZIp1bBqrw42BqkUNlsEdKcyEDb06oeXNDQJ1iMmeIZIofG5OimmVNISAckbEHJ/y6vrCO8M373/g7+XHNHNr5Zb0QfN9/8Mv9FP28ssdXohauf7iJJdQP2lA8pjkkhgNfZFp5MboO73sL1ucN3O8Ds9D02VetRrV55MaWRSBKt6oEXuPkiiY8tvCQbQu01XyL9YhiyZDBHPDLXXfdUi8NPv38EmV8DDp/xD/PWe3dFoxEicMxsR1qa8KV2jUOFzUzZWLP8ZXtpi+PuV13215yNQfjYXJVodShXlXy5quRpPugn8cWATPAsKYVWV//oc2XsgLq1eC0uJDlmW8sHRFG6+6OyjguTMYSiKdci2mRAOYPj9/8HxXzfCUmrCRVP5/4fMOxTNfD1QnHUWUq0ZGTIrIt3cMObxcdizaTesJgsMcq/yjpdBQWh48gKyJXIoVb6QBHojLi4O/33xQ2zYdhBWq4DGIxAJCQkAElC/UIdz6nAoCjPhHxWGpKQkxDX9Bj/sLB/kzHdvGNq2bQvYbGisNeCsVywUllJE+Ieiffv2SGocgyUHcwAAnnvr4K9OrEmtuyAjuDlkEiMiJRq0a9cO7eoF4IMjJbBJZZAnR9gH/OuQeRqp3g1gs+rhozWjcePGaBw4Et7786CTKaGXhiAmOhoSmRR9WiXhvAFQQAMhN8Pf3x9o/xQSf9yCi8VFKCxRQCotby14tn1bGHZuhKyoBCWK8uMua/Y4/m/j/7Cr2ABpTils0vLmsCkDhyNx9NPIlLqjOObPeXXq3ouFy17Bd8dSAQAmuYAGwOBB4xH+QFukugfgcuyf57VPJB6ObAK3DWtRrPCA8c+BAuu37IRzJdHQmMxIr/NnC4VUhrghk/Hd27OQp/KCCC1PZNzUKqT2/Ai/7DiBlk3r2c93/4GjsTQlDEUGG/rWvTrnkmXIx9iy/TzuiQuwl0m9gxA45E0AQHT81UH++tzXBp3uSYS3W8W+J7Nf6IZ/iglwR0yAe6XyfyY2RFQ9mNxUIalGg/CPK96RFPDSS/B5/HG4JVydE0YRHIzg1ydCoqo4p5NHl86QBwdD1aDB1XWqVfDodh+kKrW9hQYAVGEhMNerB3XY1VsphU1A6u4OiVwOmfxqU7ZC7QYPoxmBoVc/qIXMHT45+fABEBbf8Op6pWo0PH0B/l16Xq2r9kdEahoiADTo1uNqXeGB+46eg++gZ6BQln8RCc8QxCWnIFYiRdxHA+x1I0uleGzXfnj06gUfXx8AQIh/GAau+QVlFhMi539lr/tEqQUtly1GUeOWCA8tb6qPCwzDhK++Rpm5DOnPv2avO8oiR6f5H+JcUDT8/rwjpG5gKF5fvBQl+hIcvO9Je91JXoHo8Mk0pHsGwn1Yed1o3wC89923yMnPQUqrXva6/wsMwW+TJyPLzRvWhwYCAELd3bFg8ZfIysnEiYSrrWszA0Pw0ISxyNZ4o7RT+Tq8FXJ8P3cGcvIu4ljdP+9Gk0rxXmgEek8YhXyNO/LGvVX+/+MVgsWL38ClvIs453n1MsZrsY3Q6b/jUaJS4dJzf95FElAP364ciXOXziLb8+r/5+tNWiNp6CAUaDTIfHZMeWFQPJat+xzHzx9Dpk8YJH9OlTGkWUdEPtYfBR5uyHhqXHldnwi8mZKCE8k7cNY3ElJV+fnTrmFLWEaOR7FGhQtDJ5bXVbqjk1WF0G2/ITmoPqTq8o8Rbx8fRGYYoJaYkdG97dXzJKkbsDMHx/1jIPcuT5AkMhlyuozA1pNX0Lx1V3vdgOdexntZDSFXyPGkh7e93PDqp9i6PxPd4q9eulE3ToDPkPHwkwDePlfr3te9HaKaNUEdv7/90ACwZuJDUCtkUCuuXq7p3SQUvZtUnql5ULvoSmWeagU81exUS1QbMLmpQhK5HOqGDSuUaVpWvhYuDwiA36BBlcp9HnmkUpkiLAwRsyuP7hU0biyCxo2tUKasE44GyQcq1Q2ZOBFBY8ZAor76YS/z8kLM6lUQFmuFpMn3iSfgcc89UNS52idAIpcj7H/vlddVXW0qd+/QHlKNBuq/JW6w2eA/YACE2Qy179VWKUVoCAKatYBHg3j7PFsA4BEQCE1ZGQLrhF/dngTwNdsQFR4MT4+rMXvZrPDQlqHNfVdH3YXVhpASHeJahyAi/Oqv7xCLEQEFV3DPw53sZcJgQL3cPNSLjEVMbJi9PLSkCMHZ2ah3z9X12sr0aJSVhfC63oipe3U63wC9FgFZl+E24Oo+C4sVdXML4BvqgeC60X/ugwR+APyz8lDWI95eVyWXocHlPAT4SKCOi7PXDRNA2PlM5HTrikaNysfq8FAqEZ9TiAK1Fyx1rya8wZ6eUGQVI6fD1RYDhZsbAuXegFEJRaOrLXyeTZpCmZYHU+TV23OlXl6QNOsCfakFEc1b2ct9H3wIWdYAmOrEXq3r7o6yMdNwJr8MLVslXY1h5CisbdIHnm5ukMrKP0Ykcjk8vlmNy7ml6B56tbOfb48eSIxpgbYyKdSKq8lB35eeQhejBV5/awmRKpVIfacv/ql/izro36JOpfLxPRtUKosOcEf0NVpNfP82Ei0RuTbeLUW1hq2sDMJqhdTNDZI/EySbXg9rqRYSpQLyvyVTpouXAIsZ8tBQSP9MyKzFxTBnZUGq0UAZdfXShOHkSQijEcq6dSHzKG9ZsFy5AlNaOqTu7lA3qG+vazxzBjaDEcqoSMj+PH9sOh3Mly9DolJD+bckzVJQAGGxQObtDan6z3mULBbYDEZI5DJ7GRER/TveCn4DTG6IiIhqH0e+v3mvIBEREbkUJjdENYhery+/06lxY+j1tz/MPxHR3YgdiolqECEEUlJS7M+JiMhxbLkhIiIil8LkhoiIiFwKkxsiIiJyKUxuiIiIyKUwuSEiIiKXwruliGoQiUSCqD9HT/77tBhERHTzmNwQ1SAajQbp6enODoOIqFbjZSkiIiJyKUxuiIiIyKUwuSGqQcrKytCqVSu0atUKZWVlzg6HiKhWYp8bohrEZrPhwIED9udEROQ4ttwQERGRS2FyQ0RERC6FyQ0RERG5FCY3RERE5FKY3BAREZFL4d1SRDVMQECAs0MgIqrVmNwQ1SDu7u7Iy8tzdhhERLUaL0sRERGRS2FyQ0RERC6FyQ1RDVJWVoYuXbqgS5cunH6BiOgWsc8NUQ1is9mwdetW+3MiInIcW26IiIjIpTC5ISIiIpfC5IaIiIhcCpMbIiIicilMboiIiMil8G4pohpGo9E4OwQiolqNyQ1RDeLu7g6dTufsMIiIajVeliIiIiKXwuSGiIiIXAqTG6IaxGAwoG/fvujbty8MBoOzwyEiqpXY54aoBrFarVi/fr39OREROY4tN0RERORSmNwQERGRS2FyQ0RERC7F6cnNnDlzEBMTA7VajcTERGzfvv2G9bdu3YrExESo1WrExsZi7ty5dyhSIiIiqg2cmtwsX74cY8aMwaRJk3Do0CF07NgRvXv3RkZGxjXrp6WloU+fPujYsSMOHTqE119/HaNGjcIPP/xwhyMnIiKimkoihBDO2nibNm3QsmVLfP755/ay+Ph49OvXD9OnT69U/9VXX8XPP/+M1NRUe9nw4cNx5MgR7N69+5rbMBqNMBqN9tfFxcWIjIxEZmYmvLy8qnBviG6fTqdDWFgYACArKwvu7u5OjoiIqGYoKSlBREQEioqK4O3tfePKwkmMRqOQyWRi1apVFcpHjRolOnXqdM1lOnbsKEaNGlWhbNWqVUIulwuTyXTNZd58800BgA8++OCDDz74cIFHZmbmv+YYThvnJj8/H1arFcHBwRXKg4ODcfny5Wsuc/ny5WvWt1gsyM/PR2hoaKVlJk6ciHHjxtlf22w2FBYWwt/fHxKJpAr25MZatWqF/fv3O3U9ji57s/Vvpt6N6lzvvb+y89rYulZV/993cju3uq6ael7d6P3aem7dTefVrSxbVecWz6uavS0hBBITE+2t2zfi9EH8/plgCCFumHRcq/61yv+iUqmgUqkqlPn4+NxCpLdGJpNVycl+O+txdNmbrX8z9W5U59+W9/LyqlUfFEDV/X/fye3c6rpq6nl1M+/XtnPrbjqvbmXZqjq3eF7V/G0plUpIpf/eXdhpHYoDAgIgk8kqtdLk5uZWap35S0hIyDXry+Vy+Pv7V1ust+Oll15y+nocXfZm699MvRvVqapjU5PcqX2qyu3c6rpq6nnlyLZqi7vpvLqVZavq3OJ5VfO3dbPrcXqH4sTERMyZM8de1qhRIzz00EPX7VC8Zs0apKSk2MtefPFFHD58+Lodiqn2KSkpgbe3N4qLi2vVryCq+XhuUXXgeVXzOPVW8HHjxmH+/PlYuHAhUlNTMXbsWGRkZGD48OEAyvvLDBo0yF5/+PDhuHDhAsaNG4fU1FQsXLgQCxYswPjx4521C1QNVCoV3nzzzUqXE4luF88tqg48r2oep7bcAOWD+M2YMQPZ2dlISEjAxx9/jE6dOgEABg8ejPT0dGzZssVef+vWrRg7dixOnDiBsLAwvPrqq/ZkiIiIiMjpyQ0RERFRVXL69AtEREREVYnJDREREbkUJjdERETkUpjcEBERkUthckO1WmlpKVq1aoXmzZujSZMm+PLLL50dErmAzMxMdOnSBY0aNULTpk2xcuVKZ4dELqJ///7w9fXFo48+6uxQXBrvlqJazWq1wmg0QqPRQK/XIyEhAfv376+xI1ZT7ZCdnY2cnBw0b94cubm5aNmyJU6dOsVZ2um2bd68GVqtFl9//TW+//57Z4fjsthyQ7WaTCaDRqMBABgMBlitVjBfp9sVGhqK5s2bAwCCgoLg5+eHwsJC5wZFLqFr167w9PR0dhguj8kNVatt27bhgQceQFhYGCQSCX788cdKdebMmYOYmBio1WokJiZi+/btDm2jqKgIzZo1Q506dTBhwgQEBARUUfRUU92J8+ovBw4cgM1mQ0RExG1GTTXdnTyvqHoxuaFqpdPp0KxZM8yePfua7y9fvhxjxozBpEmTcOjQIXTs2BG9e/dGRkaGvU5iYiISEhIqPbKysgCUz/J+5MgRpKWl4dtvv0VOTs4d2TdynjtxXgFAQUEBBg0ahHnz5lX7PpHz3anziu4AQXSHABCrV6+uUNa6dWsxfPjwCmUNGzYUr7322i1tY/jw4WLFihW3GiLVQtV1XhkMBtGxY0exePHiqgiTapnq/LzavHmzeOSRR243RLoBttyQ05hMJiQnJ6NHjx4Vynv06IFdu3bd1DpycnJQUlICoHxm3m3btqFBgwZVHivVHlVxXgkhMHjwYNx777145plnqiNMqmWq4ryiO0fu7ADo7pWfnw+r1Yrg4OAK5cHBwbh8+fJNrePixYsYNmwYhBAQQmDkyJFo2rRpdYRLtURVnFc7d+7E8uXL0bRpU3u/iyVLlqBJkyZVHS7VElVxXgFAz549cfDgQeh0OtSpUwerV69Gq1atqjrcux6TG3I6iURS4bUQolLZ9SQmJuLw4cPVEBXVdrdzXt1zzz2w2WzVERbVcrdzXgHAb7/9VtUh0TXwshQ5TUBAAGQyWaVfPbm5uZV+HRHdLJ5XVB14XtUuTG7IaZRKJRITE7Fhw4YK5Rs2bED79u2dFBXVdjyvqDrwvKpdeFmKqpVWq8XZs2ftr9PS0nD48GH4+fkhMjIS48aNwzPPPIOkpCS0a9cO8+bNQ0ZGBoYPH+7EqKmm43lF1YHnlQtx5q1a5Po2b94sAFR6PPvss/Y6n332mYiKihJKpVK0bNlSbN261XkBU63A84qqA88r18G5pYiIiMilsM8NERERuRQmN0RERORSmNwQERGRS2FyQ0RERC6FyQ0RERG5FCY3RERE5FKY3BAREZFLYXJDRERELoXJDREREbkUJjdERETkUpjcEBERkUthckNEREQuhckNEbmEffv2oUuXLnBzc0PDhg2xf/9+zJs3Dw8++KCzQyOiO4yzghNRrbdnzx507doVb775Jh555BG8+uqrMBqNOH36NFasWIEWLVo4O0QiuoOY3BBRrde+fXvExsbim2++AQCsWLECTz75JB566CGsWrXKydER0Z3Gy1JEVKtdvHgRu3fvxosvvmgvUyqVEELgrbfecmJkROQsTG6IqFZLTU0FACQlJdnLTp06hdatW6NJkybOCouInIjJDRHVasXFxZDJZPbXhYWFmDFjBlQqlROjIiJnYnJDRLVa8+bNYbVaMWPGDJw8eRJPPvkkoqKikJqaigsXLjg7PCJyAiY3RFSrxcXFYerUqZg1axZatGiB0NBQ/P7774iIiEC3bt2cHR4ROQHvliIiIiKXwpYbIiIicilMboiIiMilMLkhIiIil8LkhoiIiFwKkxsiIiJyKUxuiIiIyKUwuSEiIiKXwuSGiIiIXAqTGyIiInIpTG6IiIjIpTC5ISIiIpfy/0SsQ919To2SAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ymin, ymax =0, 1\n",
    "lasso = model[-1]\n",
    "plt.semilogx(lasso.alphas_, lasso.mse_path_, linestyle=\":\")\n",
    "plt.plot(\n",
    "    lasso.alphas_,\n",
    "    lasso.mse_path_.mean(axis=-1),\n",
    "    color=\"black\",\n",
    "    label=\"Average across the folds\",\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.axvline(lasso.alpha_, linestyle=\"--\", color=\"black\", label=\"alpha: CV estimate\")\n",
    "\n",
    "plt.ylim(ymin, ymax)\n",
    "plt.xlabel(r\"$\\alpha$\")\n",
    "plt.ylabel(\"Mean square error\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d8e3935-01a9-4adb-8939-18d9e9c2c150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0026414491572292246"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0be9143b-967c-41fc-8e33-04f34afa9a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cv_model= RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d10f1531-da2b-4bb6-b83b-3558bdac2339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.973e+02, tolerance: 1.527e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6175, 333)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsvc =  Lasso(alpha=lasso.alpha_).fit(X, y)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7b74110-bf78-4a20-973b-7f81bf68a4a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MW</th>\n",
       "      <td>0.001522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMW</th>\n",
       "      <td>-0.004765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sp</th>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Si</th>\n",
       "      <td>-0.001728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mp</th>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s34_relSize</th>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s34_phSize</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s34_phRelSize</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chiralMoment</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chiralPhMoment</th>\n",
       "      <td>-0.000894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1585 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    coef\n",
       "MW              0.001522\n",
       "AMW            -0.004765\n",
       "Sp             -0.000000\n",
       "Si             -0.001728\n",
       "Mp             -0.000000\n",
       "...                  ...\n",
       "s34_relSize    -0.000000\n",
       "s34_phSize      0.000000\n",
       "s34_phRelSize   0.000000\n",
       "chiralMoment    0.000000\n",
       "chiralPhMoment -0.000894\n",
       "\n",
       "[1585 rows x 1 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_coef=pd.DataFrame(lsvc.coef_)\n",
    "lasso_coef.index=X_NAomit_data.columns\n",
    "lasso_coef.columns=[\"coef\"]\n",
    "lasso_coef\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "013bbde3-8f0e-4b0f-9f7f-8ba774658777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MW</th>\n",
       "      <td>0.001522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMW</th>\n",
       "      <td>-0.004765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Si</th>\n",
       "      <td>-0.001728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nAA</th>\n",
       "      <td>-0.002683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RBN</th>\n",
       "      <td>-0.000231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         coef\n",
       "MW   0.001522\n",
       "AMW -0.004765\n",
       "Si  -0.001728\n",
       "nAA -0.002683\n",
       "RBN -0.000231"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_coef_last=lasso_coef[(lasso_coef[\"coef\"]>0)|(lasso_coef[\"coef\"]<0)]\n",
    "lasso_coef_last.to_csv(\"./Supplementary Data S6.csv\",sep=',')\n",
    "lasso_coef_last.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c39feec4-a989-406f-b6a2-1c3428dc3b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MW</th>\n",
       "      <th>AMW</th>\n",
       "      <th>Si</th>\n",
       "      <th>nAA</th>\n",
       "      <th>RBN</th>\n",
       "      <th>H%</th>\n",
       "      <th>X%</th>\n",
       "      <th>max_conj_path</th>\n",
       "      <th>Rperim</th>\n",
       "      <th>D/Dtr03</th>\n",
       "      <th>...</th>\n",
       "      <th>nLevel4</th>\n",
       "      <th>nLevel8</th>\n",
       "      <th>arLevel2</th>\n",
       "      <th>s2_size</th>\n",
       "      <th>s4_numSharedNeighbors</th>\n",
       "      <th>s4_numRotBonds</th>\n",
       "      <th>s3_numAroBonds</th>\n",
       "      <th>s4_numAroBonds</th>\n",
       "      <th>s34_size</th>\n",
       "      <th>chiralPhMoment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2083077.0</th>\n",
       "      <td>425.57</td>\n",
       "      <td>8.866042</td>\n",
       "      <td>53.2847</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>39.583333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138703696.0</th>\n",
       "      <td>526.58</td>\n",
       "      <td>8.493226</td>\n",
       "      <td>70.2759</td>\n",
       "      <td>26.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>37.096774</td>\n",
       "      <td>1.612903</td>\n",
       "      <td>17.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>27.00</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>16.822604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139316163.0</th>\n",
       "      <td>585.65</td>\n",
       "      <td>8.134028</td>\n",
       "      <td>82.7424</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>41.666667</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>17.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>16.822604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139315919.0</th>\n",
       "      <td>541.62</td>\n",
       "      <td>7.965000</td>\n",
       "      <td>77.9875</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>42.647059</td>\n",
       "      <td>2.941176</td>\n",
       "      <td>17.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>16.822604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138703798.0</th>\n",
       "      <td>491.63</td>\n",
       "      <td>7.448939</td>\n",
       "      <td>75.2290</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>45.454545</td>\n",
       "      <td>1.515152</td>\n",
       "      <td>17.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>13.379088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158020.0</th>\n",
       "      <td>462.06</td>\n",
       "      <td>7.831525</td>\n",
       "      <td>66.3847</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>47.457627</td>\n",
       "      <td>1.694915</td>\n",
       "      <td>9.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9631088.0</th>\n",
       "      <td>246.31</td>\n",
       "      <td>9.122593</td>\n",
       "      <td>29.9962</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>37.037037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3247362.0</th>\n",
       "      <td>463.62</td>\n",
       "      <td>7.244062</td>\n",
       "      <td>71.3113</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>45.312500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>71.065768</td>\n",
       "      <td>...</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>9.75</td>\n",
       "      <td>11.75</td>\n",
       "      <td>31.500000</td>\n",
       "      <td>5.196152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5428531.0</th>\n",
       "      <td>282.31</td>\n",
       "      <td>8.066000</td>\n",
       "      <td>38.7440</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138502092.0</th>\n",
       "      <td>477.67</td>\n",
       "      <td>8.096102</td>\n",
       "      <td>66.7363</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>45.762712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6175 rows × 333 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 MW       AMW       Si   nAA   RBN         H%        X%  \\\n",
       "cid                                                                       \n",
       "2083077.0    425.57  8.866042  53.2847  21.0   7.0  39.583333  0.000000   \n",
       "138703696.0  526.58  8.493226  70.2759  26.0   8.0  37.096774  1.612903   \n",
       "139316163.0  585.65  8.134028  82.7424  15.0  10.0  41.666667  4.166667   \n",
       "139315919.0  541.62  7.965000  77.9875  15.0  10.0  42.647059  2.941176   \n",
       "138703798.0  491.63  7.448939  75.2290  15.0   7.0  45.454545  1.515152   \n",
       "...             ...       ...      ...   ...   ...        ...       ...   \n",
       "2158020.0    462.06  7.831525  66.3847  15.0   8.0  47.457627  1.694915   \n",
       "9631088.0    246.31  9.122593  29.9962  11.0   4.0  37.037037  0.000000   \n",
       "3247362.0    463.62  7.244062  71.3113  22.0   7.0  45.312500  0.000000   \n",
       "5428531.0    282.31  8.066000  38.7440  12.0   2.0  40.000000  0.000000   \n",
       "138502092.0  477.67  8.096102  66.7363  11.0   8.0  45.762712  0.000000   \n",
       "\n",
       "             max_conj_path  Rperim    D/Dtr03  ...   nLevel4  nLevel8  \\\n",
       "cid                                            ...                      \n",
       "2083077.0             13.0    21.0   0.000000  ...  0.000000      0.0   \n",
       "138703696.0           17.0    26.0   0.000000  ...  2.000000      3.0   \n",
       "139316163.0           17.0    23.0   0.000000  ...  2.000000      3.0   \n",
       "139315919.0           17.0    19.0   0.000000  ...  2.000000      3.0   \n",
       "138703798.0           17.0    23.0   0.000000  ...  1.333333      3.0   \n",
       "...                    ...     ...        ...  ...       ...      ...   \n",
       "2158020.0              9.0    15.0   0.000000  ...  0.000000      0.0   \n",
       "9631088.0              8.0    11.0   0.000000  ...  0.000000      0.0   \n",
       "3247362.0             10.0    25.0  71.065768  ...  4.750000      2.5   \n",
       "5428531.0             10.0    16.0   0.000000  ...  0.000000      0.0   \n",
       "138502092.0            8.0    24.0   0.000000  ...  0.000000      0.0   \n",
       "\n",
       "             arLevel2   s2_size  s4_numSharedNeighbors  s4_numRotBonds  \\\n",
       "cid                                                                      \n",
       "2083077.0        0.00  0.000000                    0.0        0.000000   \n",
       "138703696.0      0.00  1.000000                    0.0        6.000000   \n",
       "139316163.0      0.00  1.000000                    0.0        8.000000   \n",
       "139315919.0      0.00  1.000000                    0.0        8.000000   \n",
       "138703798.0      0.00  1.666667                    0.0        6.333333   \n",
       "...               ...       ...                    ...             ...   \n",
       "2158020.0        0.00  0.000000                    0.0        0.000000   \n",
       "9631088.0        0.00  0.000000                    0.0        0.000000   \n",
       "3247362.0        0.75  2.250000                    0.0        3.500000   \n",
       "5428531.0        0.00  0.000000                    0.0        0.000000   \n",
       "138502092.0      0.00  0.000000                    0.0        0.000000   \n",
       "\n",
       "             s3_numAroBonds  s4_numAroBonds   s34_size  chiralPhMoment  \n",
       "cid                                                                     \n",
       "2083077.0              0.00            0.00   0.000000        0.000000  \n",
       "138703696.0            0.00           27.00  37.000000       16.822604  \n",
       "139316163.0            0.00           16.00  40.000000       16.822604  \n",
       "139315919.0            0.00           16.00  37.000000       16.822604  \n",
       "138703798.0            0.00           16.00  33.333333       13.379088  \n",
       "...                     ...             ...        ...             ...  \n",
       "2158020.0              0.00            0.00   0.000000        0.000000  \n",
       "9631088.0              0.00            0.00   0.000000        0.000000  \n",
       "3247362.0              9.75           11.75  31.500000        5.196152  \n",
       "5428531.0              0.00            0.00   0.000000        0.000000  \n",
       "138502092.0            0.00            0.00   0.000000        0.000000  \n",
       "\n",
       "[6175 rows x 333 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lasso_data=X_NAomit_data[X_NAomit_data.columns[model.get_support()]]\n",
    "Lasso_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afcca2a8-9ce0-4c49-88db-c3265e6374fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso_data.to_csv('./Lasso_data.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9c9a6da-b735-4689-897c-972cd30e5f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MW</th>\n",
       "      <th>AMW</th>\n",
       "      <th>Si</th>\n",
       "      <th>nAA</th>\n",
       "      <th>RBN</th>\n",
       "      <th>H%</th>\n",
       "      <th>X%</th>\n",
       "      <th>max_conj_path</th>\n",
       "      <th>Rperim</th>\n",
       "      <th>D/Dtr03</th>\n",
       "      <th>...</th>\n",
       "      <th>nLevel4</th>\n",
       "      <th>nLevel8</th>\n",
       "      <th>arLevel2</th>\n",
       "      <th>s2_size</th>\n",
       "      <th>s4_numSharedNeighbors</th>\n",
       "      <th>s4_numRotBonds</th>\n",
       "      <th>s3_numAroBonds</th>\n",
       "      <th>s4_numAroBonds</th>\n",
       "      <th>s34_size</th>\n",
       "      <th>chiralPhMoment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2083077.0</th>\n",
       "      <td>0.100131</td>\n",
       "      <td>0.053573</td>\n",
       "      <td>0.079943</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.049296</td>\n",
       "      <td>0.560764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138703696.0</th>\n",
       "      <td>0.125832</td>\n",
       "      <td>0.049251</td>\n",
       "      <td>0.106433</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.056338</td>\n",
       "      <td>0.525538</td>\n",
       "      <td>0.019355</td>\n",
       "      <td>0.472222</td>\n",
       "      <td>0.270833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.396226</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056442</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.654851</td>\n",
       "      <td>0.133940</td>\n",
       "      <td>0.133495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139316163.0</th>\n",
       "      <td>0.140863</td>\n",
       "      <td>0.045086</td>\n",
       "      <td>0.125869</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.070423</td>\n",
       "      <td>0.590278</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.472222</td>\n",
       "      <td>0.239583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.396226</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.075257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.388060</td>\n",
       "      <td>0.144800</td>\n",
       "      <td>0.133495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139315919.0</th>\n",
       "      <td>0.129659</td>\n",
       "      <td>0.043126</td>\n",
       "      <td>0.118456</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.070423</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>0.035294</td>\n",
       "      <td>0.472222</td>\n",
       "      <td>0.197917</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.396226</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.075257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.388060</td>\n",
       "      <td>0.133940</td>\n",
       "      <td>0.133495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138703798.0</th>\n",
       "      <td>0.116939</td>\n",
       "      <td>0.037143</td>\n",
       "      <td>0.114155</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.049296</td>\n",
       "      <td>0.643939</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.472222</td>\n",
       "      <td>0.239583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.396226</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.059578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.388060</td>\n",
       "      <td>0.120667</td>\n",
       "      <td>0.106170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158020.0</th>\n",
       "      <td>0.109415</td>\n",
       "      <td>0.041579</td>\n",
       "      <td>0.100366</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.056338</td>\n",
       "      <td>0.672316</td>\n",
       "      <td>0.020339</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9631088.0</th>\n",
       "      <td>0.054518</td>\n",
       "      <td>0.056547</td>\n",
       "      <td>0.043635</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.524691</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.114583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3247362.0</th>\n",
       "      <td>0.109812</td>\n",
       "      <td>0.034768</td>\n",
       "      <td>0.108047</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.049296</td>\n",
       "      <td>0.641927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.260417</td>\n",
       "      <td>0.346112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.330189</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.140625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032925</td>\n",
       "      <td>0.716327</td>\n",
       "      <td>0.284981</td>\n",
       "      <td>0.114030</td>\n",
       "      <td>0.041234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5428531.0</th>\n",
       "      <td>0.063678</td>\n",
       "      <td>0.044297</td>\n",
       "      <td>0.057273</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138502092.0</th>\n",
       "      <td>0.113387</td>\n",
       "      <td>0.044646</td>\n",
       "      <td>0.100914</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.056338</td>\n",
       "      <td>0.648305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6175 rows × 333 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   MW       AMW        Si   nAA       RBN        H%        X%  \\\n",
       "cid                                                                             \n",
       "2083077.0    0.100131  0.053573  0.079943  0.42  0.049296  0.560764  0.000000   \n",
       "138703696.0  0.125832  0.049251  0.106433  0.52  0.056338  0.525538  0.019355   \n",
       "139316163.0  0.140863  0.045086  0.125869  0.30  0.070423  0.590278  0.050000   \n",
       "139315919.0  0.129659  0.043126  0.118456  0.30  0.070423  0.604167  0.035294   \n",
       "138703798.0  0.116939  0.037143  0.114155  0.30  0.049296  0.643939  0.018182   \n",
       "...               ...       ...       ...   ...       ...       ...       ...   \n",
       "2158020.0    0.109415  0.041579  0.100366  0.30  0.056338  0.672316  0.020339   \n",
       "9631088.0    0.054518  0.056547  0.043635  0.22  0.028169  0.524691  0.000000   \n",
       "3247362.0    0.109812  0.034768  0.108047  0.44  0.049296  0.641927  0.000000   \n",
       "5428531.0    0.063678  0.044297  0.057273  0.24  0.014085  0.566667  0.000000   \n",
       "138502092.0  0.113387  0.044646  0.100914  0.22  0.056338  0.648305  0.000000   \n",
       "\n",
       "             max_conj_path    Rperim   D/Dtr03  ...   nLevel4   nLevel8  \\\n",
       "cid                                             ...                       \n",
       "2083077.0         0.361111  0.218750  0.000000  ...  0.000000  0.000000   \n",
       "138703696.0       0.472222  0.270833  0.000000  ...  0.222222  0.396226   \n",
       "139316163.0       0.472222  0.239583  0.000000  ...  0.222222  0.396226   \n",
       "139315919.0       0.472222  0.197917  0.000000  ...  0.222222  0.396226   \n",
       "138703798.0       0.472222  0.239583  0.000000  ...  0.148148  0.396226   \n",
       "...                    ...       ...       ...  ...       ...       ...   \n",
       "2158020.0         0.250000  0.156250  0.000000  ...  0.000000  0.000000   \n",
       "9631088.0         0.222222  0.114583  0.000000  ...  0.000000  0.000000   \n",
       "3247362.0         0.277778  0.260417  0.346112  ...  0.527778  0.330189   \n",
       "5428531.0         0.277778  0.166667  0.000000  ...  0.000000  0.000000   \n",
       "138502092.0       0.222222  0.250000  0.000000  ...  0.000000  0.000000   \n",
       "\n",
       "             arLevel2   s2_size  s4_numSharedNeighbors  s4_numRotBonds  \\\n",
       "cid                                                                      \n",
       "2083077.0        0.00  0.000000                    0.0        0.000000   \n",
       "138703696.0      0.00  0.062500                    0.0        0.056442   \n",
       "139316163.0      0.00  0.062500                    0.0        0.075257   \n",
       "139315919.0      0.00  0.062500                    0.0        0.075257   \n",
       "138703798.0      0.00  0.104167                    0.0        0.059578   \n",
       "...               ...       ...                    ...             ...   \n",
       "2158020.0        0.00  0.000000                    0.0        0.000000   \n",
       "9631088.0        0.00  0.000000                    0.0        0.000000   \n",
       "3247362.0        0.15  0.140625                    0.0        0.032925   \n",
       "5428531.0        0.00  0.000000                    0.0        0.000000   \n",
       "138502092.0      0.00  0.000000                    0.0        0.000000   \n",
       "\n",
       "             s3_numAroBonds  s4_numAroBonds  s34_size  chiralPhMoment  \n",
       "cid                                                                    \n",
       "2083077.0          0.000000        0.000000  0.000000        0.000000  \n",
       "138703696.0        0.000000        0.654851  0.133940        0.133495  \n",
       "139316163.0        0.000000        0.388060  0.144800        0.133495  \n",
       "139315919.0        0.000000        0.388060  0.133940        0.133495  \n",
       "138703798.0        0.000000        0.388060  0.120667        0.106170  \n",
       "...                     ...             ...       ...             ...  \n",
       "2158020.0          0.000000        0.000000  0.000000        0.000000  \n",
       "9631088.0          0.000000        0.000000  0.000000        0.000000  \n",
       "3247362.0          0.716327        0.284981  0.114030        0.041234  \n",
       "5428531.0          0.000000        0.000000  0.000000        0.000000  \n",
       "138502092.0        0.000000        0.000000  0.000000        0.000000  \n",
       "\n",
       "[6175 rows x 333 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scale data\n",
    "Scaler = preprocessing.MinMaxScaler() #StandardScaler\n",
    "Transformer =Scaler.fit(Lasso_data)\n",
    "X_scaled_data=Transformer.transform(Lasso_data)\n",
    "X_scaled_data =pd.DataFrame(X_scaled_data)\n",
    "X_scaled_data.columns=Lasso_data.columns\n",
    "X_scaled_data.index=Raw_data.index\n",
    "\n",
    "joblib.dump(Transformer, './Lasso_Scaler_transformer.pkl')\n",
    "\n",
    "X_scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb74c724-573a-4894-8933-28d6277be45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the data\n",
    "Lasso_data= pd.read_csv(\"./Lasso_data.csv\",header=0,index_col=0)\n",
    "Transformer= joblib.load(filename='./Lasso_Scaler_transformer.pkl')\n",
    "X_scaled_data=Transformer.transform(Lasso_data)\n",
    "X_scaled_data =pd.DataFrame(X_scaled_data)\n",
    "X_scaled_data.columns=Lasso_data.columns\n",
    "X_scaled_data.index=Raw_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8070777b-2ae7-43d9-a6f0-877661063cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c0643c9-e6ee-441f-a6e6-f8951935a04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_results(Model_clf,X_test,y,Cv_model):\n",
    "    Model_scores= cross_validate(estimator=Model_clf, X=X_test, y=y, cv=Cv_model,scoring=( 'accuracy','f1','precision','recall','roc_auc'), return_train_score=True)\n",
    "    Model_score= cross_validate(estimator=Model_clf, X=X_test, y=y, cv=Cv_model,scoring=( 'accuracy','f1','precision','recall','roc_auc'), return_train_score=False)\n",
    "#Accuracy\n",
    "    Model_Accuracy_test_mean=Model_scores['test_accuracy'].mean()\n",
    "    Model_Accuracy_test_se=(Model_scores['test_accuracy'].std()/math.sqrt(len(Model_scores['test_accuracy']))) \n",
    "    Model_Accuracy_train_mean=Model_scores['train_accuracy'].mean()\n",
    "    Model_Accuracy_train_se=(Model_scores['train_accuracy'].std()/math.sqrt(len(Model_scores['train_accuracy']))) \n",
    "#f1\n",
    "    Model_f1_mean=Model_score['test_f1'].mean()\n",
    "    Model_f1_se=(Model_score['test_f1'].std()/math.sqrt(len(Model_score['test_f1']))) \n",
    "#precision\n",
    "    Model_precision_mean=Model_score['test_precision'].mean()\n",
    "    Model_precision_se=(Model_score['test_precision'].std()/math.sqrt(len(Model_score['test_precision']))) \n",
    "#recall\n",
    "    Model_recall_mean=Model_score['test_recall'].mean()\n",
    "    Model_recall_se=(Model_score['test_recall'].std()/math.sqrt(len(Model_score['test_recall']))) \n",
    "#roc_auc\n",
    "    Model_roc_auc_mean=Model_score['test_roc_auc'].mean()\n",
    "    Model_roc_auc_se=(Model_score['test_roc_auc'].std()/math.sqrt(len(Model_score['test_roc_auc']))) \n",
    "    Model = {'Mean':[Model_Accuracy_test_mean,Model_Accuracy_train_mean,Model_f1_mean,Model_precision_mean,Model_recall_mean,Model_roc_auc_mean],\n",
    "        'Se':[Model_Accuracy_test_se,Model_Accuracy_train_se,Model_f1_se,Model_precision_se,Model_recall_se,Model_roc_auc_se]}\n",
    "    Model = pd.DataFrame(Model, index=['Accuracy_test','Accuracy_train','F1 Score','Precision','Recall','Roc_auc']) # 这里设定了 index 个数要和列表长度一致\n",
    "    return Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e49d1bb-d5f4-4e7f-a620-a943c578fba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the KFold \n",
    "Cv_optuna= RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=0)\n",
    "Cv_model= RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=0)\n",
    "Cv_RFECV= RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b74394b-d874-4115-b66e-0bac0d514965",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data pre-processing of models\n",
    "X=np.array(X_scaled_data)\n",
    "y=Raw_data['Activity'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d026e47-e8b2-467a-a5a0-866a1888a801",
   "metadata": {},
   "source": [
    "## 1.1 DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5dd9da6-318e-4a8f-8b6e-46380439376d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=DecisionTreeClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d7e67d4-b288-4270-be5d-1598f117e05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.774494</td>\n",
       "      <td>0.001772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.998433</td>\n",
       "      <td>0.000053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.795780</td>\n",
       "      <td>0.001632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.794873</td>\n",
       "      <td>0.001899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.796916</td>\n",
       "      <td>0.002302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.772521</td>\n",
       "      <td>0.001802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean        Se\n",
       "Accuracy_test   0.774494  0.001772\n",
       "Accuracy_train  0.998433  0.000053\n",
       "F1 Score        0.795780  0.001632\n",
       "Precision       0.794873  0.001899\n",
       "Recall          0.796916  0.002302\n",
       "Roc_auc         0.772521  0.001802"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model1 （4175 descriptors）\n",
    "Model1_clf=clf\n",
    "#Model1\n",
    "Model1=Model_results(Model1_clf,X,y,Cv_model)\n",
    "Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7bb0633b-59c6-4ad7-afb1-e241f713f990",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2024-11-16 01:03:20,473]\u001b[0m A new study created in memory with name: no-name-3bd916f5-f242-4f90-b365-23afec28ec23\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:24,260]\u001b[0m Trial 0 finished with value: 0.7452793522267207 and parameters: {'max_depth': 4, 'max_features': 17, 'min_samples_split': 16}. Best is trial 0 with value: 0.7452793522267207.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:24,695]\u001b[0m Trial 1 finished with value: 0.7461862348178135 and parameters: {'max_depth': 4, 'max_features': 14, 'min_samples_split': 17}. Best is trial 1 with value: 0.7461862348178135.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:25,235]\u001b[0m Trial 2 finished with value: 0.7396113360323886 and parameters: {'max_depth': 4, 'max_features': 19, 'min_samples_split': 25}. Best is trial 1 with value: 0.7461862348178135.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:25,839]\u001b[0m Trial 3 finished with value: 0.7502186234817815 and parameters: {'max_depth': 4, 'max_features': 18, 'min_samples_split': 14}. Best is trial 3 with value: 0.7502186234817815.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:26,492]\u001b[0m Trial 4 finished with value: 0.7552226720647772 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 3}. Best is trial 4 with value: 0.7552226720647772.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:26,924]\u001b[0m Trial 5 finished with value: 0.7473846153846155 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 21}. Best is trial 4 with value: 0.7552226720647772.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:27,592]\u001b[0m Trial 6 finished with value: 0.7494736842105262 and parameters: {'max_depth': 5, 'max_features': 19, 'min_samples_split': 25}. Best is trial 4 with value: 0.7552226720647772.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:28,132]\u001b[0m Trial 7 finished with value: 0.7515303643724696 and parameters: {'max_depth': 5, 'max_features': 15, 'min_samples_split': 20}. Best is trial 4 with value: 0.7552226720647772.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:28,686]\u001b[0m Trial 8 finished with value: 0.7307206477732794 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 5}. Best is trial 4 with value: 0.7552226720647772.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:29,226]\u001b[0m Trial 9 finished with value: 0.7517732793522268 and parameters: {'max_depth': 5, 'max_features': 15, 'min_samples_split': 11}. Best is trial 4 with value: 0.7552226720647772.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:29,756]\u001b[0m Trial 10 finished with value: 0.7463481781376519 and parameters: {'max_depth': 3, 'max_features': 12, 'min_samples_split': 2}. Best is trial 4 with value: 0.7552226720647772.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:30,297]\u001b[0m Trial 11 finished with value: 0.7534736842105261 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 9}. Best is trial 4 with value: 0.7552226720647772.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:30,747]\u001b[0m Trial 12 finished with value: 0.7530850202429149 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 8}. Best is trial 4 with value: 0.7552226720647772.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:31,150]\u001b[0m Trial 13 finished with value: 0.737327935222672 and parameters: {'max_depth': 4, 'max_features': 11, 'min_samples_split': 8}. Best is trial 4 with value: 0.7552226720647772.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:31,648]\u001b[0m Trial 14 finished with value: 0.7517732793522266 and parameters: {'max_depth': 5, 'max_features': 20, 'min_samples_split': 2}. Best is trial 4 with value: 0.7552226720647772.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:32,282]\u001b[0m Trial 15 finished with value: 0.7466396761133604 and parameters: {'max_depth': 4, 'max_features': 13, 'min_samples_split': 6}. Best is trial 4 with value: 0.7552226720647772.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:32,843]\u001b[0m Trial 16 finished with value: 0.7517894736842107 and parameters: {'max_depth': 5, 'max_features': 16, 'min_samples_split': 11}. Best is trial 4 with value: 0.7552226720647772.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:33,372]\u001b[0m Trial 17 finished with value: 0.7395465587044536 and parameters: {'max_depth': 3, 'max_features': 13, 'min_samples_split': 5}. Best is trial 4 with value: 0.7552226720647772.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:33,790]\u001b[0m Trial 18 finished with value: 0.7490364372469633 and parameters: {'max_depth': 4, 'max_features': 10, 'min_samples_split': 10}. Best is trial 4 with value: 0.7552226720647772.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:34,210]\u001b[0m Trial 19 finished with value: 0.7551417004048582 and parameters: {'max_depth': 4, 'max_features': 15, 'min_samples_split': 4}. Best is trial 4 with value: 0.7552226720647772.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:34,630]\u001b[0m Trial 20 finished with value: 0.7436113360323888 and parameters: {'max_depth': 3, 'max_features': 16, 'min_samples_split': 4}. Best is trial 4 with value: 0.7552226720647772.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:35,267]\u001b[0m Trial 21 finished with value: 0.7491497975708501 and parameters: {'max_depth': 4, 'max_features': 14, 'min_samples_split': 8}. Best is trial 4 with value: 0.7552226720647772.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:35,841]\u001b[0m Trial 22 finished with value: 0.7508502024291497 and parameters: {'max_depth': 4, 'max_features': 14, 'min_samples_split': 3}. Best is trial 4 with value: 0.7552226720647772.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:36,244]\u001b[0m Trial 23 finished with value: 0.7405344129554656 and parameters: {'max_depth': 4, 'max_features': 12, 'min_samples_split': 7}. Best is trial 4 with value: 0.7552226720647772.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:36,802]\u001b[0m Trial 24 finished with value: 0.7515465587044535 and parameters: {'max_depth': 5, 'max_features': 16, 'min_samples_split': 10}. Best is trial 4 with value: 0.7552226720647772.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:37,359]\u001b[0m Trial 25 finished with value: 0.754072874493927 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 4}. Best is trial 4 with value: 0.7552226720647772.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:37,811]\u001b[0m Trial 26 finished with value: 0.7371983805668016 and parameters: {'max_depth': 4, 'max_features': 19, 'min_samples_split': 5}. Best is trial 4 with value: 0.7552226720647772.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:38,464]\u001b[0m Trial 27 finished with value: 0.7552712550607287 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 2}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:39,006]\u001b[0m Trial 28 finished with value: 0.7377165991902835 and parameters: {'max_depth': 3, 'max_features': 18, 'min_samples_split': 2}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:39,461]\u001b[0m Trial 29 finished with value: 0.7450526315789474 and parameters: {'max_depth': 4, 'max_features': 17, 'min_samples_split': 13}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:40,175]\u001b[0m Trial 30 finished with value: 0.7505263157894737 and parameters: {'max_depth': 4, 'max_features': 18, 'min_samples_split': 6}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:40,779]\u001b[0m Trial 31 finished with value: 0.754072874493927 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 4}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:41,473]\u001b[0m Trial 32 finished with value: 0.7552226720647772 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 3}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:42,046]\u001b[0m Trial 33 finished with value: 0.7371983805668016 and parameters: {'max_depth': 4, 'max_features': 19, 'min_samples_split': 3}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:42,498]\u001b[0m Trial 34 finished with value: 0.7552712550607287 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 2}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:42,963]\u001b[0m Trial 35 finished with value: 0.7552712550607287 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 2}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:43,401]\u001b[0m Trial 36 finished with value: 0.7371983805668016 and parameters: {'max_depth': 4, 'max_features': 19, 'min_samples_split': 2}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:43,959]\u001b[0m Trial 37 finished with value: 0.7376518218623481 and parameters: {'max_depth': 4, 'max_features': 19, 'min_samples_split': 17}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:44,405]\u001b[0m Trial 38 finished with value: 0.7517085020242914 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 6}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:44,853]\u001b[0m Trial 39 finished with value: 0.7507368421052633 and parameters: {'max_depth': 4, 'max_features': 18, 'min_samples_split': 3}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:45,287]\u001b[0m Trial 40 finished with value: 0.74982995951417 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 14}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:45,985]\u001b[0m Trial 41 finished with value: 0.7552712550607287 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 2}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:46,640]\u001b[0m Trial 42 finished with value: 0.7371983805668016 and parameters: {'max_depth': 4, 'max_features': 19, 'min_samples_split': 2}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:47,307]\u001b[0m Trial 43 finished with value: 0.7497004048582996 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 21}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:47,869]\u001b[0m Trial 44 finished with value: 0.7371983805668016 and parameters: {'max_depth': 4, 'max_features': 19, 'min_samples_split': 5}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:48,443]\u001b[0m Trial 45 finished with value: 0.7552226720647772 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 3}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:48,986]\u001b[0m Trial 46 finished with value: 0.7507368421052633 and parameters: {'max_depth': 4, 'max_features': 18, 'min_samples_split': 2}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:49,622]\u001b[0m Trial 47 finished with value: 0.7307206477732794 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 7}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:50,180]\u001b[0m Trial 48 finished with value: 0.7371983805668016 and parameters: {'max_depth': 4, 'max_features': 19, 'min_samples_split': 4}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:50,863]\u001b[0m Trial 49 finished with value: 0.7549797570850203 and parameters: {'max_depth': 5, 'max_features': 20, 'min_samples_split': 24}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:51,440]\u001b[0m Trial 50 finished with value: 0.7499109311740892 and parameters: {'max_depth': 4, 'max_features': 18, 'min_samples_split': 16}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:51,905]\u001b[0m Trial 51 finished with value: 0.7552226720647772 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 3}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:52,355]\u001b[0m Trial 52 finished with value: 0.7552226720647772 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 3}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:53,020]\u001b[0m Trial 53 finished with value: 0.7371983805668016 and parameters: {'max_depth': 4, 'max_features': 19, 'min_samples_split': 5}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:53,591]\u001b[0m Trial 54 finished with value: 0.7552712550607287 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 2}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:54,153]\u001b[0m Trial 55 finished with value: 0.7552712550607287 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 2}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:54,605]\u001b[0m Trial 56 finished with value: 0.7552712550607287 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 2}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:55,151]\u001b[0m Trial 57 finished with value: 0.7368582995951419 and parameters: {'max_depth': 3, 'max_features': 19, 'min_samples_split': 6}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:55,712]\u001b[0m Trial 58 finished with value: 0.7371983805668016 and parameters: {'max_depth': 4, 'max_features': 19, 'min_samples_split': 4}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:56,289]\u001b[0m Trial 59 finished with value: 0.7552712550607287 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 2}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:56,738]\u001b[0m Trial 60 finished with value: 0.75051012145749 and parameters: {'max_depth': 4, 'max_features': 18, 'min_samples_split': 7}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:57,285]\u001b[0m Trial 61 finished with value: 0.7552712550607287 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 2}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:57,842]\u001b[0m Trial 62 finished with value: 0.7552712550607287 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 2}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:58,293]\u001b[0m Trial 63 finished with value: 0.7371983805668016 and parameters: {'max_depth': 4, 'max_features': 19, 'min_samples_split': 4}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:58,870]\u001b[0m Trial 64 finished with value: 0.7521457489878542 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 5}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:03:59,328]\u001b[0m Trial 65 finished with value: 0.7552226720647772 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 3}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:00,013]\u001b[0m Trial 66 finished with value: 0.7451983805668015 and parameters: {'max_depth': 4, 'max_features': 17, 'min_samples_split': 2}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:00,511]\u001b[0m Trial 67 finished with value: 0.7371983805668016 and parameters: {'max_depth': 4, 'max_features': 19, 'min_samples_split': 4}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:00,975]\u001b[0m Trial 68 finished with value: 0.754072874493927 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 4}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:01,408]\u001b[0m Trial 69 finished with value: 0.7371983805668016 and parameters: {'max_depth': 4, 'max_features': 19, 'min_samples_split': 3}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:01,841]\u001b[0m Trial 70 finished with value: 0.7486153846153847 and parameters: {'max_depth': 5, 'max_features': 11, 'min_samples_split': 5}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:02,415]\u001b[0m Trial 71 finished with value: 0.7552712550607287 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 2}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:02,881]\u001b[0m Trial 72 finished with value: 0.7552712550607287 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 2}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:03,548]\u001b[0m Trial 73 finished with value: 0.7552712550607287 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 2}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:04,012]\u001b[0m Trial 74 finished with value: 0.7552226720647772 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 3}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:04,463]\u001b[0m Trial 75 finished with value: 0.7371983805668016 and parameters: {'max_depth': 4, 'max_features': 19, 'min_samples_split': 2}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:04,947]\u001b[0m Trial 76 finished with value: 0.7517085020242914 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 6}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:05,600]\u001b[0m Trial 77 finished with value: 0.7371983805668016 and parameters: {'max_depth': 4, 'max_features': 19, 'min_samples_split': 3}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:06,163]\u001b[0m Trial 78 finished with value: 0.74982995951417 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 12}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:06,829]\u001b[0m Trial 79 finished with value: 0.7371983805668016 and parameters: {'max_depth': 4, 'max_features': 19, 'min_samples_split': 4}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:07,483]\u001b[0m Trial 80 finished with value: 0.7521457489878542 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 5}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:07,935]\u001b[0m Trial 81 finished with value: 0.7552712550607287 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 2}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:08,494]\u001b[0m Trial 82 finished with value: 0.7552712550607287 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 2}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:09,073]\u001b[0m Trial 83 finished with value: 0.7552226720647772 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 3}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:09,725]\u001b[0m Trial 84 finished with value: 0.7371983805668016 and parameters: {'max_depth': 4, 'max_features': 19, 'min_samples_split': 2}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:10,283]\u001b[0m Trial 85 finished with value: 0.7552226720647772 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 3}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:10,733]\u001b[0m Trial 86 finished with value: 0.7504777327935224 and parameters: {'max_depth': 4, 'max_features': 18, 'min_samples_split': 4}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:11,399]\u001b[0m Trial 87 finished with value: 0.7376518218623481 and parameters: {'max_depth': 4, 'max_features': 19, 'min_samples_split': 19}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:11,863]\u001b[0m Trial 88 finished with value: 0.7552712550607287 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 2}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:12,532]\u001b[0m Trial 89 finished with value: 0.7552712550607287 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 2}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:12,982]\u001b[0m Trial 90 finished with value: 0.7552226720647772 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 3}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:13,433]\u001b[0m Trial 91 finished with value: 0.7552712550607287 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 2}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:13,887]\u001b[0m Trial 92 finished with value: 0.7552226720647772 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 3}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:14,462]\u001b[0m Trial 93 finished with value: 0.7371983805668016 and parameters: {'max_depth': 4, 'max_features': 19, 'min_samples_split': 4}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:15,025]\u001b[0m Trial 94 finished with value: 0.7552712550607287 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 2}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:15,596]\u001b[0m Trial 95 finished with value: 0.7552226720647772 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 3}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:16,267]\u001b[0m Trial 96 finished with value: 0.7371983805668016 and parameters: {'max_depth': 4, 'max_features': 19, 'min_samples_split': 2}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:16,844]\u001b[0m Trial 97 finished with value: 0.754072874493927 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 4}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:17,296]\u001b[0m Trial 98 finished with value: 0.7552712550607287 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 2}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:17,859]\u001b[0m Trial 99 finished with value: 0.7371983805668016 and parameters: {'max_depth': 4, 'max_features': 19, 'min_samples_split': 3}. Best is trial 27 with value: 0.7552712550607287.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Tuning hyperparameters\n",
    "#Step 1. Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'max_depth': trial.suggest_int('max_depth',3,5,1),\n",
    "        'max_features' : trial.suggest_int(\"max_features\",10,20,1),\n",
    "        'min_samples_split':trial.suggest_int('min_samples_split',2,25,1)\n",
    "    }\n",
    "    model = DecisionTreeClassifier(**param,random_state=1)\n",
    "\n",
    " \n",
    "# Step 2: Scoring method:\n",
    "    score = model_selection.cross_val_score(model, X, y, n_jobs=12, scoring=\"accuracy\",cv=Cv_optuna)\n",
    "    accuracy = score.mean()\n",
    "    return accuracy\n",
    " \n",
    "# Step 3: Running it\n",
    "study = optuna.create_study(direction=\"maximize\",sampler=optuna.samplers.TPESampler(seed=0))\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28ac7c97-6c21-4117-add1-12a7be2e8f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are : \n",
      "{'max_depth': 4, 'max_features': 20, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "# Getting the best parameters:\n",
    "print(f\"The best parameters are : \\n{study.best_params}\")\n",
    "# Setting the best model\n",
    "clf =DecisionTreeClassifier(max_depth = study.best_params['max_depth']\n",
    "              ,max_features = study.best_params['max_features']\n",
    "              #,n_estimators = study.best_params['n_estimators']\n",
    "              #,learning_rate = study.best_params['learning_rate']\n",
    "              ,min_samples_split= study.best_params['min_samples_split']\n",
    "              ,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0b71e00-d630-4d2b-b1d4-a79589e02288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.755271</td>\n",
       "      <td>0.001920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.766312</td>\n",
       "      <td>0.000759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.798802</td>\n",
       "      <td>0.002613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.731317</td>\n",
       "      <td>0.003648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.885609</td>\n",
       "      <td>0.009367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.841363</td>\n",
       "      <td>0.001637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean        Se\n",
       "Accuracy_test   0.755271  0.001920\n",
       "Accuracy_train  0.766312  0.000759\n",
       "F1 Score        0.798802  0.002613\n",
       "Precision       0.731317  0.003648\n",
       "Recall          0.885609  0.009367\n",
       "Roc_auc         0.841363  0.001637"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model2\n",
    "Model2=Model_results(clf,X,y,Cv_model)\n",
    "Model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4359628-316f-47c6-bdef-b9471932492e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Method</th>\n",
       "      <th colspan=\"4\" halign=\"left\">DT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Model 1</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Model 2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Values</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.774494</td>\n",
       "      <td>0.001772</td>\n",
       "      <td>0.755271</td>\n",
       "      <td>0.001920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.998433</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.766312</td>\n",
       "      <td>0.000759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.795780</td>\n",
       "      <td>0.001632</td>\n",
       "      <td>0.798802</td>\n",
       "      <td>0.002613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.794873</td>\n",
       "      <td>0.001899</td>\n",
       "      <td>0.731317</td>\n",
       "      <td>0.003648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.796916</td>\n",
       "      <td>0.002302</td>\n",
       "      <td>0.885609</td>\n",
       "      <td>0.009367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.772521</td>\n",
       "      <td>0.001802</td>\n",
       "      <td>0.841363</td>\n",
       "      <td>0.001637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Method                DT                              \n",
       "Model            Model 1             Model 2          \n",
       "Values              Mean        Se      Mean        Se\n",
       "Accuracy_test   0.774494  0.001772  0.755271  0.001920\n",
       "Accuracy_train  0.998433  0.000053  0.766312  0.000759\n",
       "F1 Score        0.795780  0.001632  0.798802  0.002613\n",
       "Precision       0.794873  0.001899  0.731317  0.003648\n",
       "Recall          0.796916  0.002302  0.885609  0.009367\n",
       "Roc_auc         0.772521  0.001802  0.841363  0.001637"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saving the data of model performance\n",
    "Model_data=pd.concat([Model1,Model2],axis=1)\n",
    "Model_data.columns = [['DT']*4,['Model 1','Model 1', 'Model 2','Model 2'], ['Mean', 'Se', 'Mean', 'Se']]  \n",
    "Model_data.columns.names=['Method','Model','Values']\n",
    "Model_data.to_csv(\"./DT_model_lasso_data.csv\",sep=',')\n",
    "Model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6047485d-89ab-471b-b393-08d893b317bc",
   "metadata": {},
   "source": [
    "## 1.2 LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e67efc5-4154-43ea-ad89-57f6ba68ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=LogisticRegression(solver='liblinear',random_state=0,dual=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e72f778-0bc0-4da2-b17d-d66357ab161c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.797457</td>\n",
       "      <td>0.001394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.813190</td>\n",
       "      <td>0.000392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.821570</td>\n",
       "      <td>0.001244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.798959</td>\n",
       "      <td>0.001621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.845727</td>\n",
       "      <td>0.002045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.896650</td>\n",
       "      <td>0.001078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean        Se\n",
       "Accuracy_test   0.797457  0.001394\n",
       "Accuracy_train  0.813190  0.000392\n",
       "F1 Score        0.821570  0.001244\n",
       "Precision       0.798959  0.001621\n",
       "Recall          0.845727  0.002045\n",
       "Roc_auc         0.896650  0.001078"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model1 \n",
    "Model1_clf=clf\n",
    "#Model1\n",
    "Model1=Model_results(Model1_clf,X,y,Cv_model)\n",
    "Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9338635-62a3-4fe9-912b-6acc77ef994a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2024-11-16 01:04:38,842]\u001b[0m A new study created in memory with name: no-name-332d800e-cfc3-42e7-b5be-c8c8c6667280\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:46,312]\u001b[0m Trial 0 finished with value: 0.7937489878542511 and parameters: {'logreg_c': 0.3177840006884068, 'l1_ratio': 0.7482920440979423, 'max_iter': 100}. Best is trial 0 with value: 0.7937489878542511.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:49,243]\u001b[0m Trial 1 finished with value: 0.7787368421052631 and parameters: {'logreg_c': 0.0651621545821569, 'l1_ratio': 0.23208030173540176, 'max_iter': 275}. Best is trial 0 with value: 0.7937489878542511.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:51,217]\u001b[0m Trial 2 finished with value: 0.7527935222672066 and parameters: {'logreg_c': 0.013108749615263334, 'l1_ratio': 0.411004654338743, 'max_iter': 854}. Best is trial 0 with value: 0.7937489878542511.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:04:58,888]\u001b[0m Trial 3 finished with value: 0.7994979757085021 and parameters: {'logreg_c': 1.7096232052870346, 'l1_ratio': 0.4772750629629653, 'max_iter': 1402}. Best is trial 3 with value: 0.7994979757085021.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:05:01,102]\u001b[0m Trial 4 finished with value: 0.7579595141700405 and parameters: {'logreg_c': 0.016854407828169382, 'l1_ratio': 0.8903056927518509, 'max_iter': 152}. Best is trial 3 with value: 0.7994979757085021.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:05:14,443]\u001b[0m Trial 5 finished with value: 0.804906882591093 and parameters: {'logreg_c': 10.539137268289434, 'l1_ratio': 0.4755743221304143, 'max_iter': 1162}. Best is trial 5 with value: 0.804906882591093.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:05:16,478]\u001b[0m Trial 6 finished with value: 0.7476599190283401 and parameters: {'logreg_c': 0.006955392321661603, 'l1_ratio': 0.2782913401763909, 'max_iter': 1622}. Best is trial 5 with value: 0.804906882591093.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:06:22,616]\u001b[0m Trial 7 finished with value: 0.808161943319838 and parameters: {'logreg_c': 645.0144652189372, 'l1_ratio': 0.38208176034331853, 'max_iter': 1416}. Best is trial 7 with value: 0.808161943319838.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:07:04,010]\u001b[0m Trial 8 finished with value: 0.8084858299595141 and parameters: {'logreg_c': 181.27374779133626, 'l1_ratio': 0.9051459971534626, 'max_iter': 261}. Best is trial 8 with value: 0.8084858299595141.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:07:05,906]\u001b[0m Trial 9 finished with value: 0.7427692307692308 and parameters: {'logreg_c': 0.001715255021408637, 'l1_ratio': 0.25284737760811204, 'max_iter': 1769}. Best is trial 8 with value: 0.8084858299595141.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:08:21,112]\u001b[0m Trial 10 finished with value: 0.8079514170040486 and parameters: {'logreg_c': 909.7939268284556, 'l1_ratio': 0.9808743317423054, 'max_iter': 644}. Best is trial 8 with value: 0.8084858299595141.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:09:35,601]\u001b[0m Trial 11 finished with value: 0.8081133603238867 and parameters: {'logreg_c': 843.2126062012203, 'l1_ratio': 0.7030698971831799, 'max_iter': 1245}. Best is trial 8 with value: 0.8084858299595141.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:10:02,785]\u001b[0m Trial 12 finished with value: 0.8075627530364372 and parameters: {'logreg_c': 59.11595197989844, 'l1_ratio': 0.6400356094425337, 'max_iter': 550}. Best is trial 8 with value: 0.8084858299595141.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:10:35,718]\u001b[0m Trial 13 finished with value: 0.8079028340080971 and parameters: {'logreg_c': 87.98779560699732, 'l1_ratio': 0.13275033245614, 'max_iter': 926}. Best is trial 8 with value: 0.8084858299595141.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:11:08,353]\u001b[0m Trial 14 finished with value: 0.8079352226720649 and parameters: {'logreg_c': 109.25274610160965, 'l1_ratio': 0.8522671394998345, 'max_iter': 1519}. Best is trial 8 with value: 0.8084858299595141.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:11:25,412]\u001b[0m Trial 15 finished with value: 0.8052631578947369 and parameters: {'logreg_c': 14.61008230251838, 'l1_ratio': 0.3720034754084857, 'max_iter': 1898}. Best is trial 8 with value: 0.8084858299595141.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:12:19,626]\u001b[0m Trial 16 finished with value: 0.8085506072874494 and parameters: {'logreg_c': 397.9631793209919, 'l1_ratio': 0.6000539592896698, 'max_iter': 484}. Best is trial 16 with value: 0.8085506072874494.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:12:29,998]\u001b[0m Trial 17 finished with value: 0.8011497975708503 and parameters: {'logreg_c': 2.8294524128793763, 'l1_ratio': 0.591689652204593, 'max_iter': 445}. Best is trial 16 with value: 0.8085506072874494.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:13:09,137]\u001b[0m Trial 18 finished with value: 0.808323886639676 and parameters: {'logreg_c': 171.54975942733847, 'l1_ratio': 0.8079642983057069, 'max_iter': 713}. Best is trial 16 with value: 0.8085506072874494.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:13:25,498]\u001b[0m Trial 19 finished with value: 0.805246963562753 and parameters: {'logreg_c': 13.712290398106667, 'l1_ratio': 0.997728233390458, 'max_iter': 422}. Best is trial 16 with value: 0.8085506072874494.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:13:30,373]\u001b[0m Trial 20 finished with value: 0.790834008097166 and parameters: {'logreg_c': 0.18326146322209286, 'l1_ratio': 0.6685208082301097, 'max_iter': 317}. Best is trial 16 with value: 0.8085506072874494.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:14:13,038]\u001b[0m Trial 21 finished with value: 0.8084210526315789 and parameters: {'logreg_c': 211.97018038007016, 'l1_ratio': 0.7967581704376829, 'max_iter': 733}. Best is trial 16 with value: 0.8085506072874494.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:14:58,142]\u001b[0m Trial 22 finished with value: 0.8082591093117409 and parameters: {'logreg_c': 231.02378459332883, 'l1_ratio': 0.9048211043134482, 'max_iter': 792}. Best is trial 16 with value: 0.8085506072874494.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:15:26,034]\u001b[0m Trial 23 finished with value: 0.8068987854251013 and parameters: {'logreg_c': 38.275212589793654, 'l1_ratio': 0.789368431303605, 'max_iter': 988}. Best is trial 16 with value: 0.8085506072874494.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:16:16,603]\u001b[0m Trial 24 finished with value: 0.8083400809716598 and parameters: {'logreg_c': 292.96714708367233, 'l1_ratio': 0.5826822212996895, 'max_iter': 556}. Best is trial 16 with value: 0.8085506072874494.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:16:29,346]\u001b[0m Trial 25 finished with value: 0.8023967611336031 and parameters: {'logreg_c': 4.178446593488752, 'l1_ratio': 0.9279226072545412, 'max_iter': 301}. Best is trial 16 with value: 0.8085506072874494.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:16:54,976]\u001b[0m Trial 26 finished with value: 0.8070445344129555 and parameters: {'logreg_c': 39.12728082257864, 'l1_ratio': 0.7618739218863776, 'max_iter': 541}. Best is trial 16 with value: 0.8085506072874494.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:17:48,790]\u001b[0m Trial 27 finished with value: 0.8082591093117408 and parameters: {'logreg_c': 341.26183090740665, 'l1_ratio': 0.8467124740947369, 'max_iter': 721}. Best is trial 16 with value: 0.8085506072874494.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:18:11,438]\u001b[0m Trial 28 finished with value: 0.8063967611336034 and parameters: {'logreg_c': 27.308870540897185, 'l1_ratio': 0.7191108089812019, 'max_iter': 203}. Best is trial 16 with value: 0.8085506072874494.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:18:18,730]\u001b[0m Trial 29 finished with value: 0.7952712550607289 and parameters: {'logreg_c': 0.5401885362989322, 'l1_ratio': 0.5037475263725311, 'max_iter': 117}. Best is trial 16 with value: 0.8085506072874494.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:19:13,146]\u001b[0m Trial 30 finished with value: 0.8085182186234817 and parameters: {'logreg_c': 355.73515228051855, 'l1_ratio': 0.6487412771305163, 'max_iter': 409}. Best is trial 16 with value: 0.8085506072874494.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:20:08,830]\u001b[0m Trial 31 finished with value: 0.8084534412955466 and parameters: {'logreg_c': 350.9981509016477, 'l1_ratio': 0.6058502538751018, 'max_iter': 421}. Best is trial 16 with value: 0.8085506072874494.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:21:07,123]\u001b[0m Trial 32 finished with value: 0.8085506072874494 and parameters: {'logreg_c': 421.4582992509824, 'l1_ratio': 0.6223705679607543, 'max_iter': 354}. Best is trial 16 with value: 0.8085506072874494.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:21:42,002]\u001b[0m Trial 33 finished with value: 0.8078056680161942 and parameters: {'logreg_c': 104.94288500291744, 'l1_ratio': 0.5385077223404007, 'max_iter': 281}. Best is trial 16 with value: 0.8085506072874494.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:22:44,815]\u001b[0m Trial 34 finished with value: 0.8084858299595141 and parameters: {'logreg_c': 514.433674515784, 'l1_ratio': 0.6847775888226406, 'max_iter': 421}. Best is trial 16 with value: 0.8085506072874494.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:24:08,764]\u001b[0m Trial 35 finished with value: 0.8078056680161945 and parameters: {'logreg_c': 988.2435872879354, 'l1_ratio': 0.6616657129443551, 'max_iter': 372}. Best is trial 16 with value: 0.8085506072874494.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:24:44,718]\u001b[0m Trial 36 finished with value: 0.8078704453441297 and parameters: {'logreg_c': 101.70771817796839, 'l1_ratio': 0.42759696902121375, 'max_iter': 221}. Best is trial 16 with value: 0.8085506072874494.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:24:58,997]\u001b[0m Trial 37 finished with value: 0.8036275303643723 and parameters: {'logreg_c': 6.3846982024404975, 'l1_ratio': 0.5502457687261321, 'max_iter': 599}. Best is trial 16 with value: 0.8085506072874494.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:26:01,829]\u001b[0m Trial 38 finished with value: 0.8085344129554656 and parameters: {'logreg_c': 446.40159696354067, 'l1_ratio': 0.7143538405440573, 'max_iter': 476}. Best is trial 16 with value: 0.8085506072874494.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:26:05,513]\u001b[0m Trial 39 finished with value: 0.7777327935222672 and parameters: {'logreg_c': 0.06062882247403022, 'l1_ratio': 0.7409379619296871, 'max_iter': 869}. Best is trial 16 with value: 0.8085506072874494.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:26:27,266]\u001b[0m Trial 40 finished with value: 0.8063157894736844 and parameters: {'logreg_c': 25.631663683474887, 'l1_ratio': 0.6337949621799774, 'max_iter': 477}. Best is trial 16 with value: 0.8085506072874494.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:27:29,314]\u001b[0m Trial 41 finished with value: 0.8084696356275303 and parameters: {'logreg_c': 474.37044060772416, 'l1_ratio': 0.5419328224374047, 'max_iter': 141}. Best is trial 16 with value: 0.8085506072874494.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:28:30,926]\u001b[0m Trial 42 finished with value: 0.8086801619433198 and parameters: {'logreg_c': 480.7815122005421, 'l1_ratio': 0.6943576864503579, 'max_iter': 650}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:29:34,786]\u001b[0m Trial 43 finished with value: 0.8086153846153846 and parameters: {'logreg_c': 486.5832968616945, 'l1_ratio': 0.46219410536633865, 'max_iter': 1124}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:30:04,527]\u001b[0m Trial 44 finished with value: 0.8075789473684212 and parameters: {'logreg_c': 60.87201705775844, 'l1_ratio': 0.4718177382001375, 'max_iter': 1169}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:31:32,347]\u001b[0m Trial 45 finished with value: 0.8077732793522265 and parameters: {'logreg_c': 991.4209385029247, 'l1_ratio': 0.49900204822882926, 'max_iter': 1255}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:32:13,019]\u001b[0m Trial 46 finished with value: 0.8083562753036436 and parameters: {'logreg_c': 152.96922149185383, 'l1_ratio': 0.43526267472997593, 'max_iter': 1069}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:33:08,081]\u001b[0m Trial 47 finished with value: 0.8082914979757085 and parameters: {'logreg_c': 565.3115495218777, 'l1_ratio': 0.3390657122505642, 'max_iter': 652}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:33:32,290]\u001b[0m Trial 48 finished with value: 0.8075789473684211 and parameters: {'logreg_c': 62.54821355953711, 'l1_ratio': 0.31428931006175564, 'max_iter': 821}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:33:33,978]\u001b[0m Trial 49 finished with value: 0.7432550607287449 and parameters: {'logreg_c': 0.0015035267654569466, 'l1_ratio': 0.6140630557929537, 'max_iter': 1018}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:33:41,199]\u001b[0m Trial 50 finished with value: 0.7982995951417005 and parameters: {'logreg_c': 1.2104916272673802, 'l1_ratio': 0.709203601402445, 'max_iter': 531}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:34:28,772]\u001b[0m Trial 51 finished with value: 0.8085991902834005 and parameters: {'logreg_c': 415.4791453113777, 'l1_ratio': 0.5723567269576718, 'max_iter': 356}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:35:00,552]\u001b[0m Trial 52 finished with value: 0.8085344129554656 and parameters: {'logreg_c': 139.20771001884222, 'l1_ratio': 0.5689372098596256, 'max_iter': 1361}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:35:33,246]\u001b[0m Trial 53 finished with value: 0.8083400809716601 and parameters: {'logreg_c': 155.21231132001026, 'l1_ratio': 0.5668774447327277, 'max_iter': 1313}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:36:25,794]\u001b[0m Trial 54 finished with value: 0.8082429149797571 and parameters: {'logreg_c': 563.1831326645386, 'l1_ratio': 0.5153335647657853, 'max_iter': 1566}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:37:02,620]\u001b[0m Trial 55 finished with value: 0.8083076923076921 and parameters: {'logreg_c': 240.806240256624, 'l1_ratio': 0.16867194899865523, 'max_iter': 326}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:37:34,479]\u001b[0m Trial 56 finished with value: 0.8083886639676113 and parameters: {'logreg_c': 151.6071862622638, 'l1_ratio': 0.45820127730439186, 'max_iter': 918}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:38:30,633]\u001b[0m Trial 57 finished with value: 0.8082267206477733 and parameters: {'logreg_c': 625.5263962032639, 'l1_ratio': 0.6161890396692824, 'max_iter': 479}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:39:12,987]\u001b[0m Trial 58 finished with value: 0.8082591093117409 and parameters: {'logreg_c': 318.53986817262665, 'l1_ratio': 0.38438512758363624, 'max_iter': 654}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:39:38,971]\u001b[0m Trial 59 finished with value: 0.8077732793522269 and parameters: {'logreg_c': 81.81289443738181, 'l1_ratio': 0.576561071679956, 'max_iter': 1352}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:39:40,787]\u001b[0m Trial 60 finished with value: 0.7437408906882591 and parameters: {'logreg_c': 0.003840561631688238, 'l1_ratio': 0.7484655123671795, 'max_iter': 1110}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:40:40,216]\u001b[0m Trial 61 finished with value: 0.808161943319838 and parameters: {'logreg_c': 699.0168432198719, 'l1_ratio': 0.6804083430096539, 'max_iter': 359}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:41:14,288]\u001b[0m Trial 62 finished with value: 0.8083238866396762 and parameters: {'logreg_c': 189.75052875898396, 'l1_ratio': 0.528339093161829, 'max_iter': 1459}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:42:09,403]\u001b[0m Trial 63 finished with value: 0.808663967611336 and parameters: {'logreg_c': 417.70308793299967, 'l1_ratio': 0.6290193953811514, 'max_iter': 1204}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:43:27,203]\u001b[0m Trial 64 finished with value: 0.8077894736842106 and parameters: {'logreg_c': 988.0956644311622, 'l1_ratio': 0.6340624552608236, 'max_iter': 1269}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:44:16,724]\u001b[0m Trial 65 finished with value: 0.8085991902834008 and parameters: {'logreg_c': 365.13571498833375, 'l1_ratio': 0.7210548094206097, 'max_iter': 1123}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:44:56,185]\u001b[0m Trial 66 finished with value: 0.8082914979757087 and parameters: {'logreg_c': 269.308599835345, 'l1_ratio': 0.8343511381150887, 'max_iter': 1155}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:45:17,551]\u001b[0m Trial 67 finished with value: 0.8067854251012146 and parameters: {'logreg_c': 37.522128186841876, 'l1_ratio': 0.781632227499007, 'max_iter': 1174}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:46:07,617]\u001b[0m Trial 68 finished with value: 0.8086153846153844 and parameters: {'logreg_c': 424.72665266767, 'l1_ratio': 0.5983094666031161, 'max_iter': 998}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:46:33,458]\u001b[0m Trial 69 finished with value: 0.8078056680161945 and parameters: {'logreg_c': 82.42760834989193, 'l1_ratio': 0.6897945271419422, 'max_iter': 1051}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:47:11,115]\u001b[0m Trial 70 finished with value: 0.808323886639676 and parameters: {'logreg_c': 239.44970679792007, 'l1_ratio': 0.5946714885290528, 'max_iter': 948}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:47:59,370]\u001b[0m Trial 71 finished with value: 0.8086801619433198 and parameters: {'logreg_c': 411.36554260113667, 'l1_ratio': 0.6581589961213802, 'max_iter': 1095}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:49:02,543]\u001b[0m Trial 72 finished with value: 0.8081457489878542 and parameters: {'logreg_c': 706.1937184804442, 'l1_ratio': 0.6695302916084884, 'max_iter': 1093}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:49:54,703]\u001b[0m Trial 73 finished with value: 0.8085344129554657 and parameters: {'logreg_c': 385.27380743397345, 'l1_ratio': 0.6441153454352329, 'max_iter': 975}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:50:50,118]\u001b[0m Trial 74 finished with value: 0.8085668016194332 and parameters: {'logreg_c': 447.8811929867357, 'l1_ratio': 0.622061940040242, 'max_iter': 1188}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:50:52,992]\u001b[0m Trial 75 finished with value: 0.7692955465587044 and parameters: {'logreg_c': 0.031045638444659712, 'l1_ratio': 0.7346487480941478, 'max_iter': 1435}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:51:35,379]\u001b[0m Trial 76 finished with value: 0.8082753036437248 and parameters: {'logreg_c': 236.0227192318438, 'l1_ratio': 0.7681293798633823, 'max_iter': 1232}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:52:09,309]\u001b[0m Trial 77 finished with value: 0.8079676113360325 and parameters: {'logreg_c': 112.82239056735646, 'l1_ratio': 0.6511892711139134, 'max_iter': 890}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:53:15,559]\u001b[0m Trial 78 finished with value: 0.8082105263157895 and parameters: {'logreg_c': 628.0180384617826, 'l1_ratio': 0.48281227750996053, 'max_iter': 1206}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:54:07,238]\u001b[0m Trial 79 finished with value: 0.808582995951417 and parameters: {'logreg_c': 371.302543959048, 'l1_ratio': 0.5598554801900355, 'max_iter': 1109}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:54:13,229]\u001b[0m Trial 80 finished with value: 0.794655870445344 and parameters: {'logreg_c': 0.37976816840820704, 'l1_ratio': 0.8205977629601079, 'max_iter': 1123}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:55:05,833]\u001b[0m Trial 81 finished with value: 0.8083724696356275 and parameters: {'logreg_c': 347.64048411482554, 'l1_ratio': 0.5515412009524621, 'max_iter': 1028}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:56:17,979]\u001b[0m Trial 82 finished with value: 0.8082267206477732 and parameters: {'logreg_c': 770.710170549282, 'l1_ratio': 0.6034446753810235, 'max_iter': 1310}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:57:12,000]\u001b[0m Trial 83 finished with value: 0.8086153846153846 and parameters: {'logreg_c': 419.89715414263816, 'l1_ratio': 0.7014308895766974, 'max_iter': 1206}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:57:52,550]\u001b[0m Trial 84 finished with value: 0.8082267206477732 and parameters: {'logreg_c': 201.55367062070073, 'l1_ratio': 0.6957005538126967, 'max_iter': 808}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:58:39,163]\u001b[0m Trial 85 finished with value: 0.8082753036437248 and parameters: {'logreg_c': 282.5129479743071, 'l1_ratio': 0.7264900377263649, 'max_iter': 1133}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 01:59:13,543]\u001b[0m Trial 86 finished with value: 0.8079676113360325 and parameters: {'logreg_c': 118.49601828351747, 'l1_ratio': 0.6604798172814363, 'max_iter': 983}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:00:24,510]\u001b[0m Trial 87 finished with value: 0.8081943319838056 and parameters: {'logreg_c': 768.020338775785, 'l1_ratio': 0.520444870635168, 'max_iter': 1071}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:01:23,312]\u001b[0m Trial 88 finished with value: 0.8086153846153847 and parameters: {'logreg_c': 483.4968919777879, 'l1_ratio': 0.5659134866316831, 'max_iter': 1715}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:02:23,121]\u001b[0m Trial 89 finished with value: 0.8085344129554655 and parameters: {'logreg_c': 519.1565155107692, 'l1_ratio': 0.7068750393103095, 'max_iter': 1726}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:02:27,163]\u001b[0m Trial 90 finished with value: 0.7857004048582996 and parameters: {'logreg_c': 0.12672161574160534, 'l1_ratio': 0.5862935400855835, 'max_iter': 1766}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:03:22,297]\u001b[0m Trial 91 finished with value: 0.8086153846153847 and parameters: {'logreg_c': 427.24168355551524, 'l1_ratio': 0.5632760717812343, 'max_iter': 1850}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:04:02,569]\u001b[0m Trial 92 finished with value: 0.8083724696356276 and parameters: {'logreg_c': 189.21722437029192, 'l1_ratio': 0.44448553001672, 'max_iter': 1937}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:05:00,599]\u001b[0m Trial 93 finished with value: 0.8084858299595141 and parameters: {'logreg_c': 506.950035077156, 'l1_ratio': 0.4032955087673354, 'max_iter': 1859}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:05:48,615]\u001b[0m Trial 94 finished with value: 0.8083724696356275 and parameters: {'logreg_c': 295.01844278559327, 'l1_ratio': 0.4962461702446258, 'max_iter': 1975}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:07:08,062]\u001b[0m Trial 95 finished with value: 0.8078542510121456 and parameters: {'logreg_c': 937.3317275288405, 'l1_ratio': 0.6710239033566312, 'max_iter': 1623}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:08:04,655]\u001b[0m Trial 96 finished with value: 0.8084858299595143 and parameters: {'logreg_c': 453.0227360906444, 'l1_ratio': 0.5777971417989136, 'max_iter': 1825}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:08:33,121]\u001b[0m Trial 97 finished with value: 0.80765991902834 and parameters: {'logreg_c': 68.41146455322642, 'l1_ratio': 0.6391037356132339, 'max_iter': 1273}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:09:09,013]\u001b[0m Trial 98 finished with value: 0.808323886639676 and parameters: {'logreg_c': 132.51192209218476, 'l1_ratio': 0.5306131648162634, 'max_iter': 1693}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:09:48,368]\u001b[0m Trial 99 finished with value: 0.8084048582995952 and parameters: {'logreg_c': 181.87007483198227, 'l1_ratio': 0.754619345662785, 'max_iter': 1382}. Best is trial 42 with value: 0.8086801619433198.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Tuning hyperparameters\n",
    "#Step 1. Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "    logreg_c = trial.suggest_float(\"logreg_c\", 1e-3,  1e3, log=True)\n",
    "    l1_ratio = trial.suggest_float(\"l1_ratio\",0.1,1,log=False) \n",
    "    #penalty = trial.suggest_categorical(\"penalty\",['l1','l2'])\n",
    "    max_iter = trial.suggest_int(\"max_iter\", 100,2000)\n",
    "    model =LogisticRegression(C=logreg_c,\n",
    "                              max_iter=max_iter,\n",
    "                              l1_ratio=l1_ratio,\n",
    "                              solver='liblinear',random_state=1)\n",
    "    \n",
    "# Step 2: Scoring method:\n",
    "    score = model_selection.cross_val_score(model, X, y, n_jobs=8, scoring=\"accuracy\",cv=Cv_optuna)\n",
    "    accuracy = score.mean()\n",
    "    return accuracy\n",
    "\n",
    "# Step 3: Running it\n",
    "study = optuna.create_study(direction=\"maximize\",sampler=optuna.samplers.TPESampler(seed=1))\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd09afa2-c7ae-4f28-acb3-9a0c06e89504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are : \n",
      "{'logreg_c': 480.7815122005421, 'l1_ratio': 0.6943576864503579, 'max_iter': 650}\n"
     ]
    }
   ],
   "source": [
    "# Getting the best parameters:\n",
    "print(f\"The best parameters are : \\n{study.best_params}\")\n",
    "# Setting the best model\n",
    "clf=LogisticRegression(C=study.best_params['logreg_c'],\n",
    "                              max_iter=study.best_params['max_iter'],\n",
    "                              l1_ratio=study.best_params['l1_ratio'],\n",
    "                              solver='liblinear',\n",
    "                              random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18f98263-5b44-4a58-ad76-bd2288347abf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.808680</td>\n",
       "      <td>0.001430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.841907</td>\n",
       "      <td>0.000393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.829373</td>\n",
       "      <td>0.001314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.816000</td>\n",
       "      <td>0.001531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.843377</td>\n",
       "      <td>0.002039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.902592</td>\n",
       "      <td>0.001069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean        Se\n",
       "Accuracy_test   0.808680  0.001430\n",
       "Accuracy_train  0.841907  0.000393\n",
       "F1 Score        0.829373  0.001314\n",
       "Precision       0.816000  0.001531\n",
       "Recall          0.843377  0.002039\n",
       "Roc_auc         0.902592  0.001069"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model2\n",
    "Model2=Model_results(clf,X,y,Cv_model)\n",
    "Model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0eba0a05-824b-4da4-8a16-535220341d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Method</th>\n",
       "      <th colspan=\"4\" halign=\"left\">LR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Model 1</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Model 2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Values</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.797457</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>0.808680</td>\n",
       "      <td>0.001430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.813190</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.841907</td>\n",
       "      <td>0.000393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.821570</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>0.829373</td>\n",
       "      <td>0.001314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.798959</td>\n",
       "      <td>0.001621</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>0.001531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.845727</td>\n",
       "      <td>0.002045</td>\n",
       "      <td>0.843377</td>\n",
       "      <td>0.002039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.896650</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>0.902592</td>\n",
       "      <td>0.001069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Method                LR                              \n",
       "Model            Model 1             Model 2          \n",
       "Values              Mean        Se      Mean        Se\n",
       "Accuracy_test   0.797457  0.001394  0.808680  0.001430\n",
       "Accuracy_train  0.813190  0.000392  0.841907  0.000393\n",
       "F1 Score        0.821570  0.001244  0.829373  0.001314\n",
       "Precision       0.798959  0.001621  0.816000  0.001531\n",
       "Recall          0.845727  0.002045  0.843377  0.002039\n",
       "Roc_auc         0.896650  0.001078  0.902592  0.001069"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saving the data of model performance\n",
    "Model_data=pd.concat([Model1,Model2],axis=1)\n",
    "Model_data.columns = [['LR']*4,['Model 1','Model 1', 'Model 2','Model 2'], ['Mean', 'Se', 'Mean', 'Se']]  \n",
    "Model_data.columns.names=['Method','Model','Values']\n",
    "Model_data.to_csv(\"./LR_model_lasso_data.csv\",sep=',')\n",
    "Model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0844c7c8-f6b1-410a-b253-879aedd14e57",
   "metadata": {},
   "source": [
    "## 1.3 RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d9c4731c-bbfe-47eb-bb7f-7d8b2909ea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=RandomForestClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ffdd2f35-ec29-4ef8-8309-6a84646f4539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.821879</td>\n",
       "      <td>0.001555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.998433</td>\n",
       "      <td>0.000053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.844143</td>\n",
       "      <td>0.001350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.815738</td>\n",
       "      <td>0.001724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.874802</td>\n",
       "      <td>0.001992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.915202</td>\n",
       "      <td>0.000983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean        Se\n",
       "Accuracy_test   0.821879  0.001555\n",
       "Accuracy_train  0.998433  0.000053\n",
       "F1 Score        0.844143  0.001350\n",
       "Precision       0.815738  0.001724\n",
       "Recall          0.874802  0.001992\n",
       "Roc_auc         0.915202  0.000983"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model1 （4175 descriptors）\n",
    "Model1_clf=clf\n",
    "#Model1\n",
    "Model1=Model_results(Model1_clf,X,y,Cv_model)\n",
    "Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6f4b6d2b-6cf0-4f44-8bd5-02b5d1377add",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2024-11-16 02:16:51,092]\u001b[0m A new study created in memory with name: no-name-25929d10-5859-4443-aa09-94574fdce85e\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:17:04,312]\u001b[0m Trial 0 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 594, 'max_depth': 16, 'max_features': 20, 'min_impurity_decrease': 2.724415914984484}. Best is trial 0 with value: 0.5514170040485831.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:17:12,629]\u001b[0m Trial 1 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 481, 'max_depth': 15, 'max_features': 16, 'min_impurity_decrease': 4.4588650039103985}. Best is trial 0 with value: 0.5514170040485831.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:17:31,468]\u001b[0m Trial 2 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 968, 'max_depth': 11, 'max_features': 25, 'min_impurity_decrease': 2.644474598764522}. Best is trial 0 with value: 0.5514170040485831.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:17:39,794]\u001b[0m Trial 3 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 611, 'max_depth': 19, 'max_features': 6, 'min_impurity_decrease': 0.43564649850770354}. Best is trial 0 with value: 0.5514170040485831.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:17:42,684]\u001b[0m Trial 4 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 118, 'max_depth': 18, 'max_features': 25, 'min_impurity_decrease': 4.3500607412340955}. Best is trial 0 with value: 0.5514170040485831.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:17:58,618]\u001b[0m Trial 5 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 981, 'max_depth': 17, 'max_features': 16, 'min_impurity_decrease': 3.902645881432277}. Best is trial 0 with value: 0.5514170040485831.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:18:01,977]\u001b[0m Trial 6 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 206, 'max_depth': 15, 'max_features': 8, 'min_impurity_decrease': 4.7233445852479194}. Best is trial 0 with value: 0.5514170040485831.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:18:10,251]\u001b[0m Trial 7 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 570, 'max_depth': 11, 'max_features': 11, 'min_impurity_decrease': 3.871168447171083}. Best is trial 0 with value: 0.5514170040485831.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:18:16,895]\u001b[0m Trial 8 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 510, 'max_depth': 14, 'max_features': 5, 'min_impurity_decrease': 3.0881774853793855}. Best is trial 0 with value: 0.5514170040485831.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:18:30,869]\u001b[0m Trial 9 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 651, 'max_depth': 14, 'max_features': 29, 'min_impurity_decrease': 3.409101495517417}. Best is trial 0 with value: 0.5514170040485831.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:18:45,978]\u001b[0m Trial 10 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 790, 'max_depth': 7, 'max_features': 21, 'min_impurity_decrease': 1.5046577569438309}. Best is trial 0 with value: 0.5514170040485831.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:18:52,940]\u001b[0m Trial 11 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 397, 'max_depth': 20, 'max_features': 16, 'min_impurity_decrease': 1.9887909510549393}. Best is trial 0 with value: 0.5514170040485831.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:18:59,660]\u001b[0m Trial 12 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 353, 'max_depth': 16, 'max_features': 20, 'min_impurity_decrease': 0.702937495519433}. Best is trial 0 with value: 0.5514170040485831.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:19:06,145]\u001b[0m Trial 13 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 411, 'max_depth': 12, 'max_features': 12, 'min_impurity_decrease': 4.931175650908394}. Best is trial 0 with value: 0.5514170040485831.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:19:19,012]\u001b[0m Trial 14 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 743, 'max_depth': 7, 'max_features': 19, 'min_impurity_decrease': 1.5253121430442045}. Best is trial 0 with value: 0.5514170040485831.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:19:30,446]\u001b[0m Trial 15 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 757, 'max_depth': 13, 'max_features': 12, 'min_impurity_decrease': 2.668111844473928}. Best is trial 0 with value: 0.5514170040485831.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:19:36,204]\u001b[0m Trial 16 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 286, 'max_depth': 9, 'max_features': 24, 'min_impurity_decrease': 3.4814210246729473}. Best is trial 0 with value: 0.5514170040485831.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:19:50,174]\u001b[0m Trial 17 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 807, 'max_depth': 5, 'max_features': 19, 'min_impurity_decrease': 1.4307627258174405}. Best is trial 0 with value: 0.5514170040485831.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:20:00,731]\u001b[0m Trial 18 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 714, 'max_depth': 13, 'max_features': 12, 'min_impurity_decrease': 2.5740699841477164}. Best is trial 0 with value: 0.5514170040485831.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:20:06,769]\u001b[0m Trial 19 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 291, 'max_depth': 8, 'max_features': 24, 'min_impurity_decrease': 3.3144569973486893}. Best is trial 0 with value: 0.5514170040485831.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:20:24,565]\u001b[0m Trial 20 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 871, 'max_depth': 5, 'max_features': 29, 'min_impurity_decrease': 1.0528796896599455}. Best is trial 0 with value: 0.5514170040485831.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:20:34,934]\u001b[0m Trial 21 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 670, 'max_depth': 10, 'max_features': 14, 'min_impurity_decrease': 2.1272690676619233}. Best is trial 0 with value: 0.5514170040485831.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:20:44,535]\u001b[0m Trial 22 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 502, 'max_depth': 9, 'max_features': 23, 'min_impurity_decrease': 3.0212243946132658}. Best is trial 0 with value: 0.5514170040485831.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:21:02,767]\u001b[0m Trial 23 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 871, 'max_depth': 6, 'max_features': 30, 'min_impurity_decrease': 0.8924229302747376}. Best is trial 0 with value: 0.5514170040485831.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:21:16,253]\u001b[0m Trial 24 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 673, 'max_depth': 11, 'max_features': 28, 'min_impurity_decrease': 2.1839582613393773}. Best is trial 0 with value: 0.5514170040485831.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:21:26,603]\u001b[0m Trial 25 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 548, 'max_depth': 9, 'max_features': 22, 'min_impurity_decrease': 2.0762642939740097}. Best is trial 0 with value: 0.5514170040485831.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:21:52,001]\u001b[0m Trial 26 finished with value: 0.7368259109311741 and parameters: {'n_estimators': 859, 'max_depth': 7, 'max_features': 27, 'min_impurity_decrease': 0.05016303754931073}. Best is trial 26 with value: 0.7368259109311741.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:22:29,392]\u001b[0m Trial 27 finished with value: 0.7493927125506071 and parameters: {'n_estimators': 889, 'max_depth': 6, 'max_features': 27, 'min_impurity_decrease': 0.0085734336612443}. Best is trial 27 with value: 0.7493927125506071.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:22:57,588]\u001b[0m Trial 28 finished with value: 0.7377004048582996 and parameters: {'n_estimators': 901, 'max_depth': 7, 'max_features': 27, 'min_impurity_decrease': 0.0444905724402096}. Best is trial 27 with value: 0.7493927125506071.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:23:25,154]\u001b[0m Trial 29 finished with value: 0.7365506072874493 and parameters: {'n_estimators': 918, 'max_depth': 7, 'max_features': 27, 'min_impurity_decrease': 0.06568686852538644}. Best is trial 27 with value: 0.7493927125506071.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:23:42,075]\u001b[0m Trial 30 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 855, 'max_depth': 6, 'max_features': 26, 'min_impurity_decrease': 0.3179551086113592}. Best is trial 27 with value: 0.7493927125506071.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:24:00,746]\u001b[0m Trial 31 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 937, 'max_depth': 7, 'max_features': 27, 'min_impurity_decrease': 0.18490386200494155}. Best is trial 27 with value: 0.7493927125506071.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:24:43,629]\u001b[0m Trial 32 finished with value: 0.7726315789473683 and parameters: {'n_estimators': 913, 'max_depth': 8, 'max_features': 27, 'min_impurity_decrease': 0.004905793398416951}. Best is trial 32 with value: 0.7726315789473683.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:25:40,745]\u001b[0m Trial 33 finished with value: 0.7855222672064777 and parameters: {'n_estimators': 917, 'max_depth': 6, 'max_features': 30, 'min_impurity_decrease': 0.0020653183224312883}. Best is trial 33 with value: 0.7855222672064777.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:26:01,000]\u001b[0m Trial 34 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 989, 'max_depth': 5, 'max_features': 29, 'min_impurity_decrease': 0.6299600959488391}. Best is trial 33 with value: 0.7855222672064777.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:26:21,218]\u001b[0m Trial 35 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 921, 'max_depth': 8, 'max_features': 30, 'min_impurity_decrease': 0.46338942427376173}. Best is trial 33 with value: 0.7855222672064777.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:26:37,918]\u001b[0m Trial 36 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 808, 'max_depth': 6, 'max_features': 25, 'min_impurity_decrease': 1.0293121320437244}. Best is trial 33 with value: 0.7855222672064777.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:26:57,094]\u001b[0m Trial 37 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 936, 'max_depth': 8, 'max_features': 26, 'min_impurity_decrease': 0.4600400216967294}. Best is trial 33 with value: 0.7855222672064777.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:27:16,618]\u001b[0m Trial 38 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 998, 'max_depth': 10, 'max_features': 23, 'min_impurity_decrease': 0.6726799043596885}. Best is trial 33 with value: 0.7855222672064777.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:27:35,086]\u001b[0m Trial 39 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 890, 'max_depth': 6, 'max_features': 28, 'min_impurity_decrease': 1.2782349340929955}. Best is trial 33 with value: 0.7855222672064777.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:28:05,531]\u001b[0m Trial 40 finished with value: 0.740502024291498 and parameters: {'n_estimators': 831, 'max_depth': 5, 'max_features': 25, 'min_impurity_decrease': 0.024123964900358442}. Best is trial 33 with value: 0.7855222672064777.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:28:22,365]\u001b[0m Trial 41 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 830, 'max_depth': 5, 'max_features': 26, 'min_impurity_decrease': 0.31739342122319364}. Best is trial 33 with value: 0.7855222672064777.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:29:00,101]\u001b[0m Trial 42 finished with value: 0.7400971659919029 and parameters: {'n_estimators': 946, 'max_depth': 6, 'max_features': 30, 'min_impurity_decrease': 0.021986609957610734}. Best is trial 33 with value: 0.7855222672064777.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:29:20,049]\u001b[0m Trial 43 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 950, 'max_depth': 6, 'max_features': 30, 'min_impurity_decrease': 0.2656034449404653}. Best is trial 33 with value: 0.7855222672064777.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:29:35,465]\u001b[0m Trial 44 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 761, 'max_depth': 5, 'max_features': 28, 'min_impurity_decrease': 0.8244094328287186}. Best is trial 33 with value: 0.7855222672064777.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:29:55,664]\u001b[0m Trial 45 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 968, 'max_depth': 8, 'max_features': 29, 'min_impurity_decrease': 0.5478322323285494}. Best is trial 33 with value: 0.7855222672064777.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:30:24,401]\u001b[0m Trial 46 finished with value: 0.7398866396761135 and parameters: {'n_estimators': 839, 'max_depth': 6, 'max_features': 25, 'min_impurity_decrease': 0.029760360244799347}. Best is trial 33 with value: 0.7855222672064777.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:30:41,075]\u001b[0m Trial 47 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 786, 'max_depth': 5, 'max_features': 30, 'min_impurity_decrease': 0.29597043460840733}. Best is trial 33 with value: 0.7855222672064777.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:30:55,476]\u001b[0m Trial 48 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 710, 'max_depth': 8, 'max_features': 28, 'min_impurity_decrease': 1.7554372932180642}. Best is trial 33 with value: 0.7855222672064777.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:30:57,996]\u001b[0m Trial 49 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 107, 'max_depth': 10, 'max_features': 21, 'min_impurity_decrease': 0.8214989241724661}. Best is trial 33 with value: 0.7855222672064777.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:31:14,862]\u001b[0m Trial 50 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 904, 'max_depth': 6, 'max_features': 24, 'min_impurity_decrease': 1.1428130805476215}. Best is trial 33 with value: 0.7855222672064777.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:31:44,390]\u001b[0m Trial 51 finished with value: 0.7405182186234818 and parameters: {'n_estimators': 830, 'max_depth': 6, 'max_features': 25, 'min_impurity_decrease': 0.021600304156371496}. Best is trial 33 with value: 0.7855222672064777.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:32:04,560]\u001b[0m Trial 52 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 958, 'max_depth': 7, 'max_features': 26, 'min_impurity_decrease': 0.21718286729614222}. Best is trial 33 with value: 0.7855222672064777.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:32:16,262]\u001b[0m Trial 53 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 829, 'max_depth': 5, 'max_features': 7, 'min_impurity_decrease': 0.41543120512681814}. Best is trial 33 with value: 0.7855222672064777.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:33:03,931]\u001b[0m Trial 54 finished with value: 0.7918866396761133 and parameters: {'n_estimators': 885, 'max_depth': 6, 'max_features': 23, 'min_impurity_decrease': 0.00020454575578033804}. Best is trial 54 with value: 0.7918866396761133.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:33:15,589]\u001b[0m Trial 55 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 616, 'max_depth': 18, 'max_features': 23, 'min_impurity_decrease': 0.5829495913183987}. Best is trial 54 with value: 0.7918866396761133.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:33:29,630]\u001b[0m Trial 56 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 741, 'max_depth': 9, 'max_features': 22, 'min_impurity_decrease': 4.063447563238093}. Best is trial 54 with value: 0.7918866396761133.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:33:42,896]\u001b[0m Trial 57 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 787, 'max_depth': 5, 'max_features': 17, 'min_impurity_decrease': 0.2308250946269657}. Best is trial 54 with value: 0.7918866396761133.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:34:00,239]\u001b[0m Trial 58 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 878, 'max_depth': 7, 'max_features': 25, 'min_impurity_decrease': 0.7427996116386499}. Best is trial 54 with value: 0.7918866396761133.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:34:13,044]\u001b[0m Trial 59 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 711, 'max_depth': 6, 'max_features': 19, 'min_impurity_decrease': 0.41500420231874374}. Best is trial 54 with value: 0.7918866396761133.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:34:28,990]\u001b[0m Trial 60 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 819, 'max_depth': 8, 'max_features': 24, 'min_impurity_decrease': 2.3325041140325435}. Best is trial 54 with value: 0.7918866396761133.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:35:08,870]\u001b[0m Trial 61 finished with value: 0.7554331983805669 and parameters: {'n_estimators': 895, 'max_depth': 6, 'max_features': 29, 'min_impurity_decrease': 0.00804419284634018}. Best is trial 54 with value: 0.7918866396761133.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:35:27,314]\u001b[0m Trial 62 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 886, 'max_depth': 5, 'max_features': 28, 'min_impurity_decrease': 0.18047809821631536}. Best is trial 54 with value: 0.7918866396761133.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:35:45,247]\u001b[0m Trial 63 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 847, 'max_depth': 7, 'max_features': 29, 'min_impurity_decrease': 0.14326351838036389}. Best is trial 54 with value: 0.7918866396761133.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:36:03,634]\u001b[0m Trial 64 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 905, 'max_depth': 15, 'max_features': 27, 'min_impurity_decrease': 0.39297499271180003}. Best is trial 54 with value: 0.7918866396761133.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:36:22,851]\u001b[0m Trial 65 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 969, 'max_depth': 6, 'max_features': 26, 'min_impurity_decrease': 0.6030490310670441}. Best is trial 54 with value: 0.7918866396761133.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:36:34,942]\u001b[0m Trial 66 finished with value: 0.7361943319838058 and parameters: {'n_estimators': 437, 'max_depth': 7, 'max_features': 22, 'min_impurity_decrease': 0.06176369196060873}. Best is trial 54 with value: 0.7918866396761133.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:36:50,655]\u001b[0m Trial 67 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 777, 'max_depth': 5, 'max_features': 25, 'min_impurity_decrease': 2.8670835480053154}. Best is trial 54 with value: 0.7918866396761133.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:37:08,852]\u001b[0m Trial 68 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 857, 'max_depth': 6, 'max_features': 27, 'min_impurity_decrease': 1.0080199381210988}. Best is trial 54 with value: 0.7918866396761133.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:37:28,534]\u001b[0m Trial 69 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 930, 'max_depth': 8, 'max_features': 29, 'min_impurity_decrease': 0.18338592095984801}. Best is trial 54 with value: 0.7918866396761133.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:37:47,703]\u001b[0m Trial 70 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 998, 'max_depth': 14, 'max_features': 24, 'min_impurity_decrease': 0.5506133290277083}. Best is trial 54 with value: 0.7918866396761133.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:38:18,654]\u001b[0m Trial 71 finished with value: 0.7370202429149798 and parameters: {'n_estimators': 945, 'max_depth': 6, 'max_features': 30, 'min_impurity_decrease': 0.08590526653818167}. Best is trial 54 with value: 0.7918866396761133.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:38:53,646]\u001b[0m Trial 72 finished with value: 0.7398866396761133 and parameters: {'n_estimators': 913, 'max_depth': 7, 'max_features': 28, 'min_impurity_decrease': 0.030118637469662864}. Best is trial 54 with value: 0.7918866396761133.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:39:12,014]\u001b[0m Trial 73 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 875, 'max_depth': 6, 'max_features': 29, 'min_impurity_decrease': 0.29431296273911434}. Best is trial 54 with value: 0.7918866396761133.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:39:31,594]\u001b[0m Trial 74 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 975, 'max_depth': 5, 'max_features': 27, 'min_impurity_decrease': 0.44157824979709165}. Best is trial 54 with value: 0.7918866396761133.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:40:17,868]\u001b[0m Trial 75 finished with value: 0.7783805668016196 and parameters: {'n_estimators': 812, 'max_depth': 6, 'max_features': 30, 'min_impurity_decrease': 0.0034163962831970235}. Best is trial 54 with value: 0.7918866396761133.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:40:21,770]\u001b[0m Trial 76 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 159, 'max_depth': 12, 'max_features': 26, 'min_impurity_decrease': 0.18222096707696467}. Best is trial 54 with value: 0.7918866396761133.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:40:54,207]\u001b[0m Trial 77 finished with value: 0.7402429149797571 and parameters: {'n_estimators': 809, 'max_depth': 7, 'max_features': 28, 'min_impurity_decrease': 0.013876751975807303}. Best is trial 54 with value: 0.7918866396761133.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:41:06,011]\u001b[0m Trial 78 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 735, 'max_depth': 9, 'max_features': 14, 'min_impurity_decrease': 0.3442492928702137}. Best is trial 54 with value: 0.7918866396761133.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:41:23,783]\u001b[0m Trial 79 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 847, 'max_depth': 5, 'max_features': 29, 'min_impurity_decrease': 0.7068641105932276}. Best is trial 54 with value: 0.7918866396761133.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:41:41,630]\u001b[0m Trial 80 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 894, 'max_depth': 7, 'max_features': 25, 'min_impurity_decrease': 0.5416657190929857}. Best is trial 54 with value: 0.7918866396761133.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:42:18,759]\u001b[0m Trial 81 finished with value: 0.7662348178137652 and parameters: {'n_estimators': 810, 'max_depth': 7, 'max_features': 28, 'min_impurity_decrease': 0.006323452295319689}. Best is trial 54 with value: 0.7918866396761133.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:42:35,312]\u001b[0m Trial 82 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 767, 'max_depth': 6, 'max_features': 30, 'min_impurity_decrease': 0.16760144581395497}. Best is trial 54 with value: 0.7918866396761133.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:42:51,468]\u001b[0m Trial 83 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 798, 'max_depth': 6, 'max_features': 27, 'min_impurity_decrease': 0.3281966807364}. Best is trial 54 with value: 0.7918866396761133.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:43:34,704]\u001b[0m Trial 84 finished with value: 0.7772145748987853 and parameters: {'n_estimators': 826, 'max_depth': 8, 'max_features': 28, 'min_impurity_decrease': 0.0038615852061512136}. Best is trial 54 with value: 0.7918866396761133.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:44:37,279]\u001b[0m Trial 85 finished with value: 0.8012793522267206 and parameters: {'n_estimators': 861, 'max_depth': 8, 'max_features': 28, 'min_impurity_decrease': 0.00073920996096735}. Best is trial 85 with value: 0.8012793522267206.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:44:55,958]\u001b[0m Trial 86 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 869, 'max_depth': 8, 'max_features': 29, 'min_impurity_decrease': 0.1562087873442065}. Best is trial 85 with value: 0.8012793522267206.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:45:15,166]\u001b[0m Trial 87 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 926, 'max_depth': 9, 'max_features': 28, 'min_impurity_decrease': 0.27264714866771}. Best is trial 85 with value: 0.8012793522267206.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:45:34,096]\u001b[0m Trial 88 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 892, 'max_depth': 8, 'max_features': 30, 'min_impurity_decrease': 0.4949804661835662}. Best is trial 85 with value: 0.8012793522267206.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:45:41,404]\u001b[0m Trial 89 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 321, 'max_depth': 20, 'max_features': 29, 'min_impurity_decrease': 0.37222761372446705}. Best is trial 85 with value: 0.8012793522267206.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:45:55,649]\u001b[0m Trial 90 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 673, 'max_depth': 9, 'max_features': 28, 'min_impurity_decrease': 4.493469463689617}. Best is trial 85 with value: 0.8012793522267206.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:46:30,006]\u001b[0m Trial 91 finished with value: 0.7576356275303645 and parameters: {'n_estimators': 815, 'max_depth': 7, 'max_features': 27, 'min_impurity_decrease': 0.00772526404665119}. Best is trial 85 with value: 0.8012793522267206.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:46:55,045]\u001b[0m Trial 92 finished with value: 0.7365344129554657 and parameters: {'n_estimators': 859, 'max_depth': 7, 'max_features': 27, 'min_impurity_decrease': 0.11546910598324629}. Best is trial 85 with value: 0.8012793522267206.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:47:11,492]\u001b[0m Trial 93 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 815, 'max_depth': 8, 'max_features': 26, 'min_impurity_decrease': 0.25890283045236717}. Best is trial 85 with value: 0.8012793522267206.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:47:31,376]\u001b[0m Trial 94 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 878, 'max_depth': 10, 'max_features': 30, 'min_impurity_decrease': 0.14903885644786677}. Best is trial 85 with value: 0.8012793522267206.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:48:15,262]\u001b[0m Trial 95 finished with value: 0.7686801619433197 and parameters: {'n_estimators': 914, 'max_depth': 7, 'max_features': 28, 'min_impurity_decrease': 0.005748155804201368}. Best is trial 85 with value: 0.8012793522267206.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:48:34,774]\u001b[0m Trial 96 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 919, 'max_depth': 7, 'max_features': 29, 'min_impurity_decrease': 1.7433897961964038}. Best is trial 85 with value: 0.8012793522267206.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:48:51,166]\u001b[0m Trial 97 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 792, 'max_depth': 8, 'max_features': 28, 'min_impurity_decrease': 0.249808808497176}. Best is trial 85 with value: 0.8012793522267206.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:49:58,060]\u001b[0m Trial 98 finished with value: 0.797910931174089 and parameters: {'n_estimators': 954, 'max_depth': 7, 'max_features': 29, 'min_impurity_decrease': 0.0005874088492808707}. Best is trial 85 with value: 0.8012793522267206.\u001b[0m\n",
      "\u001b[32m[I 2024-11-16 02:50:11,938]\u001b[0m Trial 99 finished with value: 0.5514170040485831 and parameters: {'n_estimators': 959, 'max_depth': 9, 'max_features': 10, 'min_impurity_decrease': 0.4100709418577324}. Best is trial 85 with value: 0.8012793522267206.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Tuning hyperparameters\n",
    "#Step 1. Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\",100,1000,1) \n",
    "    max_depth = trial.suggest_int(\"max_depth\",5,20,1)\n",
    "    max_features = trial.suggest_int(\"max_features\",5,30,1)\n",
    "    min_impurity_decrease = trial.suggest_float(\"min_impurity_decrease\",0,5,log=False) \n",
    "    model = RandomForestClassifier(n_estimators = n_estimators\n",
    "              ,max_depth = max_depth\n",
    "              ,max_features = max_features\n",
    "              ,min_impurity_decrease = min_impurity_decrease\n",
    "              ,random_state=1\n",
    "              ,verbose=False\n",
    "              ,n_jobs=8)\n",
    "\n",
    "# Step 2: Scoring method:\n",
    "    score = model_selection.cross_val_score(model, X, y, n_jobs=8, scoring=\"accuracy\",cv=Cv_optuna)\n",
    "    accuracy = score.mean()\n",
    "    return accuracy\n",
    " \n",
    "# Step 3: Running it\n",
    "study = optuna.create_study(direction=\"maximize\",sampler=optuna.samplers.TPESampler(seed=0))\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ea7fc26-11ae-497b-ab4a-949cd12a43a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are : \n",
      "{'n_estimators': 861, 'max_depth': 8, 'max_features': 28, 'min_impurity_decrease': 0.00073920996096735}\n"
     ]
    }
   ],
   "source": [
    "# Getting the best parameters:\n",
    "print(f\"The best parameters are : \\n{study.best_params}\")\n",
    "# Setting the best model\n",
    "clf=RandomForestClassifier(n_estimators = study.best_params['n_estimators']\n",
    "              ,max_depth = study.best_params['max_depth']\n",
    "              ,max_features = study.best_params['max_features']\n",
    "              ,min_impurity_decrease = study.best_params['min_impurity_decrease']\n",
    "              ,random_state=0\n",
    "              ,verbose=False\n",
    "              ,n_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "90cca67e-3df8-43ff-8b96-2c9bec87ee43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.801571</td>\n",
       "      <td>0.001639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.874895</td>\n",
       "      <td>0.000510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.834178</td>\n",
       "      <td>0.001313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.773785</td>\n",
       "      <td>0.001773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.905051</td>\n",
       "      <td>0.002009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.901753</td>\n",
       "      <td>0.001233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean        Se\n",
       "Accuracy_test   0.801571  0.001639\n",
       "Accuracy_train  0.874895  0.000510\n",
       "F1 Score        0.834178  0.001313\n",
       "Precision       0.773785  0.001773\n",
       "Recall          0.905051  0.002009\n",
       "Roc_auc         0.901753  0.001233"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model2\n",
    "Model2=Model_results(clf,X,y,Cv_model)\n",
    "Model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "346eea08-6a51-48fc-bfcd-1a8d913649a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Method</th>\n",
       "      <th colspan=\"4\" halign=\"left\">RF</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Model 1</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Model 2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Values</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.821879</td>\n",
       "      <td>0.001555</td>\n",
       "      <td>0.801571</td>\n",
       "      <td>0.001639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.998433</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.874895</td>\n",
       "      <td>0.000510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.844143</td>\n",
       "      <td>0.001350</td>\n",
       "      <td>0.834178</td>\n",
       "      <td>0.001313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.815738</td>\n",
       "      <td>0.001724</td>\n",
       "      <td>0.773785</td>\n",
       "      <td>0.001773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.874802</td>\n",
       "      <td>0.001992</td>\n",
       "      <td>0.905051</td>\n",
       "      <td>0.002009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.915202</td>\n",
       "      <td>0.000983</td>\n",
       "      <td>0.901753</td>\n",
       "      <td>0.001233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Method                RF                              \n",
       "Model            Model 1             Model 2          \n",
       "Values              Mean        Se      Mean        Se\n",
       "Accuracy_test   0.821879  0.001555  0.801571  0.001639\n",
       "Accuracy_train  0.998433  0.000053  0.874895  0.000510\n",
       "F1 Score        0.844143  0.001350  0.834178  0.001313\n",
       "Precision       0.815738  0.001724  0.773785  0.001773\n",
       "Recall          0.874802  0.001992  0.905051  0.002009\n",
       "Roc_auc         0.915202  0.000983  0.901753  0.001233"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saving the data of model performance\n",
    "Model_data=pd.concat([Model1,Model2],axis=1)\n",
    "Model_data.columns = [['RF']*4,['Model 1','Model 1', 'Model 2','Model 2'], ['Mean', 'Se', 'Mean', 'Se']]  \n",
    "Model_data.columns.names=['Method','Model','Values']\n",
    "Model_data.to_csv(\"./RF_model_lasso_data.csv\",sep=',')\n",
    "Model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115be659-b066-4abf-85ac-59fccd101ad4",
   "metadata": {},
   "source": [
    "## 1.4 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ff2324c-d316-43ae-96dc-e58f404273a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=xgb.XGBClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "03979633-d8bb-44e7-a9d7-b7050c4c32c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.825263</td>\n",
       "      <td>0.001417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.998433</td>\n",
       "      <td>0.000053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.845006</td>\n",
       "      <td>0.001306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.827040</td>\n",
       "      <td>0.001578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.864023</td>\n",
       "      <td>0.002262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.918109</td>\n",
       "      <td>0.001021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean        Se\n",
       "Accuracy_test   0.825263  0.001417\n",
       "Accuracy_train  0.998433  0.000053\n",
       "F1 Score        0.845006  0.001306\n",
       "Precision       0.827040  0.001578\n",
       "Recall          0.864023  0.002262\n",
       "Roc_auc         0.918109  0.001021"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model1 \n",
    "Model1_clf=clf\n",
    "#Model1\n",
    "Model1=Model_results(Model1_clf,X,y,Cv_model)\n",
    "Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c6e5e42b-3688-4c6f-aa87-cf0fe5b1ff05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2024-11-16 02:56:32,799]\u001b[0m A new study created in memory with name: no-name-fb98c9cb-7e1f-4f1a-9a8f-3eeba6bc9870\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 02:59:28,517]\u001b[0m Trial 0 finished with value: 0.8330688259109311 and parameters: {'lambda': 0.15676677195506075, 'alpha': 0.7257005721594281, 'colsample_bytree': 0.7, 'subsample': 0.7000000000000001, 'learning_rate': 0.0801, 'n_estimators': 664}. Best is trial 0 with value: 0.8330688259109311.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 03:02:32,124]\u001b[0m Trial 1 finished with value: 0.8297165991902833 and parameters: {'lambda': 0.0562793204741517, 'alpha': 3.6905577292137624, 'colsample_bytree': 1.0, 'subsample': 0.6000000000000001, 'learning_rate': 0.1551, 'n_estimators': 552}. Best is trial 0 with value: 0.8330688259109311.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 03:04:14,980]\u001b[0m Trial 2 finished with value: 0.7884534412955466 and parameters: {'lambda': 0.18714500686240676, 'alpha': 5.039489598671215, 'colsample_bytree': 0.3, 'subsample': 0.4, 'learning_rate': 0.0001, 'n_estimators': 841}. Best is trial 0 with value: 0.8330688259109311.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 03:08:57,863]\u001b[0m Trial 3 finished with value: 0.8323886639676112 and parameters: {'lambda': 1.2960656597279736, 'alpha': 3.0202896401586674, 'colsample_bytree': 1.0, 'subsample': 0.9, 'learning_rate': 0.0901, 'n_estimators': 792}. Best is trial 0 with value: 0.8330688259109311.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 03:10:19,812]\u001b[0m Trial 4 finished with value: 0.83051012145749 and parameters: {'lambda': 0.0029723346443356552, 'alpha': 0.3628140404024381, 'colsample_bytree': 0.4, 'subsample': 1.0, 'learning_rate': 0.10010000000000001, 'n_estimators': 444}. Best is trial 0 with value: 0.8330688259109311.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 03:12:58,273]\u001b[0m Trial 5 finished with value: 0.7984129554655871 and parameters: {'lambda': 0.011434638743472197, 'alpha': 1.2500712230836255, 'colsample_bytree': 0.6000000000000001, 'subsample': 0.7000000000000001, 'learning_rate': 0.0001, 'n_estimators': 637}. Best is trial 0 with value: 0.8330688259109311.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 03:15:55,707]\u001b[0m Trial 6 finished with value: 0.8317408906882591 and parameters: {'lambda': 0.2807908107885728, 'alpha': 0.2935864364395359, 'colsample_bytree': 1.0, 'subsample': 0.8, 'learning_rate': 0.07010000000000001, 'n_estimators': 465}. Best is trial 0 with value: 0.8330688259109311.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 03:16:55,250]\u001b[0m Trial 7 finished with value: 0.8242914979757084 and parameters: {'lambda': 0.6173405204074314, 'alpha': 0.0017414134181586202, 'colsample_bytree': 0.8, 'subsample': 0.8, 'learning_rate': 0.040100000000000004, 'n_estimators': 172}. Best is trial 0 with value: 0.8330688259109311.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 03:17:37,187]\u001b[0m Trial 8 finished with value: 0.8241781376518219 and parameters: {'lambda': 0.01826894228153233, 'alpha': 0.028499883436971588, 'colsample_bytree': 0.7, 'subsample': 0.7000000000000001, 'learning_rate': 0.1951, 'n_estimators': 147}. Best is trial 0 with value: 0.8330688259109311.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 03:18:59,652]\u001b[0m Trial 9 finished with value: 0.8277085020242915 and parameters: {'lambda': 0.006847105576684045, 'alpha': 0.004418125737902547, 'colsample_bytree': 0.8, 'subsample': 0.5, 'learning_rate': 0.0901, 'n_estimators': 282}. Best is trial 0 with value: 0.8330688259109311.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 03:21:47,549]\u001b[0m Trial 10 finished with value: 0.8320485829959513 and parameters: {'lambda': 5.790132527437159, 'alpha': 0.034364938361176656, 'colsample_bytree': 0.5, 'subsample': 0.5, 'learning_rate': 0.1351, 'n_estimators': 980}. Best is trial 0 with value: 0.8330688259109311.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 03:26:17,972]\u001b[0m Trial 11 finished with value: 0.8323400809716598 and parameters: {'lambda': 1.6949515818605698, 'alpha': 1.029662836454581, 'colsample_bytree': 0.9000000000000001, 'subsample': 1.0, 'learning_rate': 0.050100000000000006, 'n_estimators': 745}. Best is trial 0 with value: 0.8330688259109311.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 03:28:41,626]\u001b[0m Trial 12 finished with value: 0.8292145748987854 and parameters: {'lambda': 9.927946948569323, 'alpha': 8.19008932305862, 'colsample_bytree': 0.6000000000000001, 'subsample': 0.9, 'learning_rate': 0.1201, 'n_estimators': 760}. Best is trial 0 with value: 0.8330688259109311.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 03:33:12,302]\u001b[0m Trial 13 finished with value: 0.8332307692307692 and parameters: {'lambda': 1.039327884351906, 'alpha': 0.15436829749486006, 'colsample_bytree': 0.8, 'subsample': 0.8, 'learning_rate': 0.07010000000000001, 'n_estimators': 971}. Best is trial 13 with value: 0.8332307692307692.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 03:37:25,914]\u001b[0m Trial 14 finished with value: 0.8333441295546561 and parameters: {'lambda': 0.07008570959501195, 'alpha': 0.06910620324453402, 'colsample_bytree': 0.7, 'subsample': 0.8, 'learning_rate': 0.040100000000000004, 'n_estimators': 1000}. Best is trial 14 with value: 0.8333441295546561.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 03:42:06,623]\u001b[0m Trial 15 finished with value: 0.832080971659919 and parameters: {'lambda': 0.05372822055706686, 'alpha': 0.05306392656241848, 'colsample_bytree': 0.8, 'subsample': 0.8, 'learning_rate': 0.0301, 'n_estimators': 984}. Best is trial 14 with value: 0.8333441295546561.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 03:45:07,147]\u001b[0m Trial 16 finished with value: 0.8320000000000001 and parameters: {'lambda': 2.30516650380047, 'alpha': 0.008636834106745522, 'colsample_bytree': 0.5, 'subsample': 0.9, 'learning_rate': 0.0601, 'n_estimators': 893}. Best is trial 14 with value: 0.8333441295546561.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 03:50:09,976]\u001b[0m Trial 17 finished with value: 0.8315789473684212 and parameters: {'lambda': 0.0010007385532741766, 'alpha': 0.1256168977967911, 'colsample_bytree': 0.9000000000000001, 'subsample': 0.6000000000000001, 'learning_rate': 0.0251, 'n_estimators': 910}. Best is trial 14 with value: 0.8333441295546561.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 03:54:02,028]\u001b[0m Trial 18 finished with value: 0.8314817813765183 and parameters: {'lambda': 0.04072367985115076, 'alpha': 0.11594964309788192, 'colsample_bytree': 0.7, 'subsample': 0.8, 'learning_rate': 0.11510000000000001, 'n_estimators': 993}. Best is trial 14 with value: 0.8333441295546561.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 03:55:50,148]\u001b[0m Trial 19 finished with value: 0.824663967611336 and parameters: {'lambda': 0.4167177992014545, 'alpha': 0.015195449047010904, 'colsample_bytree': 0.9000000000000001, 'subsample': 0.6000000000000001, 'learning_rate': 0.0201, 'n_estimators': 318}. Best is trial 14 with value: 0.8333441295546561.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 03:57:59,849]\u001b[0m Trial 20 finished with value: 0.832421052631579 and parameters: {'lambda': 0.8684081574771194, 'alpha': 0.1340248306937695, 'colsample_bytree': 0.5, 'subsample': 0.9, 'learning_rate': 0.07010000000000001, 'n_estimators': 659}. Best is trial 14 with value: 0.8333441295546561.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 04:01:01,585]\u001b[0m Trial 21 finished with value: 0.8326639676113359 and parameters: {'lambda': 0.15163216473656607, 'alpha': 1.66811474769697, 'colsample_bytree': 0.7, 'subsample': 0.7000000000000001, 'learning_rate': 0.0751, 'n_estimators': 679}. Best is trial 14 with value: 0.8333441295546561.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 04:05:28,086]\u001b[0m Trial 22 finished with value: 0.8321295546558705 and parameters: {'lambda': 0.10973517558130552, 'alpha': 0.3141837072624121, 'colsample_bytree': 0.8, 'subsample': 0.8, 'learning_rate': 0.0551, 'n_estimators': 889}. Best is trial 14 with value: 0.8333441295546561.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 04:08:39,947]\u001b[0m Trial 23 finished with value: 0.8323238866396763 and parameters: {'lambda': 0.03142875183640858, 'alpha': 0.6173868482331999, 'colsample_bytree': 0.6000000000000001, 'subsample': 0.7000000000000001, 'learning_rate': 0.0801, 'n_estimators': 838}. Best is trial 14 with value: 0.8333441295546561.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 04:11:15,399]\u001b[0m Trial 24 finished with value: 0.8322429149797571 and parameters: {'lambda': 3.5977307902558184, 'alpha': 0.07362524950411414, 'colsample_bytree': 0.7, 'subsample': 0.7000000000000001, 'learning_rate': 0.1051, 'n_estimators': 589}. Best is trial 14 with value: 0.8333441295546561.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 04:11:33,734]\u001b[0m Trial 25 finished with value: 0.8093603238866397 and parameters: {'lambda': 0.3780314876736387, 'alpha': 0.22614717964793002, 'colsample_bytree': 0.8, 'subsample': 0.8, 'learning_rate': 0.0451, 'n_estimators': 59}. Best is trial 14 with value: 0.8333441295546561.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 04:14:00,122]\u001b[0m Trial 26 finished with value: 0.8304453441295547 and parameters: {'lambda': 0.0935243739576242, 'alpha': 0.6753594775294495, 'colsample_bytree': 0.6000000000000001, 'subsample': 0.6000000000000001, 'learning_rate': 0.1401, 'n_estimators': 722}. Best is trial 14 with value: 0.8333441295546561.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 04:18:12,339]\u001b[0m Trial 27 finished with value: 0.8318056680161944 and parameters: {'lambda': 0.8125148602792741, 'alpha': 0.017569239362001957, 'colsample_bytree': 0.7, 'subsample': 0.9, 'learning_rate': 0.0201, 'n_estimators': 936}. Best is trial 14 with value: 0.8333441295546561.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 04:20:46,249]\u001b[0m Trial 28 finished with value: 0.8303643724696356 and parameters: {'lambda': 0.2568011348920487, 'alpha': 0.048983086952536156, 'colsample_bytree': 0.9000000000000001, 'subsample': 0.7000000000000001, 'learning_rate': 0.0601, 'n_estimators': 470}. Best is trial 14 with value: 0.8333441295546561.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 04:24:49,200]\u001b[0m Trial 29 finished with value: 0.8313360323886642 and parameters: {'lambda': 0.08371139813307134, 'alpha': 0.5277127595207798, 'colsample_bytree': 0.8, 'subsample': 0.5, 'learning_rate': 0.035100000000000006, 'n_estimators': 825}. Best is trial 14 with value: 0.8333441295546561.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 04:26:59,493]\u001b[0m Trial 30 finished with value: 0.8300404858299596 and parameters: {'lambda': 0.024022235422772265, 'alpha': 2.308339959533686, 'colsample_bytree': 0.7, 'subsample': 1.0, 'learning_rate': 0.0801, 'n_estimators': 548}. Best is trial 14 with value: 0.8333441295546561.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 04:29:56,000]\u001b[0m Trial 31 finished with value: 0.833344129554656 and parameters: {'lambda': 0.15145067504759233, 'alpha': 1.9399252353199008, 'colsample_bytree': 0.7, 'subsample': 0.7000000000000001, 'learning_rate': 0.0751, 'n_estimators': 663}. Best is trial 14 with value: 0.8333441295546561.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 04:32:13,174]\u001b[0m Trial 32 finished with value: 0.8297975708502026 and parameters: {'lambda': 0.17139492970828904, 'alpha': 4.372573606280472, 'colsample_bytree': 0.6000000000000001, 'subsample': 0.6000000000000001, 'learning_rate': 0.0901, 'n_estimators': 604}. Best is trial 14 with value: 0.8333441295546561.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 04:35:12,480]\u001b[0m Trial 33 finished with value: 0.8319352226720648 and parameters: {'lambda': 0.05687220362586181, 'alpha': 0.21657388046061396, 'colsample_bytree': 0.7, 'subsample': 0.8, 'learning_rate': 0.0651, 'n_estimators': 692}. Best is trial 14 with value: 0.8333441295546561.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 04:39:39,659]\u001b[0m Trial 34 finished with value: 0.8211012145748988 and parameters: {'lambda': 0.5157975823937437, 'alpha': 9.301858944783424, 'colsample_bytree': 0.8, 'subsample': 0.7000000000000001, 'learning_rate': 0.0101, 'n_estimators': 808}. Best is trial 14 with value: 0.8333441295546561.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 04:41:07,895]\u001b[0m Trial 35 finished with value: 0.8273198380566802 and parameters: {'lambda': 0.012117820451003777, 'alpha': 2.4966840468126694, 'colsample_bytree': 0.5, 'subsample': 0.4, 'learning_rate': 0.10010000000000001, 'n_estimators': 507}. Best is trial 14 with value: 0.8333441295546561.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 04:42:00,735]\u001b[0m Trial 36 finished with value: 0.831417004048583 and parameters: {'lambda': 0.197613236968479, 'alpha': 1.1761930596679906, 'colsample_bytree': 0.3, 'subsample': 0.9, 'learning_rate': 0.08510000000000001, 'n_estimators': 371}. Best is trial 14 with value: 0.8333441295546561.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 04:47:12,973]\u001b[0m Trial 37 finished with value: 0.8330688259109312 and parameters: {'lambda': 1.122476184082414, 'alpha': 0.801235864283031, 'colsample_bytree': 1.0, 'subsample': 0.8, 'learning_rate': 0.0451, 'n_estimators': 774}. Best is trial 14 with value: 0.8333441295546561.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 04:53:55,788]\u001b[0m Trial 38 finished with value: 0.8334736842105263 and parameters: {'lambda': 2.761675502516357, 'alpha': 0.4893734156096741, 'colsample_bytree': 1.0, 'subsample': 0.8, 'learning_rate': 0.040100000000000004, 'n_estimators': 942}. Best is trial 38 with value: 0.8334736842105263.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 04:59:30,724]\u001b[0m Trial 39 finished with value: 0.790072874493927 and parameters: {'lambda': 2.9571917132791676, 'alpha': 0.441834766022255, 'colsample_bytree': 1.0, 'subsample': 0.8, 'learning_rate': 0.0001, 'n_estimators': 874}. Best is trial 38 with value: 0.8334736842105263.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 05:02:05,259]\u001b[0m Trial 40 finished with value: 0.831417004048583 and parameters: {'lambda': 4.777974070386933, 'alpha': 0.14998641332483648, 'colsample_bytree': 0.4, 'subsample': 0.9, 'learning_rate': 0.1851, 'n_estimators': 948}. Best is trial 38 with value: 0.8334736842105263.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 05:08:25,305]\u001b[0m Trial 41 finished with value: 0.8346720647773279 and parameters: {'lambda': 1.5639845554549834, 'alpha': 0.8887736907873799, 'colsample_bytree': 1.0, 'subsample': 0.8, 'learning_rate': 0.0451, 'n_estimators': 939}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 05:14:53,123]\u001b[0m Trial 42 finished with value: 0.8337975708502025 and parameters: {'lambda': 1.4098313926108954, 'alpha': 1.5316660244866682, 'colsample_bytree': 1.0, 'subsample': 0.8, 'learning_rate': 0.035100000000000006, 'n_estimators': 932}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 05:21:02,868]\u001b[0m Trial 43 finished with value: 0.829165991902834 and parameters: {'lambda': 1.7880112343338415, 'alpha': 1.5862686153183414, 'colsample_bytree': 1.0, 'subsample': 0.7000000000000001, 'learning_rate': 0.0101, 'n_estimators': 929}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 05:26:43,931]\u001b[0m Trial 44 finished with value: 0.8320971659919029 and parameters: {'lambda': 8.028532557019458, 'alpha': 3.488815547669012, 'colsample_bytree': 1.0, 'subsample': 0.7000000000000001, 'learning_rate': 0.035100000000000006, 'n_estimators': 850}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 05:32:19,228]\u001b[0m Trial 45 finished with value: 0.8308987854251012 and parameters: {'lambda': 2.0741136961852518, 'alpha': 5.815001866365921, 'colsample_bytree': 0.9000000000000001, 'subsample': 0.8, 'learning_rate': 0.0451, 'n_estimators': 936}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 05:37:18,938]\u001b[0m Trial 46 finished with value: 0.833910931174089 and parameters: {'lambda': 0.6392292247900676, 'alpha': 1.7972681523907217, 'colsample_bytree': 0.9000000000000001, 'subsample': 0.8, 'learning_rate': 0.0551, 'n_estimators': 872}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 05:44:03,772]\u001b[0m Trial 47 finished with value: 0.8323076923076923 and parameters: {'lambda': 1.3334680978029436, 'alpha': 1.129317537609012, 'colsample_bytree': 1.0, 'subsample': 0.9, 'learning_rate': 0.015099999999999999, 'n_estimators': 997}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 05:49:39,217]\u001b[0m Trial 48 finished with value: 0.8343967611336031 and parameters: {'lambda': 4.425705089331524, 'alpha': 0.8101865914933563, 'colsample_bytree': 0.9000000000000001, 'subsample': 0.8, 'learning_rate': 0.0301, 'n_estimators': 866}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 05:55:17,736]\u001b[0m Trial 49 finished with value: 0.8304291497975709 and parameters: {'lambda': 5.154170437055029, 'alpha': 0.8948355223620812, 'colsample_bytree': 0.9000000000000001, 'subsample': 1.0, 'learning_rate': 0.0301, 'n_estimators': 869}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 06:00:00,073]\u001b[0m Trial 50 finished with value: 0.8329554655870445 and parameters: {'lambda': 3.0024725627771573, 'alpha': 0.40159609121398104, 'colsample_bytree': 0.9000000000000001, 'subsample': 0.9, 'learning_rate': 0.050100000000000006, 'n_estimators': 792}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 06:06:36,274]\u001b[0m Trial 51 finished with value: 0.8330688259109311 and parameters: {'lambda': 0.7277877860676264, 'alpha': 2.9925443191965253, 'colsample_bytree': 1.0, 'subsample': 0.8, 'learning_rate': 0.0301, 'n_estimators': 911}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 06:12:39,448]\u001b[0m Trial 52 finished with value: 0.8317732793522268 and parameters: {'lambda': 1.579844203850132, 'alpha': 6.047475744863491, 'colsample_bytree': 1.0, 'subsample': 0.8, 'learning_rate': 0.040100000000000004, 'n_estimators': 964}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 06:17:57,091]\u001b[0m Trial 53 finished with value: 0.8341538461538459 and parameters: {'lambda': 3.79534456205335, 'alpha': 1.2892718455212078, 'colsample_bytree': 0.9000000000000001, 'subsample': 0.8, 'learning_rate': 0.0551, 'n_estimators': 894}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 06:22:30,523]\u001b[0m Trial 54 finished with value: 0.8328906882591095 and parameters: {'lambda': 6.9224346407074115, 'alpha': 1.4479466384181627, 'colsample_bytree': 0.9000000000000001, 'subsample': 0.8, 'learning_rate': 0.0551, 'n_estimators': 735}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 06:31:19,817]\u001b[0m Trial 55 finished with value: 0.8324858299595143 and parameters: {'lambda': 3.758812442464991, 'alpha': 0.0010430520155247781, 'colsample_bytree': 0.9000000000000001, 'subsample': 0.9, 'learning_rate': 0.0251, 'n_estimators': 857}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 06:47:26,061]\u001b[0m Trial 56 finished with value: 0.833165991902834 and parameters: {'lambda': 2.2852149871752254, 'alpha': 0.9049791005844122, 'colsample_bytree': 1.0, 'subsample': 0.8, 'learning_rate': 0.0551, 'n_estimators': 905}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 07:02:54,090]\u001b[0m Trial 57 finished with value: 0.8257489878542511 and parameters: {'lambda': 8.755977433376069, 'alpha': 0.2523959182419449, 'colsample_bytree': 1.0, 'subsample': 0.8, 'learning_rate': 0.0101, 'n_estimators': 815}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 07:13:15,327]\u001b[0m Trial 58 finished with value: 0.833732793522267 and parameters: {'lambda': 4.403281340029628, 'alpha': 0.5594088284920058, 'colsample_bytree': 0.9000000000000001, 'subsample': 0.9, 'learning_rate': 0.0651, 'n_estimators': 882}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 07:17:57,486]\u001b[0m Trial 59 finished with value: 0.8326477732793524 and parameters: {'lambda': 4.382021328011123, 'alpha': 2.3228598244465104, 'colsample_bytree': 0.9000000000000001, 'subsample': 0.9, 'learning_rate': 0.0651, 'n_estimators': 782}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 07:23:26,382]\u001b[0m Trial 60 finished with value: 0.8330688259109311 and parameters: {'lambda': 6.157321565320631, 'alpha': 1.3991802671679698, 'colsample_bytree': 0.9000000000000001, 'subsample': 0.9, 'learning_rate': 0.0651, 'n_estimators': 886}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 07:29:59,431]\u001b[0m Trial 61 finished with value: 0.8336842105263158 and parameters: {'lambda': 2.7909034634467775, 'alpha': 0.6621004711450614, 'colsample_bytree': 1.0, 'subsample': 0.8, 'learning_rate': 0.040100000000000004, 'n_estimators': 958}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 07:36:06,175]\u001b[0m Trial 62 finished with value: 0.8331497975708502 and parameters: {'lambda': 1.2405652787857548, 'alpha': 0.6679176418097839, 'colsample_bytree': 0.9000000000000001, 'subsample': 0.8, 'learning_rate': 0.050100000000000006, 'n_estimators': 964}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 07:41:04,173]\u001b[0m Trial 63 finished with value: 0.8308987854251013 and parameters: {'lambda': 1.6562920030670332, 'alpha': 1.029129409047447, 'colsample_bytree': 0.8, 'subsample': 1.0, 'learning_rate': 0.035100000000000006, 'n_estimators': 842}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 07:47:22,082]\u001b[0m Trial 64 finished with value: 0.8324696356275304 and parameters: {'lambda': 0.5786482682342696, 'alpha': 0.32830549380568436, 'colsample_bytree': 1.0, 'subsample': 0.8, 'learning_rate': 0.0201, 'n_estimators': 907}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 07:53:23,843]\u001b[0m Trial 65 finished with value: 0.8334574898785426 and parameters: {'lambda': 3.403059182040785, 'alpha': 1.8718331214428139, 'colsample_bytree': 0.9000000000000001, 'subsample': 0.7000000000000001, 'learning_rate': 0.0601, 'n_estimators': 966}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 07:59:43,554]\u001b[0m Trial 66 finished with value: 0.834008097165992 and parameters: {'lambda': 2.3571161325292427, 'alpha': 0.654282152209134, 'colsample_bytree': 1.0, 'subsample': 0.9, 'learning_rate': 0.050100000000000006, 'n_estimators': 868}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 08:04:49,604]\u001b[0m Trial 67 finished with value: 0.8327611336032388 and parameters: {'lambda': 5.939945791324669, 'alpha': 3.4679607994273303, 'colsample_bytree': 0.9000000000000001, 'subsample': 0.9, 'learning_rate': 0.0751, 'n_estimators': 818}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 08:07:56,209]\u001b[0m Trial 68 finished with value: 0.8304615384615387 and parameters: {'lambda': 0.8308244935305021, 'alpha': 4.741560265353102, 'colsample_bytree': 0.8, 'subsample': 1.0, 'learning_rate': 0.050100000000000006, 'n_estimators': 759}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 08:13:49,628]\u001b[0m Trial 69 finished with value: 0.8332469635627531 and parameters: {'lambda': 9.876212275446877, 'alpha': 0.5652406603780161, 'colsample_bytree': 1.0, 'subsample': 0.9, 'learning_rate': 0.0251, 'n_estimators': 867}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 08:18:13,640]\u001b[0m Trial 70 finished with value: 0.8305587044534412 and parameters: {'lambda': 0.003804979794126558, 'alpha': 0.18962311962293865, 'colsample_bytree': 0.9000000000000001, 'subsample': 0.9, 'learning_rate': 0.0551, 'n_estimators': 698}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 08:24:53,641]\u001b[0m Trial 71 finished with value: 0.833570850202429 and parameters: {'lambda': 2.5625074918454294, 'alpha': 0.7560523983554717, 'colsample_bytree': 1.0, 'subsample': 0.8, 'learning_rate': 0.040100000000000004, 'n_estimators': 924}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 08:31:15,839]\u001b[0m Trial 72 finished with value: 0.834008097165992 and parameters: {'lambda': 4.206896538020257, 'alpha': 1.239246787403647, 'colsample_bytree': 1.0, 'subsample': 0.8, 'learning_rate': 0.0451, 'n_estimators': 887}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 08:37:16,200]\u001b[0m Trial 73 finished with value: 0.8327287449392712 and parameters: {'lambda': 4.151440245346929, 'alpha': 2.6589318735277137, 'colsample_bytree': 1.0, 'subsample': 0.8, 'learning_rate': 0.0601, 'n_estimators': 883}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 08:42:46,164]\u001b[0m Trial 74 finished with value: 0.8330688259109312 and parameters: {'lambda': 1.9827773476800068, 'alpha': 2.0189957072286995, 'colsample_bytree': 0.9000000000000001, 'subsample': 0.9, 'learning_rate': 0.050100000000000006, 'n_estimators': 841}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 08:48:34,059]\u001b[0m Trial 75 finished with value: 0.8343157894736842 and parameters: {'lambda': 1.0288058257605446, 'alpha': 1.365456899231398, 'colsample_bytree': 1.0, 'subsample': 0.7000000000000001, 'learning_rate': 0.0301, 'n_estimators': 802}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 08:54:15,436]\u001b[0m Trial 76 finished with value: 0.8338461538461539 and parameters: {'lambda': 0.973358980399394, 'alpha': 1.2590006904668494, 'colsample_bytree': 1.0, 'subsample': 0.7000000000000001, 'learning_rate': 0.0301, 'n_estimators': 808}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 08:59:38,027]\u001b[0m Trial 77 finished with value: 0.8181214574898786 and parameters: {'lambda': 1.0443293124068658, 'alpha': 1.1330426023446176, 'colsample_bytree': 1.0, 'subsample': 0.6000000000000001, 'learning_rate': 0.0051, 'n_estimators': 798}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 09:04:26,735]\u001b[0m Trial 78 finished with value: 0.8311093117408908 and parameters: {'lambda': 0.3483625666646466, 'alpha': 0.3803785240668916, 'colsample_bytree': 1.0, 'subsample': 0.7000000000000001, 'learning_rate': 0.0201, 'n_estimators': 724}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 09:09:23,785]\u001b[0m Trial 79 finished with value: 0.8339271255060728 and parameters: {'lambda': 0.43171817161301207, 'alpha': 0.8257637703108948, 'colsample_bytree': 1.0, 'subsample': 0.7000000000000001, 'learning_rate': 0.0301, 'n_estimators': 773}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 09:15:14,161]\u001b[0m Trial 80 finished with value: 0.8334574898785426 and parameters: {'lambda': 0.46817786454330695, 'alpha': 0.9313186397818353, 'colsample_bytree': 1.0, 'subsample': 0.7000000000000001, 'learning_rate': 0.0451, 'n_estimators': 838}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 09:21:18,715]\u001b[0m Trial 81 finished with value: 0.8339919028340081 and parameters: {'lambda': 1.0142192839311641, 'alpha': 1.2363934920138147, 'colsample_bytree': 1.0, 'subsample': 0.7000000000000001, 'learning_rate': 0.0301, 'n_estimators': 762}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 09:27:53,890]\u001b[0m Trial 82 finished with value: 0.8339919028340081 and parameters: {'lambda': 0.671472993932188, 'alpha': 1.7975976597474403, 'colsample_bytree': 1.0, 'subsample': 0.7000000000000001, 'learning_rate': 0.0251, 'n_estimators': 856}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 09:33:26,639]\u001b[0m Trial 83 finished with value: 0.8333927125506073 and parameters: {'lambda': 0.6357972155956496, 'alpha': 0.7988106774247107, 'colsample_bytree': 1.0, 'subsample': 0.6000000000000001, 'learning_rate': 0.0251, 'n_estimators': 748}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 09:38:21,417]\u001b[0m Trial 84 finished with value: 0.8288906882591093 and parameters: {'lambda': 0.2847223619929445, 'alpha': 1.3317395256512659, 'colsample_bytree': 1.0, 'subsample': 0.7000000000000001, 'learning_rate': 0.015099999999999999, 'n_estimators': 614}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 09:44:18,457]\u001b[0m Trial 85 finished with value: 0.8333603238866396 and parameters: {'lambda': 2.1292401665821816, 'alpha': 0.4909976330137207, 'colsample_bytree': 1.0, 'subsample': 0.7000000000000001, 'learning_rate': 0.0301, 'n_estimators': 769}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 09:50:33,414]\u001b[0m Trial 86 finished with value: 0.8305425101214575 and parameters: {'lambda': 1.2570117498810336, 'alpha': 2.8962322894916843, 'colsample_bytree': 1.0, 'subsample': 0.7000000000000001, 'learning_rate': 0.015099999999999999, 'n_estimators': 825}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 09:55:59,337]\u001b[0m Trial 87 finished with value: 0.8337004048582997 and parameters: {'lambda': 0.7810525136081207, 'alpha': 2.240438633748921, 'colsample_bytree': 1.0, 'subsample': 0.6000000000000001, 'learning_rate': 0.035100000000000006, 'n_estimators': 715}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 10:01:06,615]\u001b[0m Trial 88 finished with value: 0.8103481781376519 and parameters: {'lambda': 3.3372660257388485, 'alpha': 3.9814338192575436, 'colsample_bytree': 1.0, 'subsample': 0.7000000000000001, 'learning_rate': 0.0051, 'n_estimators': 649}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 10:02:32,325]\u001b[0m Trial 89 finished with value: 0.8280323886639677 and parameters: {'lambda': 1.650659283357175, 'alpha': 0.283192871387346, 'colsample_bytree': 1.0, 'subsample': 0.7000000000000001, 'learning_rate': 0.0451, 'n_estimators': 202}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 10:08:54,799]\u001b[0m Trial 90 finished with value: 0.8332469635627531 and parameters: {'lambda': 0.0011367667949660942, 'alpha': 1.6833709489430964, 'colsample_bytree': 1.0, 'subsample': 0.6000000000000001, 'learning_rate': 0.035100000000000006, 'n_estimators': 904}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 10:13:43,695]\u001b[0m Trial 91 finished with value: 0.83234008097166 and parameters: {'lambda': 0.6487461901980048, 'alpha': 0.003652175373940075, 'colsample_bytree': 0.9000000000000001, 'subsample': 0.7000000000000001, 'learning_rate': 0.0201, 'n_estimators': 859}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 10:17:37,365]\u001b[0m Trial 92 finished with value: 0.8306396761133604 and parameters: {'lambda': 0.39947422806788907, 'alpha': 1.0306142222840868, 'colsample_bytree': 0.9000000000000001, 'subsample': 0.7000000000000001, 'learning_rate': 0.1601, 'n_estimators': 780}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 10:23:08,037]\u001b[0m Trial 93 finished with value: 0.83382995951417 and parameters: {'lambda': 0.2224584902330959, 'alpha': 1.9114329020830663, 'colsample_bytree': 0.9000000000000001, 'subsample': 0.8, 'learning_rate': 0.040100000000000004, 'n_estimators': 924}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 10:28:25,772]\u001b[0m Trial 94 finished with value: 0.8336194331983806 and parameters: {'lambda': 0.9377289954739503, 'alpha': 1.4648063055011635, 'colsample_bytree': 0.8, 'subsample': 0.7000000000000001, 'learning_rate': 0.0301, 'n_estimators': 983}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 10:31:26,336]\u001b[0m Trial 95 finished with value: 0.829748987854251 and parameters: {'lambda': 1.4050891625954032, 'alpha': 0.8563188483281585, 'colsample_bytree': 1.0, 'subsample': 0.8, 'learning_rate': 0.0251, 'n_estimators': 416}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 10:37:24,447]\u001b[0m Trial 96 finished with value: 0.8319028340080973 and parameters: {'lambda': 7.35706670334729, 'alpha': 7.195265602010595, 'colsample_bytree': 0.9000000000000001, 'subsample': 0.8, 'learning_rate': 0.0451, 'n_estimators': 890}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 10:44:16,719]\u001b[0m Trial 97 finished with value: 0.8343157894736841 and parameters: {'lambda': 0.12836205390518968, 'alpha': 0.6793087111947833, 'colsample_bytree': 1.0, 'subsample': 0.7000000000000001, 'learning_rate': 0.0551, 'n_estimators': 867}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 10:49:48,639]\u001b[0m Trial 98 finished with value: 0.833748987854251 and parameters: {'lambda': 2.6399557977433, 'alpha': 0.6452258564835909, 'colsample_bytree': 1.0, 'subsample': 0.7000000000000001, 'learning_rate': 0.050100000000000006, 'n_estimators': 850}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_78252\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-11-16 10:54:49,417]\u001b[0m Trial 99 finished with value: 0.8327125506072874 and parameters: {'lambda': 0.31414885745727733, 'alpha': 1.2352258801724152, 'colsample_bytree': 1.0, 'subsample': 0.6000000000000001, 'learning_rate': 0.035100000000000006, 'n_estimators': 804}. Best is trial 41 with value: 0.8346720647773279.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Tuning hyperparameters\n",
    "#Step 1. Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
    "        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3,1.0,step=0.1),\n",
    "        'subsample': trial.suggest_float('subsample', 0.4, 1.0, step=0.1),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.2, step=0.005),\n",
    "        'n_estimators': trial.suggest_int(\"n_estimators\",50,1000,1)\n",
    "        #'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n",
    "    }\n",
    "    model = xgb.XGBClassifier(**param,random_state=1,n_jobs=8)\n",
    "\n",
    " \n",
    "# Step 2: Scoring method:\n",
    "    score = model_selection.cross_val_score(model, X, y, n_jobs=8, scoring=\"accuracy\",cv=Cv_optuna)\n",
    "    accuracy = score.mean()\n",
    "    return accuracy\n",
    "\n",
    "# Step 3: Running it\n",
    "study = optuna.create_study(direction=\"maximize\",sampler=optuna.samplers.TPESampler(seed=0))\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b2be4542-3f0e-438f-8e41-c67d96410fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are : \n",
      "{'lambda': 1.5639845554549834, 'alpha': 0.8887736907873799, 'colsample_bytree': 1.0, 'subsample': 0.8, 'learning_rate': 0.0451, 'n_estimators': 939}\n"
     ]
    }
   ],
   "source": [
    "# Getting the best parameters:\n",
    "print(f\"The best parameters are : \\n{study.best_params}\")\n",
    "# Setting the best model\n",
    "clf=xgb.XGBClassifier(alpha = study.best_params['alpha']\n",
    "              ,colsample_bytree = study.best_params['colsample_bytree']\n",
    "              ,subsample = study.best_params['subsample']\n",
    "              ,n_estimators = study.best_params['n_estimators']\n",
    "              ,learning_rate= study.best_params['learning_rate'], n_jobs=8\n",
    "              ,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d0687ace-1f1a-44a0-9329-fc50b681ff53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.833360</td>\n",
       "      <td>0.001281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.998433</td>\n",
       "      <td>0.000053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.852611</td>\n",
       "      <td>0.001139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.832289</td>\n",
       "      <td>0.001556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.874156</td>\n",
       "      <td>0.001885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.924063</td>\n",
       "      <td>0.000899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean        Se\n",
       "Accuracy_test   0.833360  0.001281\n",
       "Accuracy_train  0.998433  0.000053\n",
       "F1 Score        0.852611  0.001139\n",
       "Precision       0.832289  0.001556\n",
       "Recall          0.874156  0.001885\n",
       "Roc_auc         0.924063  0.000899"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model2\n",
    "Model2=Model_results(clf,X,y,Cv_model)\n",
    "Model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0bb46026-eb5e-4693-8e81-0a0a2ed16564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Method</th>\n",
       "      <th colspan=\"4\" halign=\"left\">XGB</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Model 1</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Model 2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Values</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.825263</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>0.833360</td>\n",
       "      <td>0.001281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.998433</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.998433</td>\n",
       "      <td>0.000053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.845006</td>\n",
       "      <td>0.001306</td>\n",
       "      <td>0.852611</td>\n",
       "      <td>0.001139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.827040</td>\n",
       "      <td>0.001578</td>\n",
       "      <td>0.832289</td>\n",
       "      <td>0.001556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.864023</td>\n",
       "      <td>0.002262</td>\n",
       "      <td>0.874156</td>\n",
       "      <td>0.001885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.918109</td>\n",
       "      <td>0.001021</td>\n",
       "      <td>0.924063</td>\n",
       "      <td>0.000899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Method               XGB                              \n",
       "Model            Model 1             Model 2          \n",
       "Values              Mean        Se      Mean        Se\n",
       "Accuracy_test   0.825263  0.001417  0.833360  0.001281\n",
       "Accuracy_train  0.998433  0.000053  0.998433  0.000053\n",
       "F1 Score        0.845006  0.001306  0.852611  0.001139\n",
       "Precision       0.827040  0.001578  0.832289  0.001556\n",
       "Recall          0.864023  0.002262  0.874156  0.001885\n",
       "Roc_auc         0.918109  0.001021  0.924063  0.000899"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saving the data of model performance\n",
    "Model_data=pd.concat([Model1,Model2],axis=1)\n",
    "Model_data.columns = [['XGB']*4,['Model 1','Model 1', 'Model 2','Model 2'], ['Mean', 'Se', 'Mean', 'Se']]  \n",
    "Model_data.columns.names=['Method','Model','Values']\n",
    "Model_data.to_csv(\"./XGB_model_lasso_data.csv\",sep=',')\n",
    "Model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208fbfd4-b156-43fb-b7b8-5f38ee0e24ef",
   "metadata": {},
   "source": [
    "# 2. MLREM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "45019f5e-5468-4d1b-a4de-8b8c86e33654",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Results/MLREM_col.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m col_data\u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./Results/MLREM_col.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mE:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mE:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Results/MLREM_col.csv'"
     ]
    }
   ],
   "source": [
    "col_data= pd.read_csv(\"./Results/MLREM_col.csv\",header=0,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f6d91ba6-9049-4060-818a-8cf59d7a47b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MW</th>\n",
       "      <th>D/Dtr09</th>\n",
       "      <th>ZM1MulPer</th>\n",
       "      <th>ECC</th>\n",
       "      <th>CENT</th>\n",
       "      <th>SMTI</th>\n",
       "      <th>SMTIV</th>\n",
       "      <th>GMTIV</th>\n",
       "      <th>Wap</th>\n",
       "      <th>IDMT</th>\n",
       "      <th>...</th>\n",
       "      <th>ATSC5s</th>\n",
       "      <th>P_VSA_MR_3</th>\n",
       "      <th>P_VSA_ppp_ar</th>\n",
       "      <th>P_VSA_ppp_con</th>\n",
       "      <th>P_VSA_charge_2</th>\n",
       "      <th>SM15_EA(ed)</th>\n",
       "      <th>T(O..Br)</th>\n",
       "      <th>TPSA(Tot)</th>\n",
       "      <th>SAdon</th>\n",
       "      <th>Vx</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ma_2019_A</th>\n",
       "      <td>267.28</td>\n",
       "      <td>66.871159</td>\n",
       "      <td>348.869542</td>\n",
       "      <td>135.0</td>\n",
       "      <td>383.0</td>\n",
       "      <td>2886.0</td>\n",
       "      <td>5576.0</td>\n",
       "      <td>10547.0</td>\n",
       "      <td>5729.0</td>\n",
       "      <td>4739.692713</td>\n",
       "      <td>...</td>\n",
       "      <td>90.763661</td>\n",
       "      <td>139.049917</td>\n",
       "      <td>75.680233</td>\n",
       "      <td>63.202194</td>\n",
       "      <td>1.899093</td>\n",
       "      <td>36.892542</td>\n",
       "      <td>0.0</td>\n",
       "      <td>139.54</td>\n",
       "      <td>160.947217</td>\n",
       "      <td>291.295681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ma_2019_U</th>\n",
       "      <td>244.23</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>340.039426</td>\n",
       "      <td>118.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>2084.0</td>\n",
       "      <td>4432.0</td>\n",
       "      <td>8734.0</td>\n",
       "      <td>2194.0</td>\n",
       "      <td>3416.557603</td>\n",
       "      <td>...</td>\n",
       "      <td>139.839496</td>\n",
       "      <td>139.049917</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48.373245</td>\n",
       "      <td>46.279992</td>\n",
       "      <td>36.205320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>124.78</td>\n",
       "      <td>146.060780</td>\n",
       "      <td>262.840532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ma_2019_C</th>\n",
       "      <td>243.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>319.988367</td>\n",
       "      <td>118.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>2084.0</td>\n",
       "      <td>4240.0</td>\n",
       "      <td>7972.0</td>\n",
       "      <td>2194.0</td>\n",
       "      <td>3416.557603</td>\n",
       "      <td>...</td>\n",
       "      <td>137.031903</td>\n",
       "      <td>139.049917</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>67.057867</td>\n",
       "      <td>3.124314</td>\n",
       "      <td>36.205320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>130.83</td>\n",
       "      <td>160.947217</td>\n",
       "      <td>269.667774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ma_2019_G</th>\n",
       "      <td>283.28</td>\n",
       "      <td>71.547747</td>\n",
       "      <td>387.546658</td>\n",
       "      <td>144.0</td>\n",
       "      <td>446.0</td>\n",
       "      <td>3270.0</td>\n",
       "      <td>6542.0</td>\n",
       "      <td>12768.0</td>\n",
       "      <td>6578.0</td>\n",
       "      <td>5547.544286</td>\n",
       "      <td>...</td>\n",
       "      <td>117.471961</td>\n",
       "      <td>139.049917</td>\n",
       "      <td>32.387883</td>\n",
       "      <td>80.922082</td>\n",
       "      <td>45.054770</td>\n",
       "      <td>36.939118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>159.51</td>\n",
       "      <td>178.957968</td>\n",
       "      <td>301.046512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ma_2019_dA</th>\n",
       "      <td>251.28</td>\n",
       "      <td>63.146800</td>\n",
       "      <td>315.599992</td>\n",
       "      <td>128.0</td>\n",
       "      <td>336.0</td>\n",
       "      <td>2584.0</td>\n",
       "      <td>4822.0</td>\n",
       "      <td>8812.0</td>\n",
       "      <td>5068.0</td>\n",
       "      <td>4099.715332</td>\n",
       "      <td>...</td>\n",
       "      <td>52.221046</td>\n",
       "      <td>96.366574</td>\n",
       "      <td>75.680233</td>\n",
       "      <td>63.202194</td>\n",
       "      <td>1.899093</td>\n",
       "      <td>36.335611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>119.31</td>\n",
       "      <td>118.263874</td>\n",
       "      <td>281.544850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tang_2019_ArabinoC</th>\n",
       "      <td>243.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>319.988367</td>\n",
       "      <td>118.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>2084.0</td>\n",
       "      <td>4240.0</td>\n",
       "      <td>7972.0</td>\n",
       "      <td>2194.0</td>\n",
       "      <td>3416.557603</td>\n",
       "      <td>...</td>\n",
       "      <td>137.031903</td>\n",
       "      <td>139.049917</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>67.057867</td>\n",
       "      <td>3.124314</td>\n",
       "      <td>36.205320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>130.83</td>\n",
       "      <td>160.947217</td>\n",
       "      <td>269.667774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tang_2019_DideoxyC</th>\n",
       "      <td>211.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>253.494297</td>\n",
       "      <td>103.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>1585.0</td>\n",
       "      <td>2963.0</td>\n",
       "      <td>5062.0</td>\n",
       "      <td>1633.0</td>\n",
       "      <td>2398.199373</td>\n",
       "      <td>...</td>\n",
       "      <td>59.519699</td>\n",
       "      <td>53.683231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>67.057867</td>\n",
       "      <td>3.124314</td>\n",
       "      <td>34.619300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.37</td>\n",
       "      <td>75.580531</td>\n",
       "      <td>250.166113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Peters_2014_3</th>\n",
       "      <td>268.26</td>\n",
       "      <td>66.871159</td>\n",
       "      <td>369.000658</td>\n",
       "      <td>135.0</td>\n",
       "      <td>383.0</td>\n",
       "      <td>2886.0</td>\n",
       "      <td>5763.0</td>\n",
       "      <td>11275.0</td>\n",
       "      <td>5729.0</td>\n",
       "      <td>4739.692713</td>\n",
       "      <td>...</td>\n",
       "      <td>100.945467</td>\n",
       "      <td>139.049917</td>\n",
       "      <td>32.387883</td>\n",
       "      <td>87.641582</td>\n",
       "      <td>27.044020</td>\n",
       "      <td>36.892542</td>\n",
       "      <td>0.0</td>\n",
       "      <td>133.49</td>\n",
       "      <td>146.060780</td>\n",
       "      <td>284.468439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Plank_2016_2</th>\n",
       "      <td>393.17</td>\n",
       "      <td>71.547747</td>\n",
       "      <td>412.608258</td>\n",
       "      <td>144.0</td>\n",
       "      <td>446.0</td>\n",
       "      <td>3270.0</td>\n",
       "      <td>6760.0</td>\n",
       "      <td>13668.0</td>\n",
       "      <td>6578.0</td>\n",
       "      <td>5547.544286</td>\n",
       "      <td>...</td>\n",
       "      <td>73.852917</td>\n",
       "      <td>96.366574</td>\n",
       "      <td>32.387883</td>\n",
       "      <td>80.922082</td>\n",
       "      <td>45.054770</td>\n",
       "      <td>36.939118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>139.28</td>\n",
       "      <td>136.274624</td>\n",
       "      <td>334.202658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Du2021_L_G</th>\n",
       "      <td>283.28</td>\n",
       "      <td>71.547747</td>\n",
       "      <td>387.546658</td>\n",
       "      <td>144.0</td>\n",
       "      <td>446.0</td>\n",
       "      <td>3270.0</td>\n",
       "      <td>6542.0</td>\n",
       "      <td>12768.0</td>\n",
       "      <td>6578.0</td>\n",
       "      <td>5547.544286</td>\n",
       "      <td>...</td>\n",
       "      <td>117.471961</td>\n",
       "      <td>139.049917</td>\n",
       "      <td>32.387883</td>\n",
       "      <td>80.922082</td>\n",
       "      <td>45.054770</td>\n",
       "      <td>36.939118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>159.51</td>\n",
       "      <td>178.957968</td>\n",
       "      <td>301.046512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        MW    D/Dtr09   ZM1MulPer    ECC   CENT    SMTI  \\\n",
       "ID                                                                        \n",
       "Ma_2019_A           267.28  66.871159  348.869542  135.0  383.0  2886.0   \n",
       "Ma_2019_U           244.23   0.000000  340.039426  118.0  295.0  2084.0   \n",
       "Ma_2019_C           243.25   0.000000  319.988367  118.0  295.0  2084.0   \n",
       "Ma_2019_G           283.28  71.547747  387.546658  144.0  446.0  3270.0   \n",
       "Ma_2019_dA          251.28  63.146800  315.599992  128.0  336.0  2584.0   \n",
       "...                    ...        ...         ...    ...    ...     ...   \n",
       "Tang_2019_ArabinoC  243.25   0.000000  319.988367  118.0  295.0  2084.0   \n",
       "Tang_2019_DideoxyC  211.25   0.000000  253.494297  103.0  213.0  1585.0   \n",
       "Peters_2014_3       268.26  66.871159  369.000658  135.0  383.0  2886.0   \n",
       "Plank_2016_2        393.17  71.547747  412.608258  144.0  446.0  3270.0   \n",
       "Du2021_L_G          283.28  71.547747  387.546658  144.0  446.0  3270.0   \n",
       "\n",
       "                     SMTIV    GMTIV     Wap         IDMT  ...      ATSC5s  \\\n",
       "ID                                                        ...               \n",
       "Ma_2019_A           5576.0  10547.0  5729.0  4739.692713  ...   90.763661   \n",
       "Ma_2019_U           4432.0   8734.0  2194.0  3416.557603  ...  139.839496   \n",
       "Ma_2019_C           4240.0   7972.0  2194.0  3416.557603  ...  137.031903   \n",
       "Ma_2019_G           6542.0  12768.0  6578.0  5547.544286  ...  117.471961   \n",
       "Ma_2019_dA          4822.0   8812.0  5068.0  4099.715332  ...   52.221046   \n",
       "...                    ...      ...     ...          ...  ...         ...   \n",
       "Tang_2019_ArabinoC  4240.0   7972.0  2194.0  3416.557603  ...  137.031903   \n",
       "Tang_2019_DideoxyC  2963.0   5062.0  1633.0  2398.199373  ...   59.519699   \n",
       "Peters_2014_3       5763.0  11275.0  5729.0  4739.692713  ...  100.945467   \n",
       "Plank_2016_2        6760.0  13668.0  6578.0  5547.544286  ...   73.852917   \n",
       "Du2021_L_G          6542.0  12768.0  6578.0  5547.544286  ...  117.471961   \n",
       "\n",
       "                    P_VSA_MR_3  P_VSA_ppp_ar  P_VSA_ppp_con  P_VSA_charge_2  \\\n",
       "ID                                                                            \n",
       "Ma_2019_A           139.049917     75.680233      63.202194        1.899093   \n",
       "Ma_2019_U           139.049917      0.000000      48.373245       46.279992   \n",
       "Ma_2019_C           139.049917      0.000000      67.057867        3.124314   \n",
       "Ma_2019_G           139.049917     32.387883      80.922082       45.054770   \n",
       "Ma_2019_dA           96.366574     75.680233      63.202194        1.899093   \n",
       "...                        ...           ...            ...             ...   \n",
       "Tang_2019_ArabinoC  139.049917      0.000000      67.057867        3.124314   \n",
       "Tang_2019_DideoxyC   53.683231      0.000000      67.057867        3.124314   \n",
       "Peters_2014_3       139.049917     32.387883      87.641582       27.044020   \n",
       "Plank_2016_2         96.366574     32.387883      80.922082       45.054770   \n",
       "Du2021_L_G          139.049917     32.387883      80.922082       45.054770   \n",
       "\n",
       "                    SM15_EA(ed)  T(O..Br)  TPSA(Tot)       SAdon          Vx  \n",
       "ID                                                                            \n",
       "Ma_2019_A             36.892542       0.0     139.54  160.947217  291.295681  \n",
       "Ma_2019_U             36.205320       0.0     124.78  146.060780  262.840532  \n",
       "Ma_2019_C             36.205320       0.0     130.83  160.947217  269.667774  \n",
       "Ma_2019_G             36.939118       0.0     159.51  178.957968  301.046512  \n",
       "Ma_2019_dA            36.335611       0.0     119.31  118.263874  281.544850  \n",
       "...                         ...       ...        ...         ...         ...  \n",
       "Tang_2019_ArabinoC    36.205320       0.0     130.83  160.947217  269.667774  \n",
       "Tang_2019_DideoxyC    34.619300       0.0      90.37   75.580531  250.166113  \n",
       "Peters_2014_3         36.892542       0.0     133.49  146.060780  284.468439  \n",
       "Plank_2016_2          36.939118       0.0     139.28  136.274624  334.202658  \n",
       "Du2021_L_G            36.939118       0.0     159.51  178.957968  301.046512  \n",
       "\n",
       "[71 rows x 28 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MRLEM_data=X_NAomit_data[col_data.index]\n",
    "MRLEM_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6d8bf42e-2223-4c7b-9bd2-7c2787032802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MW</th>\n",
       "      <th>D/Dtr09</th>\n",
       "      <th>ZM1MulPer</th>\n",
       "      <th>ECC</th>\n",
       "      <th>CENT</th>\n",
       "      <th>SMTI</th>\n",
       "      <th>SMTIV</th>\n",
       "      <th>GMTIV</th>\n",
       "      <th>Wap</th>\n",
       "      <th>IDMT</th>\n",
       "      <th>...</th>\n",
       "      <th>ATSC5s</th>\n",
       "      <th>P_VSA_MR_3</th>\n",
       "      <th>P_VSA_ppp_ar</th>\n",
       "      <th>P_VSA_ppp_con</th>\n",
       "      <th>P_VSA_charge_2</th>\n",
       "      <th>SM15_EA(ed)</th>\n",
       "      <th>T(O..Br)</th>\n",
       "      <th>TPSA(Tot)</th>\n",
       "      <th>SAdon</th>\n",
       "      <th>Vx</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ma_2019_A</th>\n",
       "      <td>0.128645</td>\n",
       "      <td>0.181294</td>\n",
       "      <td>0.193207</td>\n",
       "      <td>0.036281</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.030815</td>\n",
       "      <td>0.037803</td>\n",
       "      <td>0.044265</td>\n",
       "      <td>0.024087</td>\n",
       "      <td>0.024617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176802</td>\n",
       "      <td>0.999046</td>\n",
       "      <td>0.285905</td>\n",
       "      <td>0.104723</td>\n",
       "      <td>0.015761</td>\n",
       "      <td>0.446054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.472652</td>\n",
       "      <td>0.769847</td>\n",
       "      <td>0.069993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ma_2019_U</th>\n",
       "      <td>0.075722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.175319</td>\n",
       "      <td>0.017007</td>\n",
       "      <td>0.012863</td>\n",
       "      <td>0.011819</td>\n",
       "      <td>0.021252</td>\n",
       "      <td>0.029633</td>\n",
       "      <td>0.003299</td>\n",
       "      <td>0.010706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.401921</td>\n",
       "      <td>0.999046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031193</td>\n",
       "      <td>0.384100</td>\n",
       "      <td>0.311208</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.330770</td>\n",
       "      <td>0.665700</td>\n",
       "      <td>0.021569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ma_2019_C</th>\n",
       "      <td>0.073472</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.134701</td>\n",
       "      <td>0.017007</td>\n",
       "      <td>0.012863</td>\n",
       "      <td>0.011819</td>\n",
       "      <td>0.018475</td>\n",
       "      <td>0.023484</td>\n",
       "      <td>0.003299</td>\n",
       "      <td>0.010706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.389042</td>\n",
       "      <td>0.999046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.123842</td>\n",
       "      <td>0.025930</td>\n",
       "      <td>0.311208</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.388926</td>\n",
       "      <td>0.769847</td>\n",
       "      <td>0.033187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ma_2019_G</th>\n",
       "      <td>0.165381</td>\n",
       "      <td>0.193972</td>\n",
       "      <td>0.271557</td>\n",
       "      <td>0.046485</td>\n",
       "      <td>0.036549</td>\n",
       "      <td>0.039910</td>\n",
       "      <td>0.051778</td>\n",
       "      <td>0.062188</td>\n",
       "      <td>0.029080</td>\n",
       "      <td>0.033111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.299317</td>\n",
       "      <td>0.999046</td>\n",
       "      <td>0.122355</td>\n",
       "      <td>0.192589</td>\n",
       "      <td>0.373931</td>\n",
       "      <td>0.455193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.664616</td>\n",
       "      <td>0.895853</td>\n",
       "      <td>0.086587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ma_2019_dA</th>\n",
       "      <td>0.091909</td>\n",
       "      <td>0.171197</td>\n",
       "      <td>0.125811</td>\n",
       "      <td>0.028345</td>\n",
       "      <td>0.019294</td>\n",
       "      <td>0.023662</td>\n",
       "      <td>0.026894</td>\n",
       "      <td>0.030263</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>0.017889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666030</td>\n",
       "      <td>0.285905</td>\n",
       "      <td>0.104723</td>\n",
       "      <td>0.015761</td>\n",
       "      <td>0.336774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.278189</td>\n",
       "      <td>0.471230</td>\n",
       "      <td>0.053399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  MW   D/Dtr09  ZM1MulPer       ECC      CENT      SMTI  \\\n",
       "ID                                                                        \n",
       "Ma_2019_A   0.128645  0.181294   0.193207  0.036281  0.026667  0.030815   \n",
       "Ma_2019_U   0.075722  0.000000   0.175319  0.017007  0.012863  0.011819   \n",
       "Ma_2019_C   0.073472  0.000000   0.134701  0.017007  0.012863  0.011819   \n",
       "Ma_2019_G   0.165381  0.193972   0.271557  0.046485  0.036549  0.039910   \n",
       "Ma_2019_dA  0.091909  0.171197   0.125811  0.028345  0.019294  0.023662   \n",
       "\n",
       "               SMTIV     GMTIV       Wap      IDMT  ...    ATSC5s  P_VSA_MR_3  \\\n",
       "ID                                                  ...                         \n",
       "Ma_2019_A   0.037803  0.044265  0.024087  0.024617  ...  0.176802    0.999046   \n",
       "Ma_2019_U   0.021252  0.029633  0.003299  0.010706  ...  0.401921    0.999046   \n",
       "Ma_2019_C   0.018475  0.023484  0.003299  0.010706  ...  0.389042    0.999046   \n",
       "Ma_2019_G   0.051778  0.062188  0.029080  0.033111  ...  0.299317    0.999046   \n",
       "Ma_2019_dA  0.026894  0.030263  0.020200  0.017889  ...  0.000000    0.666030   \n",
       "\n",
       "            P_VSA_ppp_ar  P_VSA_ppp_con  P_VSA_charge_2  SM15_EA(ed)  \\\n",
       "ID                                                                     \n",
       "Ma_2019_A       0.285905       0.104723        0.015761     0.446054   \n",
       "Ma_2019_U       0.000000       0.031193        0.384100     0.311208   \n",
       "Ma_2019_C       0.000000       0.123842        0.025930     0.311208   \n",
       "Ma_2019_G       0.122355       0.192589        0.373931     0.455193   \n",
       "Ma_2019_dA      0.285905       0.104723        0.015761     0.336774   \n",
       "\n",
       "            T(O..Br)  TPSA(Tot)     SAdon        Vx  \n",
       "ID                                                   \n",
       "Ma_2019_A        0.0   0.472652  0.769847  0.069993  \n",
       "Ma_2019_U        0.0   0.330770  0.665700  0.021569  \n",
       "Ma_2019_C        0.0   0.388926  0.769847  0.033187  \n",
       "Ma_2019_G        0.0   0.664616  0.895853  0.086587  \n",
       "Ma_2019_dA       0.0   0.278189  0.471230  0.053399  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scale data\n",
    "Scaler = preprocessing.MinMaxScaler() #StandardScaler\n",
    "Transformer =Scaler.fit(MRLEM_data)\n",
    "X_scaled_data=Transformer.transform(MRLEM_data)\n",
    "X_scaled_data =pd.DataFrame(X_scaled_data)\n",
    "X_scaled_data.columns=MRLEM_data.columns\n",
    "X_scaled_data.index=Raw_data.index\n",
    "X_scaled_data.to_csv(\"./Original data/MRLEM_data_X_scaled_data.csv\",sep=',',header=1,index=1)\n",
    "joblib.dump(Transformer, './Models/MRLEM_data_Scaler_transformer.pkl')\n",
    "\n",
    "X_scaled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c5eb8b75-76bc-4fcb-aad5-6762048df1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_results(Model_clf,X_test,y,Cv_model):\n",
    "    Model_scores= cross_validate(estimator=Model_clf, X=X_test, y=y, cv=Cv_model,scoring=( 'accuracy','f1','precision','recall','roc_auc'), return_train_score=True)\n",
    "    Model_score= cross_validate(estimator=Model_clf, X=X_test, y=y, cv=Cv_model,scoring=( 'accuracy','f1','precision','recall','roc_auc'), return_train_score=False)\n",
    "#Accuracy\n",
    "    Model_Accuracy_test_mean=Model_scores['test_accuracy'].mean()\n",
    "    Model_Accuracy_test_se=(Model_scores['test_accuracy'].std()/math.sqrt(len(Model_scores['test_accuracy']))) \n",
    "    Model_Accuracy_train_mean=Model_scores['train_accuracy'].mean()\n",
    "    Model_Accuracy_train_se=(Model_scores['train_accuracy'].std()/math.sqrt(len(Model_scores['train_accuracy']))) \n",
    "#f1\n",
    "    Model_f1_mean=Model_score['test_f1'].mean()\n",
    "    Model_f1_se=(Model_score['test_f1'].std()/math.sqrt(len(Model_score['test_f1']))) \n",
    "#precision\n",
    "    Model_precision_mean=Model_score['test_precision'].mean()\n",
    "    Model_precision_se=(Model_score['test_precision'].std()/math.sqrt(len(Model_score['test_precision']))) \n",
    "#recall\n",
    "    Model_recall_mean=Model_score['test_recall'].mean()\n",
    "    Model_recall_se=(Model_score['test_recall'].std()/math.sqrt(len(Model_score['test_recall']))) \n",
    "#roc_auc\n",
    "    Model_roc_auc_mean=Model_score['test_roc_auc'].mean()\n",
    "    Model_roc_auc_se=(Model_score['test_roc_auc'].std()/math.sqrt(len(Model_score['test_roc_auc']))) \n",
    "    Model = {'Mean':[Model_Accuracy_test_mean,Model_Accuracy_train_mean,Model_f1_mean,Model_precision_mean,Model_recall_mean,Model_roc_auc_mean],\n",
    "        'Se':[Model_Accuracy_test_se,Model_Accuracy_train_se,Model_f1_se,Model_precision_se,Model_recall_se,Model_roc_auc_se]}\n",
    "    Model = pd.DataFrame(Model, index=['Accuracy_test','Accuracy_train','F1 Score','Precision','Recall','Roc_auc'])\n",
    "    return Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "43024221-dcb1-4c20-a862-8ba15cc4978a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the KFold \n",
    "Cv_optuna= RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=0)\n",
    "Cv_model= RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=0)\n",
    "Cv_RFECV= RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ef2d9106-5c7e-4daa-89d3-1c228a5ac78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data pre-processing of models\n",
    "X=np.array(X_scaled_data)\n",
    "y=Raw_data['Hydrogel-forming ability'].values\n",
    "clf=LogisticRegression(solver='liblinear',random_state=0,dual=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b68d6d-1a1e-439b-aee8-42486486d23e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.1 DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "74f1bf04-5bde-4d51-bc10-aee4e7d2c583",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=DecisionTreeClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7c30897c-633f-4a53-96c3-a415fd50308c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.629333</td>\n",
       "      <td>0.015032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.729981</td>\n",
       "      <td>0.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.697627</td>\n",
       "      <td>0.013235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.620752</td>\n",
       "      <td>0.011961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.811071</td>\n",
       "      <td>0.020893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.664796</td>\n",
       "      <td>0.020543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean        Se\n",
       "Accuracy_test   0.629333  0.015032\n",
       "Accuracy_train  0.729981  0.004700\n",
       "F1 Score        0.697627  0.013235\n",
       "Precision       0.620752  0.011961\n",
       "Recall          0.811071  0.020893\n",
       "Roc_auc         0.664796  0.020543"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model1 \n",
    "Model1_clf=clf\n",
    "#Model1\n",
    "Model1=Model_results(Model1_clf,X,y,Cv_model)\n",
    "Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f86a6975-0d73-46e6-9ba4-c3c488b2a565",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2024-01-12 01:29:56,212]\u001b[0m A new study created in memory with name: no-name-6a0b1671-0ec2-4905-98f9-69d2f7e33654\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:57,476]\u001b[0m Trial 0 finished with value: 0.6312380952380953 and parameters: {'max_depth': 4, 'max_features': 17, 'min_samples_split': 16}. Best is trial 0 with value: 0.6312380952380953.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:57,505]\u001b[0m Trial 1 finished with value: 0.5926666666666667 and parameters: {'max_depth': 4, 'max_features': 14, 'min_samples_split': 17}. Best is trial 0 with value: 0.6312380952380953.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:57,530]\u001b[0m Trial 2 finished with value: 0.6375238095238095 and parameters: {'max_depth': 4, 'max_features': 19, 'min_samples_split': 25}. Best is trial 2 with value: 0.6375238095238095.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:57,554]\u001b[0m Trial 3 finished with value: 0.6208571428571428 and parameters: {'max_depth': 4, 'max_features': 18, 'min_samples_split': 14}. Best is trial 2 with value: 0.6375238095238095.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:57,579]\u001b[0m Trial 4 finished with value: 0.6296190476190476 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 3}. Best is trial 2 with value: 0.6375238095238095.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:57,600]\u001b[0m Trial 5 finished with value: 0.6120952380952381 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 21}. Best is trial 2 with value: 0.6375238095238095.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:57,624]\u001b[0m Trial 6 finished with value: 0.6318095238095238 and parameters: {'max_depth': 5, 'max_features': 19, 'min_samples_split': 25}. Best is trial 2 with value: 0.6375238095238095.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:57,646]\u001b[0m Trial 7 finished with value: 0.5998095238095238 and parameters: {'max_depth': 5, 'max_features': 15, 'min_samples_split': 20}. Best is trial 2 with value: 0.6375238095238095.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:57,666]\u001b[0m Trial 8 finished with value: 0.6439999999999999 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 5}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:57,689]\u001b[0m Trial 9 finished with value: 0.6057142857142856 and parameters: {'max_depth': 5, 'max_features': 15, 'min_samples_split': 11}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:57,714]\u001b[0m Trial 10 finished with value: 0.601047619047619 and parameters: {'max_depth': 3, 'max_features': 12, 'min_samples_split': 2}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:57,739]\u001b[0m Trial 11 finished with value: 0.6411428571428571 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 8}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:57,765]\u001b[0m Trial 12 finished with value: 0.6411428571428571 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 9}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:57,789]\u001b[0m Trial 13 finished with value: 0.6411428571428571 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 7}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:57,813]\u001b[0m Trial 14 finished with value: 0.6280952380952382 and parameters: {'max_depth': 3, 'max_features': 13, 'min_samples_split': 6}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:57,838]\u001b[0m Trial 15 finished with value: 0.64 and parameters: {'max_depth': 3, 'max_features': 16, 'min_samples_split': 5}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:57,862]\u001b[0m Trial 16 finished with value: 0.64 and parameters: {'max_depth': 3, 'max_features': 16, 'min_samples_split': 10}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:57,887]\u001b[0m Trial 17 finished with value: 0.6042857142857144 and parameters: {'max_depth': 3, 'max_features': 20, 'min_samples_split': 6}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:57,914]\u001b[0m Trial 18 finished with value: 0.6207619047619047 and parameters: {'max_depth': 4, 'max_features': 18, 'min_samples_split': 12}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:57,939]\u001b[0m Trial 19 finished with value: 0.6295238095238096 and parameters: {'max_depth': 3, 'max_features': 13, 'min_samples_split': 4}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:57,964]\u001b[0m Trial 20 finished with value: 0.6385714285714286 and parameters: {'max_depth': 3, 'max_features': 16, 'min_samples_split': 7}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:57,989]\u001b[0m Trial 21 finished with value: 0.6411428571428571 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 9}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,013]\u001b[0m Trial 22 finished with value: 0.6111428571428571 and parameters: {'max_depth': 3, 'max_features': 18, 'min_samples_split': 13}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,038]\u001b[0m Trial 23 finished with value: 0.6411428571428571 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 8}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,063]\u001b[0m Trial 24 finished with value: 0.6253333333333333 and parameters: {'max_depth': 3, 'max_features': 19, 'min_samples_split': 9}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,088]\u001b[0m Trial 25 finished with value: 0.6405714285714285 and parameters: {'max_depth': 4, 'max_features': 16, 'min_samples_split': 10}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,114]\u001b[0m Trial 26 finished with value: 0.6097142857142858 and parameters: {'max_depth': 3, 'max_features': 14, 'min_samples_split': 4}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,137]\u001b[0m Trial 27 finished with value: 0.6129523809523809 and parameters: {'max_depth': 3, 'max_features': 15, 'min_samples_split': 6}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,163]\u001b[0m Trial 28 finished with value: 0.6222857142857143 and parameters: {'max_depth': 4, 'max_features': 18, 'min_samples_split': 7}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,187]\u001b[0m Trial 29 finished with value: 0.6396190476190476 and parameters: {'max_depth': 4, 'max_features': 17, 'min_samples_split': 14}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,212]\u001b[0m Trial 30 finished with value: 0.6363809523809524 and parameters: {'max_depth': 3, 'max_features': 19, 'min_samples_split': 16}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,237]\u001b[0m Trial 31 finished with value: 0.6411428571428571 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 8}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,260]\u001b[0m Trial 32 finished with value: 0.6385714285714286 and parameters: {'max_depth': 3, 'max_features': 16, 'min_samples_split': 9}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,285]\u001b[0m Trial 33 finished with value: 0.624 and parameters: {'max_depth': 3, 'max_features': 18, 'min_samples_split': 2}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,312]\u001b[0m Trial 34 finished with value: 0.6411428571428571 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 11}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,335]\u001b[0m Trial 35 finished with value: 0.614 and parameters: {'max_depth': 4, 'max_features': 15, 'min_samples_split': 4}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,360]\u001b[0m Trial 36 finished with value: 0.6111428571428571 and parameters: {'max_depth': 3, 'max_features': 18, 'min_samples_split': 12}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,386]\u001b[0m Trial 37 finished with value: 0.6011428571428571 and parameters: {'max_depth': 4, 'max_features': 14, 'min_samples_split': 7}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,413]\u001b[0m Trial 38 finished with value: 0.6281904761904762 and parameters: {'max_depth': 3, 'max_features': 19, 'min_samples_split': 5}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,439]\u001b[0m Trial 39 finished with value: 0.6067619047619047 and parameters: {'max_depth': 4, 'max_features': 10, 'min_samples_split': 16}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,466]\u001b[0m Trial 40 finished with value: 0.6393333333333333 and parameters: {'max_depth': 5, 'max_features': 16, 'min_samples_split': 11}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,491]\u001b[0m Trial 41 finished with value: 0.6411428571428571 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 9}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,519]\u001b[0m Trial 42 finished with value: 0.6368571428571428 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 14}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,545]\u001b[0m Trial 43 finished with value: 0.6411428571428571 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 8}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,570]\u001b[0m Trial 44 finished with value: 0.614 and parameters: {'max_depth': 3, 'max_features': 18, 'min_samples_split': 10}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,597]\u001b[0m Trial 45 finished with value: 0.6411428571428571 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 8}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,624]\u001b[0m Trial 46 finished with value: 0.6056190476190477 and parameters: {'max_depth': 3, 'max_features': 20, 'min_samples_split': 5}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,651]\u001b[0m Trial 47 finished with value: 0.64 and parameters: {'max_depth': 3, 'max_features': 16, 'min_samples_split': 3}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,678]\u001b[0m Trial 48 finished with value: 0.6143809523809524 and parameters: {'max_depth': 3, 'max_features': 15, 'min_samples_split': 8}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,704]\u001b[0m Trial 49 finished with value: 0.6180952380952381 and parameters: {'max_depth': 3, 'max_features': 18, 'min_samples_split': 21}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,729]\u001b[0m Trial 50 finished with value: 0.6406666666666666 and parameters: {'max_depth': 3, 'max_features': 11, 'min_samples_split': 3}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,758]\u001b[0m Trial 51 finished with value: 0.6411428571428571 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 11}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,785]\u001b[0m Trial 52 finished with value: 0.6411428571428571 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 11}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,811]\u001b[0m Trial 53 finished with value: 0.6371428571428572 and parameters: {'max_depth': 3, 'max_features': 16, 'min_samples_split': 12}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,838]\u001b[0m Trial 54 finished with value: 0.6425714285714286 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 6}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,863]\u001b[0m Trial 55 finished with value: 0.6211428571428571 and parameters: {'max_depth': 3, 'max_features': 18, 'min_samples_split': 6}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,888]\u001b[0m Trial 56 finished with value: 0.6375238095238095 and parameters: {'max_depth': 3, 'max_features': 19, 'min_samples_split': 24}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,915]\u001b[0m Trial 57 finished with value: 0.6422857142857145 and parameters: {'max_depth': 5, 'max_features': 16, 'min_samples_split': 7}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,941]\u001b[0m Trial 58 finished with value: 0.6604761904761905 and parameters: {'max_depth': 5, 'max_features': 16, 'min_samples_split': 5}. Best is trial 58 with value: 0.6604761904761905.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,966]\u001b[0m Trial 59 finished with value: 0.6406666666666666 and parameters: {'max_depth': 5, 'max_features': 15, 'min_samples_split': 6}. Best is trial 58 with value: 0.6604761904761905.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:58,992]\u001b[0m Trial 60 finished with value: 0.6604761904761905 and parameters: {'max_depth': 5, 'max_features': 16, 'min_samples_split': 5}. Best is trial 58 with value: 0.6604761904761905.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,017]\u001b[0m Trial 61 finished with value: 0.6604761904761905 and parameters: {'max_depth': 5, 'max_features': 16, 'min_samples_split': 5}. Best is trial 58 with value: 0.6604761904761905.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,044]\u001b[0m Trial 62 finished with value: 0.647904761904762 and parameters: {'max_depth': 5, 'max_features': 14, 'min_samples_split': 3}. Best is trial 58 with value: 0.6604761904761905.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,073]\u001b[0m Trial 63 finished with value: 0.6742857142857143 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 2}. Best is trial 63 with value: 0.6742857142857143.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,099]\u001b[0m Trial 64 finished with value: 0.6520952380952381 and parameters: {'max_depth': 5, 'max_features': 14, 'min_samples_split': 2}. Best is trial 63 with value: 0.6742857142857143.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,125]\u001b[0m Trial 65 finished with value: 0.6742857142857143 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 2}. Best is trial 63 with value: 0.6742857142857143.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,151]\u001b[0m Trial 66 finished with value: 0.68 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 3}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,176]\u001b[0m Trial 67 finished with value: 0.6742857142857143 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 2}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,201]\u001b[0m Trial 68 finished with value: 0.6742857142857143 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 2}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,226]\u001b[0m Trial 69 finished with value: 0.6742857142857143 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 2}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,252]\u001b[0m Trial 70 finished with value: 0.6742857142857143 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 2}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,278]\u001b[0m Trial 71 finished with value: 0.6742857142857143 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 2}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,302]\u001b[0m Trial 72 finished with value: 0.6742857142857143 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 2}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,328]\u001b[0m Trial 73 finished with value: 0.6690476190476191 and parameters: {'max_depth': 5, 'max_features': 12, 'min_samples_split': 3}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,355]\u001b[0m Trial 74 finished with value: 0.6761904761904762 and parameters: {'max_depth': 5, 'max_features': 12, 'min_samples_split': 2}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,380]\u001b[0m Trial 75 finished with value: 0.6676190476190478 and parameters: {'max_depth': 5, 'max_features': 12, 'min_samples_split': 4}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,416]\u001b[0m Trial 76 finished with value: 0.6742857142857143 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 2}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,444]\u001b[0m Trial 77 finished with value: 0.6690476190476191 and parameters: {'max_depth': 5, 'max_features': 12, 'min_samples_split': 3}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,471]\u001b[0m Trial 78 finished with value: 0.6451428571428572 and parameters: {'max_depth': 5, 'max_features': 11, 'min_samples_split': 4}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,496]\u001b[0m Trial 79 finished with value: 0.6742857142857143 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 2}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,522]\u001b[0m Trial 80 finished with value: 0.6676190476190478 and parameters: {'max_depth': 5, 'max_features': 12, 'min_samples_split': 4}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,548]\u001b[0m Trial 81 finished with value: 0.6742857142857143 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 2}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,573]\u001b[0m Trial 82 finished with value: 0.68 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 3}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,599]\u001b[0m Trial 83 finished with value: 0.647904761904762 and parameters: {'max_depth': 5, 'max_features': 14, 'min_samples_split': 3}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,624]\u001b[0m Trial 84 finished with value: 0.6478095238095238 and parameters: {'max_depth': 5, 'max_features': 11, 'min_samples_split': 3}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,649]\u001b[0m Trial 85 finished with value: 0.6759999999999999 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 4}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,675]\u001b[0m Trial 86 finished with value: 0.6676190476190478 and parameters: {'max_depth': 5, 'max_features': 12, 'min_samples_split': 4}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,701]\u001b[0m Trial 87 finished with value: 0.6337142857142858 and parameters: {'max_depth': 5, 'max_features': 12, 'min_samples_split': 18}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,729]\u001b[0m Trial 88 finished with value: 0.68 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 3}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,758]\u001b[0m Trial 89 finished with value: 0.647904761904762 and parameters: {'max_depth': 5, 'max_features': 14, 'min_samples_split': 3}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,784]\u001b[0m Trial 90 finished with value: 0.68 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 3}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,809]\u001b[0m Trial 91 finished with value: 0.6759999999999999 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 4}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,836]\u001b[0m Trial 92 finished with value: 0.6759999999999999 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 4}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,862]\u001b[0m Trial 93 finished with value: 0.6759999999999999 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 4}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,889]\u001b[0m Trial 94 finished with value: 0.6379047619047619 and parameters: {'max_depth': 5, 'max_features': 14, 'min_samples_split': 4}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,915]\u001b[0m Trial 95 finished with value: 0.6676190476190478 and parameters: {'max_depth': 5, 'max_features': 12, 'min_samples_split': 4}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,942]\u001b[0m Trial 96 finished with value: 0.68 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 3}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,968]\u001b[0m Trial 97 finished with value: 0.6421904761904762 and parameters: {'max_depth': 5, 'max_features': 14, 'min_samples_split': 5}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:29:59,993]\u001b[0m Trial 98 finished with value: 0.6634285714285714 and parameters: {'max_depth': 5, 'max_features': 12, 'min_samples_split': 5}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:30:00,019]\u001b[0m Trial 99 finished with value: 0.6478095238095238 and parameters: {'max_depth': 5, 'max_features': 11, 'min_samples_split': 3}. Best is trial 66 with value: 0.68.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Tuning hyperparameters\n",
    "#Step 1. Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'max_depth': trial.suggest_int('max_depth',3,5,1),\n",
    "        'max_features' : trial.suggest_int(\"max_features\",10,20,1),\n",
    "        'min_samples_split':trial.suggest_int('min_samples_split',2,25,1)\n",
    "    }\n",
    "    model = DecisionTreeClassifier(**param,random_state=1)\n",
    "\n",
    " \n",
    "# Step 2: Scoring method:\n",
    "    score = model_selection.cross_val_score(model, X, y, n_jobs=8, scoring=\"accuracy\",cv=Cv_optuna)\n",
    "    accuracy = score.mean()\n",
    "    return accuracy\n",
    " \n",
    "# Step 3: Running it\n",
    "study = optuna.create_study(direction=\"maximize\",sampler=optuna.samplers.TPESampler(seed=0))\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6ec0811a-1713-4b39-b21f-cb09e9594c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are : \n",
      "{'max_depth': 5, 'max_features': 13, 'min_samples_split': 3}\n"
     ]
    }
   ],
   "source": [
    "# Getting the best parameters:\n",
    "print(f\"The best parameters are : \\n{study.best_params}\")\n",
    "# Setting the best model\n",
    "clf =DecisionTreeClassifier(max_depth = study.best_params['max_depth']\n",
    "              ,max_features = study.best_params['max_features']\n",
    "              #,n_estimators = study.best_params['n_estimators']\n",
    "              #,learning_rate = study.best_params['learning_rate']\n",
    "              ,min_samples_split= study.best_params['min_samples_split']\n",
    "              ,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9d1899f3-a5c0-4f3d-8fb2-176c72ea4a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.012329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.925345</td>\n",
       "      <td>0.005584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.706865</td>\n",
       "      <td>0.012092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.698240</td>\n",
       "      <td>0.013460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.735714</td>\n",
       "      <td>0.020076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.690561</td>\n",
       "      <td>0.015281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean        Se\n",
       "Accuracy_test   0.680000  0.012329\n",
       "Accuracy_train  0.925345  0.005584\n",
       "F1 Score        0.706865  0.012092\n",
       "Precision       0.698240  0.013460\n",
       "Recall          0.735714  0.020076\n",
       "Roc_auc         0.690561  0.015281"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model2\n",
    "Model2=Model_results(clf,X,y,Cv_model)\n",
    "Model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "390c5554-ccf3-490e-9e3e-7d53299bb3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Method</th>\n",
       "      <th colspan=\"4\" halign=\"left\">DT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Model 1</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Model 2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Values</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.629333</td>\n",
       "      <td>0.015032</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.012329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.729981</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.925345</td>\n",
       "      <td>0.005584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.697627</td>\n",
       "      <td>0.013235</td>\n",
       "      <td>0.706865</td>\n",
       "      <td>0.012092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.620752</td>\n",
       "      <td>0.011961</td>\n",
       "      <td>0.698240</td>\n",
       "      <td>0.013460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.811071</td>\n",
       "      <td>0.020893</td>\n",
       "      <td>0.735714</td>\n",
       "      <td>0.020076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.664796</td>\n",
       "      <td>0.020543</td>\n",
       "      <td>0.690561</td>\n",
       "      <td>0.015281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Method                DT                              \n",
       "Model            Model 1             Model 2          \n",
       "Values              Mean        Se      Mean        Se\n",
       "Accuracy_test   0.629333  0.015032  0.680000  0.012329\n",
       "Accuracy_train  0.729981  0.004700  0.925345  0.005584\n",
       "F1 Score        0.697627  0.013235  0.706865  0.012092\n",
       "Precision       0.620752  0.011961  0.698240  0.013460\n",
       "Recall          0.811071  0.020893  0.735714  0.020076\n",
       "Roc_auc         0.664796  0.020543  0.690561  0.015281"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saving the data of model performance\n",
    "Model_data=pd.concat([Model1,Model2],axis=1)\n",
    "Model_data.columns = [['DT']*4,['Model 1','Model 1', 'Model 2','Model 2'], ['Mean', 'Se', 'Mean', 'Se']]  \n",
    "Model_data.columns.names=['Method','Model','Values']\n",
    "Model_data.to_csv(\"./Results/DT_model_mlrem_data.csv\",sep=',')\n",
    "Model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff716df6-f48a-4890-819b-cca04c92e57c",
   "metadata": {},
   "source": [
    "## 2.2 LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "96eeb954-8032-43c2-afb2-e47f675b2d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.012329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.925345</td>\n",
       "      <td>0.005584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.706865</td>\n",
       "      <td>0.012092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.698240</td>\n",
       "      <td>0.013460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.735714</td>\n",
       "      <td>0.020076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.690561</td>\n",
       "      <td>0.015281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean        Se\n",
       "Accuracy_test   0.680000  0.012329\n",
       "Accuracy_train  0.925345  0.005584\n",
       "F1 Score        0.706865  0.012092\n",
       "Precision       0.698240  0.013460\n",
       "Recall          0.735714  0.020076\n",
       "Roc_auc         0.690561  0.015281"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model1\n",
    "Model1_clf=clf\n",
    "#Model1\n",
    "Model1=Model_results(Model1_clf,X,y,Cv_model)\n",
    "Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "662dc3e0-7ca9-41f6-ad45-99db640a18b8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2024-01-12 01:33:46,194]\u001b[0m A new study created in memory with name: no-name-e4ba7b5b-b8fd-4929-82c3-50137ca4832b\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:46,218]\u001b[0m Trial 0 finished with value: 0.6096190476190476 and parameters: {'logreg_c': 0.3177840006884068, 'l1_ratio': 0.7482920440979423, 'max_iter': 100}. Best is trial 0 with value: 0.6096190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:46,239]\u001b[0m Trial 1 finished with value: 0.6195238095238095 and parameters: {'logreg_c': 0.0651621545821569, 'l1_ratio': 0.23208030173540176, 'max_iter': 275}. Best is trial 1 with value: 0.6195238095238095.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:46,262]\u001b[0m Trial 2 finished with value: 0.570095238095238 and parameters: {'logreg_c': 0.013108749615263334, 'l1_ratio': 0.411004654338743, 'max_iter': 854}. Best is trial 1 with value: 0.6195238095238095.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:46,284]\u001b[0m Trial 3 finished with value: 0.6294285714285714 and parameters: {'logreg_c': 1.7096232052870346, 'l1_ratio': 0.4772750629629653, 'max_iter': 1402}. Best is trial 3 with value: 0.6294285714285714.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:46,305]\u001b[0m Trial 4 finished with value: 0.574095238095238 and parameters: {'logreg_c': 0.016854407828169382, 'l1_ratio': 0.8903056927518509, 'max_iter': 152}. Best is trial 3 with value: 0.6294285714285714.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:46,328]\u001b[0m Trial 5 finished with value: 0.6167619047619047 and parameters: {'logreg_c': 10.539137268289434, 'l1_ratio': 0.4755743221304143, 'max_iter': 1162}. Best is trial 3 with value: 0.6294285714285714.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:46,351]\u001b[0m Trial 6 finished with value: 0.5672380952380952 and parameters: {'logreg_c': 0.006955392321661603, 'l1_ratio': 0.2782913401763909, 'max_iter': 1622}. Best is trial 3 with value: 0.6294285714285714.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:46,373]\u001b[0m Trial 7 finished with value: 0.6282857142857143 and parameters: {'logreg_c': 645.0144652189372, 'l1_ratio': 0.38208176034331853, 'max_iter': 1416}. Best is trial 3 with value: 0.6294285714285714.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:46,397]\u001b[0m Trial 8 finished with value: 0.6093333333333334 and parameters: {'logreg_c': 181.27374779133626, 'l1_ratio': 0.9051459971534626, 'max_iter': 261}. Best is trial 3 with value: 0.6294285714285714.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:46,418]\u001b[0m Trial 9 finished with value: 0.5601904761904761 and parameters: {'logreg_c': 0.001715255021408637, 'l1_ratio': 0.25284737760811204, 'max_iter': 1769}. Best is trial 3 with value: 0.6294285714285714.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:46,444]\u001b[0m Trial 10 finished with value: 0.6153333333333333 and parameters: {'logreg_c': 9.589685409552947, 'l1_ratio': 0.6553689413187243, 'max_iter': 845}. Best is trial 3 with value: 0.6294285714285714.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:46,469]\u001b[0m Trial 11 finished with value: 0.6268571428571428 and parameters: {'logreg_c': 843.2126062012233, 'l1_ratio': 0.5380831473609496, 'max_iter': 1376}. Best is trial 3 with value: 0.6294285714285714.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:46,493]\u001b[0m Trial 12 finished with value: 0.6167619047619047 and parameters: {'logreg_c': 7.624466107886496, 'l1_ratio': 0.378272352651409, 'max_iter': 1494}. Best is trial 3 with value: 0.6294285714285714.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:46,518]\u001b[0m Trial 13 finished with value: 0.6162857142857143 and parameters: {'logreg_c': 51.786339691986214, 'l1_ratio': 0.6316370562712422, 'max_iter': 1128}. Best is trial 3 with value: 0.6294285714285714.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:46,543]\u001b[0m Trial 14 finished with value: 0.627904761904762 and parameters: {'logreg_c': 0.8529452363532304, 'l1_ratio': 0.13738672304214128, 'max_iter': 1780}. Best is trial 3 with value: 0.6294285714285714.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:46,570]\u001b[0m Trial 15 finished with value: 0.6281904761904762 and parameters: {'logreg_c': 763.5588094994378, 'l1_ratio': 0.3689322680983461, 'max_iter': 1971}. Best is trial 3 with value: 0.6294285714285714.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:46,596]\u001b[0m Trial 16 finished with value: 0.6195238095238096 and parameters: {'logreg_c': 0.13707192890174966, 'l1_ratio': 0.5845266407991517, 'max_iter': 1309}. Best is trial 3 with value: 0.6294285714285714.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:46,623]\u001b[0m Trial 17 finished with value: 0.6120000000000001 and parameters: {'logreg_c': 76.37303329769077, 'l1_ratio': 0.7683639981317943, 'max_iter': 716}. Best is trial 3 with value: 0.6294285714285714.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:46,649]\u001b[0m Trial 18 finished with value: 0.6252380952380953 and parameters: {'logreg_c': 2.932001936719566, 'l1_ratio': 0.10084185856253824, 'max_iter': 553}. Best is trial 3 with value: 0.6294285714285714.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:46,674]\u001b[0m Trial 19 finished with value: 0.6307619047619046 and parameters: {'logreg_c': 1.3160447446956969, 'l1_ratio': 0.4671561613242705, 'max_iter': 1312}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:46,701]\u001b[0m Trial 20 finished with value: 0.6293333333333333 and parameters: {'logreg_c': 1.4651691779773963, 'l1_ratio': 0.4935134748265029, 'max_iter': 983}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:46,725]\u001b[0m Trial 21 finished with value: 0.6237142857142858 and parameters: {'logreg_c': 0.8287025892084571, 'l1_ratio': 0.5199862466969595, 'max_iter': 1054}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:46,751]\u001b[0m Trial 22 finished with value: 0.6238095238095238 and parameters: {'logreg_c': 3.1654611271505058, 'l1_ratio': 0.4597028916076701, 'max_iter': 1266}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:46,777]\u001b[0m Trial 23 finished with value: 0.6166666666666667 and parameters: {'logreg_c': 0.1535200845989714, 'l1_ratio': 0.6782814588832947, 'max_iter': 981}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:46,803]\u001b[0m Trial 24 finished with value: 0.6294285714285714 and parameters: {'logreg_c': 1.8873063599506599, 'l1_ratio': 0.326102166138698, 'max_iter': 1522}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:46,827]\u001b[0m Trial 25 finished with value: 0.6180952380952381 and parameters: {'logreg_c': 0.41516225975438653, 'l1_ratio': 0.2979284535359352, 'max_iter': 1547}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:46,853]\u001b[0m Trial 26 finished with value: 0.6191428571428571 and parameters: {'logreg_c': 17.953162228525414, 'l1_ratio': 0.31727555023219167, 'max_iter': 1687}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:46,878]\u001b[0m Trial 27 finished with value: 0.6266666666666666 and parameters: {'logreg_c': 2.602409837373429, 'l1_ratio': 0.19868034857393024, 'max_iter': 1919}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:46,905]\u001b[0m Trial 28 finished with value: 0.6195238095238095 and parameters: {'logreg_c': 0.07229078967635784, 'l1_ratio': 0.42868388614084174, 'max_iter': 1511}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:46,931]\u001b[0m Trial 29 finished with value: 0.6194285714285716 and parameters: {'logreg_c': 0.3860177138367054, 'l1_ratio': 0.8158505713189557, 'max_iter': 1211}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:46,956]\u001b[0m Trial 30 finished with value: 0.6168571428571429 and parameters: {'logreg_c': 4.8505444157280655, 'l1_ratio': 0.989090737470458, 'max_iter': 1389}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:46,984]\u001b[0m Trial 31 finished with value: 0.6293333333333334 and parameters: {'logreg_c': 1.2065391735314397, 'l1_ratio': 0.4963584342131737, 'max_iter': 1011}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,011]\u001b[0m Trial 32 finished with value: 0.6162857142857143 and parameters: {'logreg_c': 28.72272514710678, 'l1_ratio': 0.581199409657952, 'max_iter': 582}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,038]\u001b[0m Trial 33 finished with value: 0.6293333333333333 and parameters: {'logreg_c': 1.3696220900607625, 'l1_ratio': 0.42558482659301816, 'max_iter': 1297}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,063]\u001b[0m Trial 34 finished with value: 0.6067619047619047 and parameters: {'logreg_c': 0.2875637249472787, 'l1_ratio': 0.3482207466346646, 'max_iter': 1109}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,088]\u001b[0m Trial 35 finished with value: 0.6009523809523809 and parameters: {'logreg_c': 0.03576221722627603, 'l1_ratio': 0.5765441371212959, 'max_iter': 870}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,116]\u001b[0m Trial 36 finished with value: 0.6209523809523809 and parameters: {'logreg_c': 0.6626791637945557, 'l1_ratio': 0.20185400717777569, 'max_iter': 1676}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,140]\u001b[0m Trial 37 finished with value: 0.6294285714285714 and parameters: {'logreg_c': 1.9120072183363928, 'l1_ratio': 0.4733809188346099, 'max_iter': 1467}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,165]\u001b[0m Trial 38 finished with value: 0.6177142857142858 and parameters: {'logreg_c': 19.34210830669201, 'l1_ratio': 0.44990617999528343, 'max_iter': 1557}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,192]\u001b[0m Trial 39 finished with value: 0.6139047619047618 and parameters: {'logreg_c': 0.22059945415572177, 'l1_ratio': 0.3267785933880323, 'max_iter': 1424}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,219]\u001b[0m Trial 40 finished with value: 0.6139047619047618 and parameters: {'logreg_c': 5.702688279032213, 'l1_ratio': 0.706870700092175, 'max_iter': 1216}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,247]\u001b[0m Trial 41 finished with value: 0.6307619047619047 and parameters: {'logreg_c': 2.1083445203193767, 'l1_ratio': 0.5013573618384977, 'max_iter': 1814}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,273]\u001b[0m Trial 42 finished with value: 0.6294285714285714 and parameters: {'logreg_c': 1.8778046360697456, 'l1_ratio': 0.39812276354777576, 'max_iter': 1899}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,308]\u001b[0m Trial 43 finished with value: 0.6196190476190475 and parameters: {'logreg_c': 4.005595097886426, 'l1_ratio': 0.40683807642605113, 'max_iter': 1851}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,335]\u001b[0m Trial 44 finished with value: 0.6153333333333333 and parameters: {'logreg_c': 10.51995398501394, 'l1_ratio': 0.5242462748652383, 'max_iter': 1635}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,361]\u001b[0m Trial 45 finished with value: 0.6166666666666667 and parameters: {'logreg_c': 0.43649305773415953, 'l1_ratio': 0.62669435281089, 'max_iter': 1803}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,387]\u001b[0m Trial 46 finished with value: 0.6195238095238095 and parameters: {'logreg_c': 0.07187646725784044, 'l1_ratio': 0.281967448657918, 'max_iter': 1917}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,414]\u001b[0m Trial 47 finished with value: 0.627904761904762 and parameters: {'logreg_c': 2.0310237754182316, 'l1_ratio': 0.39414395429506727, 'max_iter': 1726}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,441]\u001b[0m Trial 48 finished with value: 0.620952380952381 and parameters: {'logreg_c': 0.6082079556285477, 'l1_ratio': 0.5660058818783812, 'max_iter': 1618}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,466]\u001b[0m Trial 49 finished with value: 0.6108571428571429 and parameters: {'logreg_c': 240.59569344033093, 'l1_ratio': 0.24451803344912826, 'max_iter': 1985}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,492]\u001b[0m Trial 50 finished with value: 0.5643809523809523 and parameters: {'logreg_c': 0.003166482546125131, 'l1_ratio': 0.4598078240788601, 'max_iter': 1365}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,518]\u001b[0m Trial 51 finished with value: 0.6167619047619047 and parameters: {'logreg_c': 7.943065063340901, 'l1_ratio': 0.3551537545361903, 'max_iter': 1474}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,545]\u001b[0m Trial 52 finished with value: 0.6294285714285714 and parameters: {'logreg_c': 1.8518389019522237, 'l1_ratio': 0.49224974644280434, 'max_iter': 1844}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,571]\u001b[0m Trial 53 finished with value: 0.627904761904762 and parameters: {'logreg_c': 1.3572714107165977, 'l1_ratio': 0.49277895678099476, 'max_iter': 1458}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,598]\u001b[0m Trial 54 finished with value: 0.6210476190476191 and parameters: {'logreg_c': 3.7115076962149858, 'l1_ratio': 0.6102000361609092, 'max_iter': 1590}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,625]\u001b[0m Trial 55 finished with value: 0.6264761904761904 and parameters: {'logreg_c': 0.8911994905571492, 'l1_ratio': 0.3993140649769618, 'max_iter': 1883}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,651]\u001b[0m Trial 56 finished with value: 0.6294285714285714 and parameters: {'logreg_c': 2.2506604255090674, 'l1_ratio': 0.5420213510321465, 'max_iter': 1719}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,678]\u001b[0m Trial 57 finished with value: 0.6191428571428571 and parameters: {'logreg_c': 19.197800367683012, 'l1_ratio': 0.4652438210429772, 'max_iter': 1762}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,705]\u001b[0m Trial 58 finished with value: 0.6195238095238096 and parameters: {'logreg_c': 0.17391612699617703, 'l1_ratio': 0.5464710550797393, 'max_iter': 1365}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,730]\u001b[0m Trial 59 finished with value: 0.613904761904762 and parameters: {'logreg_c': 6.575270743844423, 'l1_ratio': 0.5466415698694522, 'max_iter': 1686}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,758]\u001b[0m Trial 60 finished with value: 0.6163809523809525 and parameters: {'logreg_c': 54.54049186437329, 'l1_ratio': 0.4325278919043865, 'max_iter': 1996}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,784]\u001b[0m Trial 61 finished with value: 0.6307619047619047 and parameters: {'logreg_c': 2.096362271932597, 'l1_ratio': 0.5160078527964562, 'max_iter': 1556}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,811]\u001b[0m Trial 62 finished with value: 0.6266666666666666 and parameters: {'logreg_c': 2.538893199184801, 'l1_ratio': 0.504806699044337, 'max_iter': 1744}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,838]\u001b[0m Trial 63 finished with value: 0.6308571428571428 and parameters: {'logreg_c': 1.6491171164828378, 'l1_ratio': 0.37983614680640526, 'max_iter': 1831}. Best is trial 63 with value: 0.6308571428571428.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,865]\u001b[0m Trial 64 finished with value: 0.6207619047619048 and parameters: {'logreg_c': 0.5680946103743453, 'l1_ratio': 0.6124516385720995, 'max_iter': 1804}. Best is trial 63 with value: 0.6308571428571428.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,891]\u001b[0m Trial 65 finished with value: 0.6194285714285714 and parameters: {'logreg_c': 12.835933863876129, 'l1_ratio': 0.3307896654069705, 'max_iter': 1545}. Best is trial 63 with value: 0.6308571428571428.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,916]\u001b[0m Trial 66 finished with value: 0.6167619047619047 and parameters: {'logreg_c': 4.284809135627299, 'l1_ratio': 0.49198373373937326, 'max_iter': 1318}. Best is trial 63 with value: 0.6308571428571428.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,944]\u001b[0m Trial 67 finished with value: 0.6307619047619047 and parameters: {'logreg_c': 0.9489377554918126, 'l1_ratio': 0.37627684812340034, 'max_iter': 1225}. Best is trial 63 with value: 0.6308571428571428.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,968]\u001b[0m Trial 68 finished with value: 0.6292380952380953 and parameters: {'logreg_c': 1.0538708885678392, 'l1_ratio': 0.6781276320502876, 'max_iter': 1209}. Best is trial 63 with value: 0.6308571428571428.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:47,993]\u001b[0m Trial 69 finished with value: 0.6067619047619047 and parameters: {'logreg_c': 0.29155306833483985, 'l1_ratio': 0.36263117743566936, 'max_iter': 1660}. Best is trial 63 with value: 0.6308571428571428.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:48,018]\u001b[0m Trial 70 finished with value: 0.6251428571428571 and parameters: {'logreg_c': 0.8333203040302765, 'l1_ratio': 0.43197726798313846, 'max_iter': 1169}. Best is trial 63 with value: 0.6308571428571428.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:48,045]\u001b[0m Trial 71 finished with value: 0.6224761904761905 and parameters: {'logreg_c': 3.2825957984619203, 'l1_ratio': 0.52930545567372, 'max_iter': 1902}. Best is trial 63 with value: 0.6308571428571428.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:48,071]\u001b[0m Trial 72 finished with value: 0.6294285714285714 and parameters: {'logreg_c': 1.707563442346071, 'l1_ratio': 0.2692265008939825, 'max_iter': 1804}. Best is trial 63 with value: 0.6308571428571428.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:48,098]\u001b[0m Trial 73 finished with value: 0.627904761904762 and parameters: {'logreg_c': 1.244565542128993, 'l1_ratio': 0.22628313375302067, 'max_iter': 1283}. Best is trial 63 with value: 0.6308571428571428.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:48,124]\u001b[0m Trial 74 finished with value: 0.617904761904762 and parameters: {'logreg_c': 0.5423233219501331, 'l1_ratio': 0.37441966805804505, 'max_iter': 1429}. Best is trial 63 with value: 0.6308571428571428.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:48,150]\u001b[0m Trial 75 finished with value: 0.6238095238095238 and parameters: {'logreg_c': 2.6428530082211377, 'l1_ratio': 0.2853040906189891, 'max_iter': 1736}. Best is trial 63 with value: 0.6308571428571428.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:48,175]\u001b[0m Trial 76 finished with value: 0.6209523809523809 and parameters: {'logreg_c': 0.10939627711828923, 'l1_ratio': 0.4735312789321377, 'max_iter': 1846}. Best is trial 63 with value: 0.6308571428571428.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:48,201]\u001b[0m Trial 77 finished with value: 0.6264761904761904 and parameters: {'logreg_c': 0.8745518604877525, 'l1_ratio': 0.442590140651025, 'max_iter': 1078}. Best is trial 63 with value: 0.6308571428571428.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:48,227]\u001b[0m Trial 78 finished with value: 0.6154285714285713 and parameters: {'logreg_c': 5.330321809025692, 'l1_ratio': 0.4137370416803211, 'max_iter': 1943}. Best is trial 63 with value: 0.6308571428571428.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:48,254]\u001b[0m Trial 79 finished with value: 0.6321904761904762 and parameters: {'logreg_c': 1.5684558195868687, 'l1_ratio': 0.51063148634346, 'max_iter': 1863}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:48,280]\u001b[0m Trial 80 finished with value: 0.6194285714285716 and parameters: {'logreg_c': 0.37976816840820854, 'l1_ratio': 0.34025799616210806, 'max_iter': 1579}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:48,304]\u001b[0m Trial 81 finished with value: 0.6308571428571428 and parameters: {'logreg_c': 1.5835184104709785, 'l1_ratio': 0.5016015552438089, 'max_iter': 1871}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:48,327]\u001b[0m Trial 82 finished with value: 0.6308571428571428 and parameters: {'logreg_c': 1.6594874201724938, 'l1_ratio': 0.3063290111973975, 'max_iter': 1960}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:48,352]\u001b[0m Trial 83 finished with value: 0.6307619047619046 and parameters: {'logreg_c': 1.343979148614231, 'l1_ratio': 0.5176219420701993, 'max_iter': 1967}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:48,376]\u001b[0m Trial 84 finished with value: 0.6223809523809524 and parameters: {'logreg_c': 0.7187697779331849, 'l1_ratio': 0.5672396458148798, 'max_iter': 1939}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:48,402]\u001b[0m Trial 85 finished with value: 0.6293333333333334 and parameters: {'logreg_c': 1.2029830464414577, 'l1_ratio': 0.5185295899090988, 'max_iter': 1859}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:48,427]\u001b[0m Trial 86 finished with value: 0.6266666666666666 and parameters: {'logreg_c': 2.9061910777908393, 'l1_ratio': 0.3132992841051993, 'max_iter': 1949}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:48,450]\u001b[0m Trial 87 finished with value: 0.6096190476190476 and parameters: {'logreg_c': 0.2725593715364528, 'l1_ratio': 0.3860887328318272, 'max_iter': 1796}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:48,475]\u001b[0m Trial 88 finished with value: 0.6165714285714287 and parameters: {'logreg_c': 0.4912865251268892, 'l1_ratio': 0.5945013797755109, 'max_iter': 2000}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:48,499]\u001b[0m Trial 89 finished with value: 0.6153333333333333 and parameters: {'logreg_c': 8.240833064906228, 'l1_ratio': 0.5139563256443173, 'max_iter': 162}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:48,523]\u001b[0m Trial 90 finished with value: 0.6307619047619047 and parameters: {'logreg_c': 1.4731369609639198, 'l1_ratio': 0.4554672724433561, 'max_iter': 1883}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:48,546]\u001b[0m Trial 91 finished with value: 0.627904761904762 and parameters: {'logreg_c': 1.2362306827283187, 'l1_ratio': 0.45224194854848165, 'max_iter': 1869}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:48,572]\u001b[0m Trial 92 finished with value: 0.628 and parameters: {'logreg_c': 1.8218108234980488, 'l1_ratio': 0.47339867982647943, 'max_iter': 1823}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:48,595]\u001b[0m Trial 93 finished with value: 0.6167619047619047 and parameters: {'logreg_c': 4.271962308570505, 'l1_ratio': 0.5609556013046808, 'max_iter': 1914}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:48,618]\u001b[0m Trial 94 finished with value: 0.6307619047619047 and parameters: {'logreg_c': 0.9774627856036887, 'l1_ratio': 0.42302130014987804, 'max_iter': 1700}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:48,643]\u001b[0m Trial 95 finished with value: 0.6222857142857143 and parameters: {'logreg_c': 0.8082919920657609, 'l1_ratio': 0.41564559443826843, 'max_iter': 1765}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:48,667]\u001b[0m Trial 96 finished with value: 0.6308571428571428 and parameters: {'logreg_c': 1.576347619069216, 'l1_ratio': 0.37665937232889934, 'max_iter': 1705}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:48,691]\u001b[0m Trial 97 finished with value: 0.6252380952380953 and parameters: {'logreg_c': 3.119735317205072, 'l1_ratio': 0.3724348739191003, 'max_iter': 1701}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:48,715]\u001b[0m Trial 98 finished with value: 0.613904761904762 and parameters: {'logreg_c': 6.204154164274675, 'l1_ratio': 0.3480031298152046, 'max_iter': 1631}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:33:48,739]\u001b[0m Trial 99 finished with value: 0.6195238095238096 and parameters: {'logreg_c': 0.6240590821298014, 'l1_ratio': 0.39483273607916664, 'max_iter': 1765}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Tuning hyperparameters\n",
    "#Step 1. Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "    logreg_c = trial.suggest_float(\"logreg_c\", 1e-3,  1e3, log=True)\n",
    "    l1_ratio = trial.suggest_float(\"l1_ratio\",0.1,1,log=False) \n",
    "    #penalty = trial.suggest_categorical(\"penalty\",['l1','l2'])\n",
    "    max_iter = trial.suggest_int(\"max_iter\", 100,2000)\n",
    "    model =LogisticRegression(C=logreg_c,\n",
    "                              max_iter=max_iter,\n",
    "                              l1_ratio=l1_ratio,\n",
    "                              solver='liblinear',random_state=1)\n",
    "    \n",
    "# Step 2: Scoring method:\n",
    "    score = model_selection.cross_val_score(model, X, y, n_jobs=8, scoring=\"accuracy\",cv=Cv_optuna)\n",
    "    accuracy = score.mean()\n",
    "    return accuracy\n",
    "\n",
    "# Step 3: Running it\n",
    "study = optuna.create_study(direction=\"maximize\",sampler=optuna.samplers.TPESampler(seed=1))\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "36a76651-b007-4715-a58f-8776deab4b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are : \n",
      "{'logreg_c': 1.5684558195868687, 'l1_ratio': 0.51063148634346, 'max_iter': 1863}\n"
     ]
    }
   ],
   "source": [
    "# Getting the best parameters:\n",
    "print(f\"The best parameters are : \\n{study.best_params}\")\n",
    "# Setting the best model\n",
    "clf=LogisticRegression(C=study.best_params['logreg_c'],\n",
    "                              max_iter=study.best_params['max_iter'],\n",
    "                              l1_ratio=study.best_params['l1_ratio'],\n",
    "                              solver='liblinear',\n",
    "                              random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b3ae0d1b-badc-4865-802b-396e3ee1b6cf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.632190</td>\n",
       "      <td>0.014648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.735909</td>\n",
       "      <td>0.004541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.694496</td>\n",
       "      <td>0.013341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.627650</td>\n",
       "      <td>0.012121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.794643</td>\n",
       "      <td>0.021058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.673325</td>\n",
       "      <td>0.020325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean        Se\n",
       "Accuracy_test   0.632190  0.014648\n",
       "Accuracy_train  0.735909  0.004541\n",
       "F1 Score        0.694496  0.013341\n",
       "Precision       0.627650  0.012121\n",
       "Recall          0.794643  0.021058\n",
       "Roc_auc         0.673325  0.020325"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model2\n",
    "Model2=Model_results(clf,X,y,Cv_model)\n",
    "Model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a5007e9c-21fb-4007-bc2e-d5875dabe240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Method</th>\n",
       "      <th colspan=\"4\" halign=\"left\">LR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Model 1</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Model 2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Values</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.012329</td>\n",
       "      <td>0.632190</td>\n",
       "      <td>0.014648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.925345</td>\n",
       "      <td>0.005584</td>\n",
       "      <td>0.735909</td>\n",
       "      <td>0.004541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.706865</td>\n",
       "      <td>0.012092</td>\n",
       "      <td>0.694496</td>\n",
       "      <td>0.013341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.698240</td>\n",
       "      <td>0.013460</td>\n",
       "      <td>0.627650</td>\n",
       "      <td>0.012121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.735714</td>\n",
       "      <td>0.020076</td>\n",
       "      <td>0.794643</td>\n",
       "      <td>0.021058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.690561</td>\n",
       "      <td>0.015281</td>\n",
       "      <td>0.673325</td>\n",
       "      <td>0.020325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Method                LR                              \n",
       "Model            Model 1             Model 2          \n",
       "Values              Mean        Se      Mean        Se\n",
       "Accuracy_test   0.680000  0.012329  0.632190  0.014648\n",
       "Accuracy_train  0.925345  0.005584  0.735909  0.004541\n",
       "F1 Score        0.706865  0.012092  0.694496  0.013341\n",
       "Precision       0.698240  0.013460  0.627650  0.012121\n",
       "Recall          0.735714  0.020076  0.794643  0.021058\n",
       "Roc_auc         0.690561  0.015281  0.673325  0.020325"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saving the data of model performance\n",
    "Model_data=pd.concat([Model1,Model2],axis=1)\n",
    "Model_data.columns = [['LR']*4,['Model 1','Model 1', 'Model 2','Model 2'], ['Mean', 'Se', 'Mean', 'Se']]  \n",
    "Model_data.columns.names=['Method','Model','Values']\n",
    "Model_data.to_csv(\"./Results/LR_model_mlrem_data.csv\",sep=',')\n",
    "Model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892cd0c1-3914-4d08-a63a-550ffd5f60fb",
   "metadata": {},
   "source": [
    "## 2.3 RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "570a3fc5-5ce1-4bd8-a18a-b9209126a44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=RandomForestClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4448a0ec-3f59-45ad-be0b-3713b3c0072b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.676381</td>\n",
       "      <td>0.014691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.978164</td>\n",
       "      <td>0.001539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.711877</td>\n",
       "      <td>0.013423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.688878</td>\n",
       "      <td>0.014746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.757857</td>\n",
       "      <td>0.020561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.756424</td>\n",
       "      <td>0.017012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean        Se\n",
       "Accuracy_test   0.676381  0.014691\n",
       "Accuracy_train  0.978164  0.001539\n",
       "F1 Score        0.711877  0.013423\n",
       "Precision       0.688878  0.014746\n",
       "Recall          0.757857  0.020561\n",
       "Roc_auc         0.756424  0.017012"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model1\n",
    "Model1_clf=clf\n",
    "#Model1\n",
    "Model1=Model_results(Model1_clf,X,y,Cv_model)\n",
    "Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d0f9ce84-3537-484b-9862-bf7cefca94b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2024-01-12 01:35:16,075]\u001b[0m A new study created in memory with name: no-name-0e368382-34a5-42fd-91ba-b1b1a0a8e3bf\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:35:19,314]\u001b[0m Trial 0 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 594, 'max_depth': 16, 'max_features': 20, 'min_impurity_decrease': 2.724415914984484}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:35:21,908]\u001b[0m Trial 1 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 481, 'max_depth': 15, 'max_features': 16, 'min_impurity_decrease': 4.4588650039103985}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:35:27,120]\u001b[0m Trial 2 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 968, 'max_depth': 11, 'max_features': 25, 'min_impurity_decrease': 2.644474598764522}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:35:30,335]\u001b[0m Trial 3 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 611, 'max_depth': 19, 'max_features': 6, 'min_impurity_decrease': 0.43564649850770354}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:35:31,006]\u001b[0m Trial 4 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 118, 'max_depth': 18, 'max_features': 25, 'min_impurity_decrease': 4.3500607412340955}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:35:36,134]\u001b[0m Trial 5 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 981, 'max_depth': 17, 'max_features': 16, 'min_impurity_decrease': 3.902645881432277}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:35:37,261]\u001b[0m Trial 6 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 206, 'max_depth': 15, 'max_features': 8, 'min_impurity_decrease': 4.7233445852479194}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:35:40,255]\u001b[0m Trial 7 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 570, 'max_depth': 11, 'max_features': 11, 'min_impurity_decrease': 3.871168447171083}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:35:42,854]\u001b[0m Trial 8 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 510, 'max_depth': 14, 'max_features': 5, 'min_impurity_decrease': 3.0881774853793855}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:35:46,294]\u001b[0m Trial 9 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 651, 'max_depth': 14, 'max_features': 29, 'min_impurity_decrease': 3.409101495517417}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:35:50,414]\u001b[0m Trial 10 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 790, 'max_depth': 7, 'max_features': 21, 'min_impurity_decrease': 1.5046577569438309}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:35:52,562]\u001b[0m Trial 11 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 397, 'max_depth': 20, 'max_features': 16, 'min_impurity_decrease': 1.9887909510549393}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:35:54,439]\u001b[0m Trial 12 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 353, 'max_depth': 16, 'max_features': 20, 'min_impurity_decrease': 0.7029374955194359}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:35:56,713]\u001b[0m Trial 13 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 411, 'max_depth': 12, 'max_features': 12, 'min_impurity_decrease': 4.931175650908394}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:36:00,636]\u001b[0m Trial 14 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 743, 'max_depth': 7, 'max_features': 19, 'min_impurity_decrease': 1.5253121430442067}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:36:04,681]\u001b[0m Trial 15 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 757, 'max_depth': 13, 'max_features': 12, 'min_impurity_decrease': 2.668111844473928}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:36:06,269]\u001b[0m Trial 16 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 286, 'max_depth': 9, 'max_features': 24, 'min_impurity_decrease': 3.4814210246729473}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:36:10,648]\u001b[0m Trial 17 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 807, 'max_depth': 5, 'max_features': 19, 'min_impurity_decrease': 1.4307627258174422}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:36:14,484]\u001b[0m Trial 18 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 714, 'max_depth': 13, 'max_features': 12, 'min_impurity_decrease': 2.574069984147719}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:36:16,040]\u001b[0m Trial 19 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 291, 'max_depth': 8, 'max_features': 24, 'min_impurity_decrease': 3.3144569973486893}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:36:20,679]\u001b[0m Trial 20 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 871, 'max_depth': 5, 'max_features': 29, 'min_impurity_decrease': 1.0528796896599464}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:36:24,313]\u001b[0m Trial 21 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 670, 'max_depth': 10, 'max_features': 14, 'min_impurity_decrease': 2.1272690676619246}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:36:27,036]\u001b[0m Trial 22 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 502, 'max_depth': 9, 'max_features': 23, 'min_impurity_decrease': 3.0212243946132653}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:36:31,734]\u001b[0m Trial 23 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 871, 'max_depth': 6, 'max_features': 30, 'min_impurity_decrease': 0.8924229302747374}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:36:35,612]\u001b[0m Trial 24 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 673, 'max_depth': 11, 'max_features': 28, 'min_impurity_decrease': 2.183958261339378}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:36:41,146]\u001b[0m Trial 25 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 548, 'max_depth': 9, 'max_features': 22, 'min_impurity_decrease': 2.0762642939740097}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:36:51,062]\u001b[0m Trial 26 finished with value: 0.6323809523809524 and parameters: {'n_estimators': 859, 'max_depth': 7, 'max_features': 27, 'min_impurity_decrease': 0.05016303754931206}. Best is trial 26 with value: 0.6323809523809524.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:37:01,691]\u001b[0m Trial 27 finished with value: 0.672 and parameters: {'n_estimators': 889, 'max_depth': 6, 'max_features': 27, 'min_impurity_decrease': 0.008573433661244079}. Best is trial 27 with value: 0.672.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:37:11,727]\u001b[0m Trial 28 finished with value: 0.6322857142857143 and parameters: {'n_estimators': 901, 'max_depth': 7, 'max_features': 27, 'min_impurity_decrease': 0.04449057244021093}. Best is trial 27 with value: 0.672.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:37:22,093]\u001b[0m Trial 29 finished with value: 0.6254285714285714 and parameters: {'n_estimators': 918, 'max_depth': 7, 'max_features': 27, 'min_impurity_decrease': 0.06568686852538762}. Best is trial 27 with value: 0.672.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:37:30,555]\u001b[0m Trial 30 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 855, 'max_depth': 6, 'max_features': 26, 'min_impurity_decrease': 0.3179551086113602}. Best is trial 27 with value: 0.672.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:37:38,485]\u001b[0m Trial 31 finished with value: 0.597047619047619 and parameters: {'n_estimators': 937, 'max_depth': 7, 'max_features': 27, 'min_impurity_decrease': 0.18490386200494244}. Best is trial 27 with value: 0.672.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:37:46,299]\u001b[0m Trial 32 finished with value: 0.6747619047619048 and parameters: {'n_estimators': 913, 'max_depth': 8, 'max_features': 27, 'min_impurity_decrease': 0.004905793398417582}. Best is trial 32 with value: 0.6747619047619048.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:37:56,619]\u001b[0m Trial 33 finished with value: 0.6761904761904762 and parameters: {'n_estimators': 917, 'max_depth': 6, 'max_features': 30, 'min_impurity_decrease': 0.0020653183224317627}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:38:05,942]\u001b[0m Trial 34 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 989, 'max_depth': 5, 'max_features': 29, 'min_impurity_decrease': 0.6299600959488392}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:38:13,475]\u001b[0m Trial 35 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 815, 'max_depth': 8, 'max_features': 30, 'min_impurity_decrease': 0.4674586648441808}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:38:23,053]\u001b[0m Trial 36 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 961, 'max_depth': 6, 'max_features': 25, 'min_impurity_decrease': 1.1357603095945366}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:38:31,082]\u001b[0m Trial 37 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 840, 'max_depth': 8, 'max_features': 26, 'min_impurity_decrease': 0.4114799952729883}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:38:40,979]\u001b[0m Trial 38 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 998, 'max_depth': 10, 'max_features': 23, 'min_impurity_decrease': 0.7115028745023322}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:38:51,024]\u001b[0m Trial 39 finished with value: 0.6297142857142857 and parameters: {'n_estimators': 903, 'max_depth': 6, 'max_features': 28, 'min_impurity_decrease': 0.06208220513260173}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:38:58,675]\u001b[0m Trial 40 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 758, 'max_depth': 5, 'max_features': 25, 'min_impurity_decrease': 1.2530333907876288}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:39:08,515]\u001b[0m Trial 41 finished with value: 0.6322857142857143 and parameters: {'n_estimators': 907, 'max_depth': 8, 'max_features': 27, 'min_impurity_decrease': 0.05286677087361633}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:39:17,698]\u001b[0m Trial 42 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 942, 'max_depth': 8, 'max_features': 30, 'min_impurity_decrease': 0.5113387488083093}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:39:26,348]\u001b[0m Trial 43 finished with value: 0.5421904761904762 and parameters: {'n_estimators': 888, 'max_depth': 10, 'max_features': 28, 'min_impurity_decrease': 0.22488874368369016}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:39:34,856]\u001b[0m Trial 44 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 829, 'max_depth': 6, 'max_features': 26, 'min_impurity_decrease': 0.9284394996974525}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:39:45,663]\u001b[0m Trial 45 finished with value: 0.6718095238095237 and parameters: {'n_estimators': 948, 'max_depth': 9, 'max_features': 28, 'min_impurity_decrease': 0.0222031983719142}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:39:55,567]\u001b[0m Trial 46 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 949, 'max_depth': 9, 'max_features': 29, 'min_impurity_decrease': 0.3420653743587697}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:40:02,772]\u001b[0m Trial 47 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 785, 'max_depth': 7, 'max_features': 9, 'min_impurity_decrease': 0.6816047837540026}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:40:08,909]\u001b[0m Trial 48 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 622, 'max_depth': 6, 'max_features': 24, 'min_impurity_decrease': 0.8511767213202808}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:40:09,968]\u001b[0m Trial 49 finished with value: 0.5309523809523808 and parameters: {'n_estimators': 107, 'max_depth': 10, 'max_features': 21, 'min_impurity_decrease': 0.26320721389361473}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:40:17,265]\u001b[0m Trial 50 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 708, 'max_depth': 12, 'max_features': 28, 'min_impurity_decrease': 1.8082382735708258}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:40:28,042]\u001b[0m Trial 51 finished with value: 0.676095238095238 and parameters: {'n_estimators': 962, 'max_depth': 7, 'max_features': 27, 'min_impurity_decrease': 0.02106459317315383}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:40:37,749]\u001b[0m Trial 52 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 955, 'max_depth': 7, 'max_features': 26, 'min_impurity_decrease': 0.49937746742507}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:40:47,221]\u001b[0m Trial 53 finished with value: 0.5323809523809523 and parameters: {'n_estimators': 989, 'max_depth': 5, 'max_features': 30, 'min_impurity_decrease': 0.24933984959020736}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:40:57,354]\u001b[0m Trial 54 finished with value: 0.671904761904762 and parameters: {'n_estimators': 850, 'max_depth': 8, 'max_features': 29, 'min_impurity_decrease': 0.00020460322913867096}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:41:06,231]\u001b[0m Trial 55 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 926, 'max_depth': 8, 'max_features': 29, 'min_impurity_decrease': 0.5829495913183985}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:41:13,996]\u001b[0m Trial 56 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 807, 'max_depth': 9, 'max_features': 25, 'min_impurity_decrease': 4.063447563238093}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:41:24,402]\u001b[0m Trial 57 finished with value: 0.6761904761904762 and parameters: {'n_estimators': 880, 'max_depth': 11, 'max_features': 28, 'min_impurity_decrease': 0.007051219363487071}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:41:26,100]\u001b[0m Trial 58 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 167, 'max_depth': 11, 'max_features': 29, 'min_impurity_decrease': 0.798147610103549}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:41:33,758]\u001b[0m Trial 59 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 770, 'max_depth': 18, 'max_features': 30, 'min_impurity_decrease': 0.382341636539112}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:41:41,537]\u001b[0m Trial 60 finished with value: 0.5309523809523808 and parameters: {'n_estimators': 843, 'max_depth': 16, 'max_features': 17, 'min_impurity_decrease': 0.2654469511283595}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:41:52,032]\u001b[0m Trial 61 finished with value: 0.6776190476190476 and parameters: {'n_estimators': 879, 'max_depth': 9, 'max_features': 28, 'min_impurity_decrease': 0.008123194334826785}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:42:01,231]\u001b[0m Trial 62 finished with value: 0.5956190476190477 and parameters: {'n_estimators': 876, 'max_depth': 12, 'max_features': 28, 'min_impurity_decrease': 0.18049512848250882}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:42:09,726]\u001b[0m Trial 63 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 889, 'max_depth': 8, 'max_features': 26, 'min_impurity_decrease': 0.5086713670369812}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:42:16,982]\u001b[0m Trial 64 finished with value: 0.5983809523809525 and parameters: {'n_estimators': 736, 'max_depth': 14, 'max_features': 23, 'min_impurity_decrease': 0.17773926474715257}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:42:26,203]\u001b[0m Trial 65 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 969, 'max_depth': 10, 'max_features': 29, 'min_impurity_decrease': 0.33631758132430556}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:42:35,065]\u001b[0m Trial 66 finished with value: 0.6165714285714285 and parameters: {'n_estimators': 925, 'max_depth': 11, 'max_features': 27, 'min_impurity_decrease': 0.1425798717417805}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:42:44,003]\u001b[0m Trial 67 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 862, 'max_depth': 6, 'max_features': 24, 'min_impurity_decrease': 2.868193262665083}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:42:51,348]\u001b[0m Trial 68 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 797, 'max_depth': 9, 'max_features': 25, 'min_impurity_decrease': 0.994325851568861}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:43:00,977]\u001b[0m Trial 69 finished with value: 0.6535238095238095 and parameters: {'n_estimators': 828, 'max_depth': 7, 'max_features': 30, 'min_impurity_decrease': 0.031484934359399815}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:43:09,542]\u001b[0m Trial 70 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 974, 'max_depth': 8, 'max_features': 15, 'min_impurity_decrease': 0.634550934357082}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:43:19,004]\u001b[0m Trial 71 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 931, 'max_depth': 9, 'max_features': 28, 'min_impurity_decrease': 0.38649297888281287}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:43:29,212]\u001b[0m Trial 72 finished with value: 0.6577142857142857 and parameters: {'n_estimators': 899, 'max_depth': 9, 'max_features': 28, 'min_impurity_decrease': 0.030118637469662382}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:43:40,472]\u001b[0m Trial 73 finished with value: 0.6464761904761906 and parameters: {'n_estimators': 997, 'max_depth': 8, 'max_features': 27, 'min_impurity_decrease': 0.03301911564319314}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:43:49,058]\u001b[0m Trial 74 finished with value: 0.5832380952380952 and parameters: {'n_estimators': 869, 'max_depth': 7, 'max_features': 29, 'min_impurity_decrease': 0.19804900719497173}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:44:00,411]\u001b[0m Trial 75 finished with value: 0.6733333333333333 and parameters: {'n_estimators': 960, 'max_depth': 11, 'max_features': 27, 'min_impurity_decrease': 0.0033010724541531968}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:44:09,453]\u001b[0m Trial 76 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 913, 'max_depth': 11, 'max_features': 26, 'min_impurity_decrease': 0.42939915296424785}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:44:18,106]\u001b[0m Trial 77 finished with value: 0.5941904761904763 and parameters: {'n_estimators': 841, 'max_depth': 13, 'max_features': 27, 'min_impurity_decrease': 0.16676348958431253}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:44:22,894]\u001b[0m Trial 78 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 457, 'max_depth': 6, 'max_features': 29, 'min_impurity_decrease': 0.7933694921818996}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:44:31,918]\u001b[0m Trial 79 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 965, 'max_depth': 12, 'max_features': 26, 'min_impurity_decrease': 0.5862286818332793}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:44:40,745]\u001b[0m Trial 80 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 894, 'max_depth': 10, 'max_features': 30, 'min_impurity_decrease': 0.3144294030115981}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:44:51,308]\u001b[0m Trial 81 finished with value: 0.6733333333333335 and parameters: {'n_estimators': 934, 'max_depth': 10, 'max_features': 28, 'min_impurity_decrease': 0.006323452295319941}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:45:00,829]\u001b[0m Trial 82 finished with value: 0.6165714285714285 and parameters: {'n_estimators': 927, 'max_depth': 10, 'max_features': 28, 'min_impurity_decrease': 0.14545468932474404}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:45:10,329]\u001b[0m Trial 83 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 975, 'max_depth': 5, 'max_features': 27, 'min_impurity_decrease': 0.33087900745123566}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:45:19,087]\u001b[0m Trial 84 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 874, 'max_depth': 10, 'max_features': 28, 'min_impurity_decrease': 2.387601930957447}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:45:29,929]\u001b[0m Trial 85 finished with value: 0.6748571428571429 and parameters: {'n_estimators': 952, 'max_depth': 11, 'max_features': 25, 'min_impurity_decrease': 0.0007392099609671288}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:45:39,247]\u001b[0m Trial 86 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 947, 'max_depth': 11, 'max_features': 26, 'min_impurity_decrease': 0.48146683809049023}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:45:47,789]\u001b[0m Trial 87 finished with value: 0.6207619047619047 and parameters: {'n_estimators': 915, 'max_depth': 13, 'max_features': 24, 'min_impurity_decrease': 0.13757031221334323}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:45:58,181]\u001b[0m Trial 88 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 998, 'max_depth': 12, 'max_features': 25, 'min_impurity_decrease': 0.30676748165353357}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:46:07,313]\u001b[0m Trial 89 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 956, 'max_depth': 11, 'max_features': 27, 'min_impurity_decrease': 4.576798205075603}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:46:10,360]\u001b[0m Trial 90 finished with value: 0.620952380952381 and parameters: {'n_estimators': 289, 'max_depth': 12, 'max_features': 7, 'min_impurity_decrease': 0.1288307028354426}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:46:20,365]\u001b[0m Trial 91 finished with value: 0.6761904761904762 and parameters: {'n_estimators': 852, 'max_depth': 7, 'max_features': 29, 'min_impurity_decrease': 0.007725264887275658}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:46:29,221]\u001b[0m Trial 92 finished with value: 0.5295238095238095 and parameters: {'n_estimators': 889, 'max_depth': 11, 'max_features': 30, 'min_impurity_decrease': 0.2520599220602134}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:46:38,312]\u001b[0m Trial 93 finished with value: 0.6732380952380952 and parameters: {'n_estimators': 822, 'max_depth': 7, 'max_features': 29, 'min_impurity_decrease': 0.02259369678524981}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:46:42,912]\u001b[0m Trial 94 finished with value: 0.617904761904762 and parameters: {'n_estimators': 812, 'max_depth': 7, 'max_features': 29, 'min_impurity_decrease': 0.14831488771027054}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:46:48,995]\u001b[0m Trial 95 finished with value: 0.6761904761904762 and parameters: {'n_estimators': 940, 'max_depth': 7, 'max_features': 28, 'min_impurity_decrease': 0.005823654890190223}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:46:54,400]\u001b[0m Trial 96 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 938, 'max_depth': 9, 'max_features': 28, 'min_impurity_decrease': 0.4235083475944678}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:46:59,625]\u001b[0m Trial 97 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 978, 'max_depth': 10, 'max_features': 27, 'min_impurity_decrease': 0.5367328068166792}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:47:05,409]\u001b[0m Trial 98 finished with value: 0.6733333333333335 and parameters: {'n_estimators': 919, 'max_depth': 8, 'max_features': 30, 'min_impurity_decrease': 0.0005874942435908203}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2024-01-12 01:47:10,306]\u001b[0m Trial 99 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 909, 'max_depth': 8, 'max_features': 30, 'min_impurity_decrease': 0.719689906318086}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Tuning hyperparameters\n",
    "#Step 1. Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\",100,1000,1) \n",
    "    max_depth = trial.suggest_int(\"max_depth\",5,20,1)\n",
    "    max_features = trial.suggest_int(\"max_features\",5,30,1)\n",
    "    min_impurity_decrease = trial.suggest_float(\"min_impurity_decrease\",0,5,log=False) \n",
    "    model = RandomForestClassifier(n_estimators = n_estimators\n",
    "              ,max_depth = max_depth\n",
    "              ,max_features = max_features\n",
    "              ,min_impurity_decrease = min_impurity_decrease\n",
    "              ,random_state=1\n",
    "              ,verbose=False\n",
    "              ,n_jobs=8)\n",
    "\n",
    "# Step 2: Scoring method:\n",
    "    score = model_selection.cross_val_score(model, X, y, n_jobs=8, scoring=\"accuracy\",cv=Cv_optuna)\n",
    "    accuracy = score.mean()\n",
    "    return accuracy\n",
    " \n",
    "# Step 3: Running it\n",
    "study = optuna.create_study(direction=\"maximize\",sampler=optuna.samplers.TPESampler(seed=0))\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9b88d250-7567-4062-b7d5-0a9638d732c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are : \n",
      "{'n_estimators': 879, 'max_depth': 9, 'max_features': 28, 'min_impurity_decrease': 0.008123194334826785}\n"
     ]
    }
   ],
   "source": [
    "# Getting the best parameters:\n",
    "print(f\"The best parameters are : \\n{study.best_params}\")\n",
    "# Setting the best model\n",
    "clf=RandomForestClassifier(n_estimators = study.best_params['n_estimators']\n",
    "              ,max_depth = study.best_params['max_depth']\n",
    "              ,max_features = study.best_params['max_features']\n",
    "              ,min_impurity_decrease = study.best_params['min_impurity_decrease']\n",
    "              ,random_state=0\n",
    "              ,verbose=False\n",
    "              ,n_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a4281260-9ed3-4ed2-93c1-ed1888b7625f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.670571</td>\n",
       "      <td>0.013460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.974643</td>\n",
       "      <td>0.001874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.703054</td>\n",
       "      <td>0.013994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.681364</td>\n",
       "      <td>0.014131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.749643</td>\n",
       "      <td>0.022551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.752985</td>\n",
       "      <td>0.015938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean        Se\n",
       "Accuracy_test   0.670571  0.013460\n",
       "Accuracy_train  0.974643  0.001874\n",
       "F1 Score        0.703054  0.013994\n",
       "Precision       0.681364  0.014131\n",
       "Recall          0.749643  0.022551\n",
       "Roc_auc         0.752985  0.015938"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model2\n",
    "Model2=Model_results(clf,X,y,Cv_model)\n",
    "Model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c06b82a5-a8ce-4b80-b011-d93c0ab3a6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Method</th>\n",
       "      <th colspan=\"4\" halign=\"left\">RF</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Model 1</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Model 2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Values</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.676381</td>\n",
       "      <td>0.014691</td>\n",
       "      <td>0.670571</td>\n",
       "      <td>0.013460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.978164</td>\n",
       "      <td>0.001539</td>\n",
       "      <td>0.974643</td>\n",
       "      <td>0.001874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.711877</td>\n",
       "      <td>0.013423</td>\n",
       "      <td>0.703054</td>\n",
       "      <td>0.013994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.688878</td>\n",
       "      <td>0.014746</td>\n",
       "      <td>0.681364</td>\n",
       "      <td>0.014131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.757857</td>\n",
       "      <td>0.020561</td>\n",
       "      <td>0.749643</td>\n",
       "      <td>0.022551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.756424</td>\n",
       "      <td>0.017012</td>\n",
       "      <td>0.752985</td>\n",
       "      <td>0.015938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Method                RF                              \n",
       "Model            Model 1             Model 2          \n",
       "Values              Mean        Se      Mean        Se\n",
       "Accuracy_test   0.676381  0.014691  0.670571  0.013460\n",
       "Accuracy_train  0.978164  0.001539  0.974643  0.001874\n",
       "F1 Score        0.711877  0.013423  0.703054  0.013994\n",
       "Precision       0.688878  0.014746  0.681364  0.014131\n",
       "Recall          0.757857  0.020561  0.749643  0.022551\n",
       "Roc_auc         0.756424  0.017012  0.752985  0.015938"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saving the data of model performance\n",
    "Model_data=pd.concat([Model1,Model2],axis=1)\n",
    "Model_data.columns = [['RF']*4,['Model 1','Model 1', 'Model 2','Model 2'], ['Mean', 'Se', 'Mean', 'Se']]  \n",
    "Model_data.columns.names=['Method','Model','Values']\n",
    "Model_data.to_csv(\"./Results/RF_model_mlrem_data.csv\",sep=',')\n",
    "Model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de27d5ec-8039-4cfd-8629-ad30d87ca90f",
   "metadata": {},
   "source": [
    "## 2.4 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "19aa8297-4182-4dac-8857-008a9ad45f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=xgb.XGBClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "66336653-640b-407f-8c25-80c9e29b3651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.662476</td>\n",
       "      <td>0.016918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.978164</td>\n",
       "      <td>0.001539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.693483</td>\n",
       "      <td>0.017356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.671509</td>\n",
       "      <td>0.015551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.737143</td>\n",
       "      <td>0.025112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.749660</td>\n",
       "      <td>0.017686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean        Se\n",
       "Accuracy_test   0.662476  0.016918\n",
       "Accuracy_train  0.978164  0.001539\n",
       "F1 Score        0.693483  0.017356\n",
       "Precision       0.671509  0.015551\n",
       "Recall          0.737143  0.025112\n",
       "Roc_auc         0.749660  0.017686"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model1 \n",
    "Model1_clf=clf\n",
    "#Model1\n",
    "Model1=Model_results(Model1_clf,X,y,Cv_model)\n",
    "Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2184b863-0c03-4641-8e98-2b00c0fd5878",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2024-01-12 01:49:39,219]\u001b[0m A new study created in memory with name: no-name-9bfd5a88-8b2a-4996-844c-9f159bf96100\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:49:44,945]\u001b[0m Trial 0 finished with value: 0.6608571428571429 and parameters: {'lambda': 0.15676677195506075, 'alpha': 0.7257005721594281, 'colsample_bytree': 0.7, 'subsample': 0.7000000000000001, 'learning_rate': 0.0801, 'n_estimators': 664}. Best is trial 0 with value: 0.6608571428571429.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:49:47,925]\u001b[0m Trial 1 finished with value: 0.6323809523809524 and parameters: {'lambda': 0.0562793204741517, 'alpha': 3.6905577292137624, 'colsample_bytree': 1.0, 'subsample': 0.6000000000000001, 'learning_rate': 0.1551, 'n_estimators': 552}. Best is trial 0 with value: 0.6608571428571429.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:49:51,796]\u001b[0m Trial 2 finished with value: 0.5408571428571428 and parameters: {'lambda': 0.18714500686240676, 'alpha': 5.039489598671215, 'colsample_bytree': 0.3, 'subsample': 0.4, 'learning_rate': 0.0001, 'n_estimators': 841}. Best is trial 0 with value: 0.6608571428571429.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:49:56,159]\u001b[0m Trial 3 finished with value: 0.6392380952380953 and parameters: {'lambda': 1.2960656597279736, 'alpha': 3.0202896401586674, 'colsample_bytree': 1.0, 'subsample': 0.9, 'learning_rate': 0.0901, 'n_estimators': 792}. Best is trial 0 with value: 0.6608571428571429.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:49:59,349]\u001b[0m Trial 4 finished with value: 0.6553333333333334 and parameters: {'lambda': 0.0029723346443356552, 'alpha': 0.3628140404024381, 'colsample_bytree': 0.4, 'subsample': 1.0, 'learning_rate': 0.10010000000000001, 'n_estimators': 444}. Best is trial 0 with value: 0.6608571428571429.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:50:06,638]\u001b[0m Trial 5 finished with value: 0.638 and parameters: {'lambda': 0.011434638743472197, 'alpha': 1.2500712230836255, 'colsample_bytree': 0.6000000000000001, 'subsample': 0.7000000000000001, 'learning_rate': 0.0001, 'n_estimators': 637}. Best is trial 0 with value: 0.6608571428571429.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:50:10,699]\u001b[0m Trial 6 finished with value: 0.6510476190476191 and parameters: {'lambda': 0.2807908107885728, 'alpha': 0.2935864364395359, 'colsample_bytree': 1.0, 'subsample': 0.8, 'learning_rate': 0.07010000000000001, 'n_estimators': 465}. Best is trial 0 with value: 0.6608571428571429.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:50:12,762]\u001b[0m Trial 7 finished with value: 0.6508571428571429 and parameters: {'lambda': 0.6173405204074314, 'alpha': 0.0017414134181586202, 'colsample_bytree': 0.8, 'subsample': 0.8, 'learning_rate': 0.040100000000000004, 'n_estimators': 172}. Best is trial 0 with value: 0.6608571428571429.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:50:13,989]\u001b[0m Trial 8 finished with value: 0.6779047619047618 and parameters: {'lambda': 0.01826894228153233, 'alpha': 0.028499883436971588, 'colsample_bytree': 0.7, 'subsample': 0.7000000000000001, 'learning_rate': 0.1951, 'n_estimators': 147}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:50:16,120]\u001b[0m Trial 9 finished with value: 0.667904761904762 and parameters: {'lambda': 0.006847105576684045, 'alpha': 0.004418125737902547, 'colsample_bytree': 0.8, 'subsample': 0.5, 'learning_rate': 0.0901, 'n_estimators': 282}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:50:16,755]\u001b[0m Trial 10 finished with value: 0.6451428571428572 and parameters: {'lambda': 5.790132527437195, 'alpha': 0.025043968115100592, 'colsample_bytree': 0.5, 'subsample': 0.5, 'learning_rate': 0.1901, 'n_estimators': 57}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:50:18,686]\u001b[0m Trial 11 finished with value: 0.6690476190476192 and parameters: {'lambda': 0.017123553109627314, 'alpha': 0.013676263870483537, 'colsample_bytree': 0.8, 'subsample': 0.5, 'learning_rate': 0.14509999999999998, 'n_estimators': 277}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:50:20,734]\u001b[0m Trial 12 finished with value: 0.6763809523809523 and parameters: {'lambda': 0.02491353701899208, 'alpha': 0.023063141329483616, 'colsample_bytree': 0.8, 'subsample': 0.4, 'learning_rate': 0.15009999999999998, 'n_estimators': 319}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:50:22,688]\u001b[0m Trial 13 finished with value: 0.662 and parameters: {'lambda': 0.04474111800996658, 'alpha': 0.038990832725213885, 'colsample_bytree': 0.6000000000000001, 'subsample': 0.4, 'learning_rate': 0.1951, 'n_estimators': 316}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:50:23,622]\u001b[0m Trial 14 finished with value: 0.6509523809523808 and parameters: {'lambda': 0.0012140452982167488, 'alpha': 0.06910620324453418, 'colsample_bytree': 0.7, 'subsample': 0.6000000000000001, 'learning_rate': 0.1551, 'n_estimators': 94}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:50:25,465]\u001b[0m Trial 15 finished with value: 0.6680000000000001 and parameters: {'lambda': 0.027126643489253296, 'alpha': 0.0045017087887461935, 'colsample_bytree': 0.9000000000000001, 'subsample': 1.0, 'learning_rate': 0.1301, 'n_estimators': 204}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:50:27,896]\u001b[0m Trial 16 finished with value: 0.6703809523809525 and parameters: {'lambda': 0.004818440651909064, 'alpha': 0.16888877355169551, 'colsample_bytree': 0.5, 'subsample': 0.6000000000000001, 'learning_rate': 0.1751, 'n_estimators': 358}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:50:29,534]\u001b[0m Trial 17 finished with value: 0.6649523809523811 and parameters: {'lambda': 0.0010007385532741818, 'alpha': 0.009646273191219181, 'colsample_bytree': 0.9000000000000001, 'subsample': 0.8, 'learning_rate': 0.1201, 'n_estimators': 181}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:50:31,793]\u001b[0m Trial 18 finished with value: 0.6763809523809524 and parameters: {'lambda': 0.061634227943790754, 'alpha': 0.07305295568841107, 'colsample_bytree': 0.7, 'subsample': 0.4, 'learning_rate': 0.1701, 'n_estimators': 367}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:50:36,908]\u001b[0m Trial 19 finished with value: 0.6748571428571429 and parameters: {'lambda': 0.08189216606299816, 'alpha': 0.09137793328553549, 'colsample_bytree': 0.5, 'subsample': 0.6000000000000001, 'learning_rate': 0.1751, 'n_estimators': 930}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:50:40,298]\u001b[0m Trial 20 finished with value: 0.6764761904761906 and parameters: {'lambda': 1.6767154928982846, 'alpha': 0.0016483661900255071, 'colsample_bytree': 0.7, 'subsample': 0.9, 'learning_rate': 0.1751, 'n_estimators': 429}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:50:44,316]\u001b[0m Trial 21 finished with value: 0.6525714285714286 and parameters: {'lambda': 7.401604059812908, 'alpha': 0.0010968075467978052, 'colsample_bytree': 0.7, 'subsample': 0.9, 'learning_rate': 0.18009999999999998, 'n_estimators': 421}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:50:48,618]\u001b[0m Trial 22 finished with value: 0.6694285714285715 and parameters: {'lambda': 2.0928422583512405, 'alpha': 0.0036652235601470915, 'colsample_bytree': 0.6000000000000001, 'subsample': 0.9, 'learning_rate': 0.1701, 'n_estimators': 558}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:50:51,373]\u001b[0m Trial 23 finished with value: 0.6751428571428572 and parameters: {'lambda': 0.6439279076937869, 'alpha': 0.04386602055339343, 'colsample_bytree': 0.7, 'subsample': 0.7000000000000001, 'learning_rate': 0.1951, 'n_estimators': 391}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:50:54,900]\u001b[0m Trial 24 finished with value: 0.6764761904761905 and parameters: {'lambda': 0.3883136303907193, 'alpha': 0.010049953917114773, 'colsample_bytree': 0.6000000000000001, 'subsample': 0.8, 'learning_rate': 0.1301, 'n_estimators': 496}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:50:59,980]\u001b[0m Trial 25 finished with value: 0.6653333333333333 and parameters: {'lambda': 2.7802503139996473, 'alpha': 0.008337657822509066, 'colsample_bytree': 0.6000000000000001, 'subsample': 0.8, 'learning_rate': 0.1301, 'n_estimators': 656}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:51:05,250]\u001b[0m Trial 26 finished with value: 0.6790476190476192 and parameters: {'lambda': 0.5989769734564016, 'alpha': 0.001754204354028918, 'colsample_bytree': 0.4, 'subsample': 0.9, 'learning_rate': 0.11510000000000001, 'n_estimators': 725}. Best is trial 26 with value: 0.6790476190476192.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:51:12,244]\u001b[0m Trial 27 finished with value: 0.6693333333333336 and parameters: {'lambda': 1.2518838419016274, 'alpha': 0.002084110757581769, 'colsample_bytree': 0.4, 'subsample': 0.9, 'learning_rate': 0.0451, 'n_estimators': 779}. Best is trial 26 with value: 0.6790476190476192.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:51:21,465]\u001b[0m Trial 28 finished with value: 0.6609523809523811 and parameters: {'lambda': 4.723187635059192, 'alpha': 0.002467585771750723, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.050100000000000006, 'n_estimators': 916}. Best is trial 26 with value: 0.6790476190476192.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:51:26,158]\u001b[0m Trial 29 finished with value: 0.6861904761904764 and parameters: {'lambda': 0.1357530766063751, 'alpha': 0.00604912334356112, 'colsample_bytree': 0.4, 'subsample': 0.7000000000000001, 'learning_rate': 0.11510000000000001, 'n_estimators': 740}. Best is trial 29 with value: 0.6861904761904764.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:51:30,752]\u001b[0m Trial 30 finished with value: 0.6750476190476192 and parameters: {'lambda': 0.18092199214591256, 'alpha': 0.0010057216538091605, 'colsample_bytree': 0.4, 'subsample': 0.7000000000000001, 'learning_rate': 0.1101, 'n_estimators': 709}. Best is trial 29 with value: 0.6861904761904764.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:51:35,311]\u001b[0m Trial 31 finished with value: 0.6751428571428572 and parameters: {'lambda': 0.11119277519798368, 'alpha': 0.006118149756823851, 'colsample_bytree': 0.5, 'subsample': 0.7000000000000001, 'learning_rate': 0.11510000000000001, 'n_estimators': 723}. Best is trial 29 with value: 0.6861904761904764.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:51:40,257]\u001b[0m Trial 32 finished with value: 0.6821904761904765 and parameters: {'lambda': 0.6301118583785598, 'alpha': 0.01900920382279658, 'colsample_bytree': 0.4, 'subsample': 0.9, 'learning_rate': 0.0651, 'n_estimators': 591}. Best is trial 29 with value: 0.6861904761904764.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:51:45,058]\u001b[0m Trial 33 finished with value: 0.6763809523809524 and parameters: {'lambda': 0.6999320996994477, 'alpha': 0.016156842125445648, 'colsample_bytree': 0.3, 'subsample': 0.7000000000000001, 'learning_rate': 0.0601, 'n_estimators': 588}. Best is trial 29 with value: 0.6861904761904764.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:51:52,041]\u001b[0m Trial 34 finished with value: 0.674952380952381 and parameters: {'lambda': 0.3527627967271589, 'alpha': 0.030241715405447074, 'colsample_bytree': 0.4, 'subsample': 0.8, 'learning_rate': 0.0751, 'n_estimators': 991}. Best is trial 29 with value: 0.6861904761904764.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:51:59,408]\u001b[0m Trial 35 finished with value: 0.6609523809523811 and parameters: {'lambda': 0.14364941444825624, 'alpha': 0.017627889213148507, 'colsample_bytree': 0.3, 'subsample': 0.6000000000000001, 'learning_rate': 0.0201, 'n_estimators': 836}. Best is trial 29 with value: 0.6861904761904764.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:52:04,321]\u001b[0m Trial 36 finished with value: 0.6680000000000001 and parameters: {'lambda': 1.0360787933743876, 'alpha': 0.006610100039382611, 'colsample_bytree': 0.4, 'subsample': 1.0, 'learning_rate': 0.10010000000000001, 'n_estimators': 590}. Best is trial 29 with value: 0.6861904761904764.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:52:07,614]\u001b[0m Trial 37 finished with value: 0.5927619047619048 and parameters: {'lambda': 0.24008342239429287, 'alpha': 8.861629685772453, 'colsample_bytree': 0.5, 'subsample': 0.7000000000000001, 'learning_rate': 0.08510000000000001, 'n_estimators': 730}. Best is trial 29 with value: 0.6861904761904764.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:52:14,227]\u001b[0m Trial 38 finished with value: 0.6707619047619049 and parameters: {'lambda': 0.037769804089962805, 'alpha': 0.17532934883981124, 'colsample_bytree': 0.4, 'subsample': 0.9, 'learning_rate': 0.0201, 'n_estimators': 622}. Best is trial 29 with value: 0.6861904761904764.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:52:17,939]\u001b[0m Trial 39 finished with value: 0.677904761904762 and parameters: {'lambda': 0.08654988469845346, 'alpha': 0.0033305961239733462, 'colsample_bytree': 0.3, 'subsample': 0.8, 'learning_rate': 0.1401, 'n_estimators': 535}. Best is trial 29 with value: 0.6861904761904764.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:52:22,157]\u001b[0m Trial 40 finished with value: 0.667904761904762 and parameters: {'lambda': 0.4364559577785702, 'alpha': 0.002986127875870649, 'colsample_bytree': 0.3, 'subsample': 0.8, 'learning_rate': 0.1051, 'n_estimators': 529}. Best is trial 29 with value: 0.6861904761904764.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:52:26,780]\u001b[0m Trial 41 finished with value: 0.6820952380952382 and parameters: {'lambda': 0.012041246302698917, 'alpha': 0.005755556753698326, 'colsample_bytree': 0.3, 'subsample': 0.9, 'learning_rate': 0.1401, 'n_estimators': 679}. Best is trial 29 with value: 0.6861904761904764.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:52:30,094]\u001b[0m Trial 42 finished with value: 0.6821904761904761 and parameters: {'lambda': 0.00976530253589791, 'alpha': 0.004521658633426094, 'colsample_bytree': 0.3, 'subsample': 0.9, 'learning_rate': 0.1401, 'n_estimators': 692}. Best is trial 29 with value: 0.6861904761904764.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:52:31,968]\u001b[0m Trial 43 finished with value: 0.6820952380952382 and parameters: {'lambda': 0.009533938648370993, 'alpha': 0.005142380479852608, 'colsample_bytree': 0.4, 'subsample': 0.9, 'learning_rate': 0.1201, 'n_estimators': 779}. Best is trial 29 with value: 0.6861904761904764.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:52:34,015]\u001b[0m Trial 44 finished with value: 0.701714285714286 and parameters: {'lambda': 0.008556178294461857, 'alpha': 0.00559859295117001, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1601, 'n_estimators': 792}. Best is trial 44 with value: 0.701714285714286.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:52:35,874]\u001b[0m Trial 45 finished with value: 0.7003809523809525 and parameters: {'lambda': 0.002736395942107596, 'alpha': 0.010967284413723879, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1601, 'n_estimators': 687}. Best is trial 44 with value: 0.701714285714286.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:52:38,163]\u001b[0m Trial 46 finished with value: 0.6960952380952382 and parameters: {'lambda': 0.0028166231869091456, 'alpha': 0.009994188044722196, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1651, 'n_estimators': 830}. Best is trial 44 with value: 0.701714285714286.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:52:40,457]\u001b[0m Trial 47 finished with value: 0.6947619047619049 and parameters: {'lambda': 0.0019027548784550043, 'alpha': 0.012393652417147363, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.0651, 'n_estimators': 836}. Best is trial 44 with value: 0.701714285714286.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:52:42,704]\u001b[0m Trial 48 finished with value: 0.6960952380952381 and parameters: {'lambda': 0.0024183815673537484, 'alpha': 0.011729643522042612, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1851, 'n_estimators': 846}. Best is trial 44 with value: 0.701714285714286.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:52:44,291]\u001b[0m Trial 49 finished with value: 0.6551428571428572 and parameters: {'lambda': 0.0019408046467350051, 'alpha': 1.0433113847404174, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1851, 'n_estimators': 854}. Best is trial 44 with value: 0.701714285714286.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:52:46,542]\u001b[0m Trial 50 finished with value: 0.7002857142857144 and parameters: {'lambda': 0.0033591891295767654, 'alpha': 0.01382541664678391, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1601, 'n_estimators': 874}. Best is trial 44 with value: 0.701714285714286.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:52:48,705]\u001b[0m Trial 51 finished with value: 0.7017142857142857 and parameters: {'lambda': 0.0034229051056133657, 'alpha': 0.012871271791450633, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1601, 'n_estimators': 866}. Best is trial 44 with value: 0.701714285714286.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:52:51,053]\u001b[0m Trial 52 finished with value: 0.7001904761904763 and parameters: {'lambda': 0.004225220871455537, 'alpha': 0.011370294688030805, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1601, 'n_estimators': 890}. Best is trial 44 with value: 0.701714285714286.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:52:53,527]\u001b[0m Trial 53 finished with value: 0.6960000000000002 and parameters: {'lambda': 0.004103597114144333, 'alpha': 0.04967022533268807, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1601, 'n_estimators': 920}. Best is trial 44 with value: 0.701714285714286.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:52:55,907]\u001b[0m Trial 54 finished with value: 0.7031428571428572 and parameters: {'lambda': 0.004043834319918063, 'alpha': 0.029633425764158693, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1601, 'n_estimators': 894}. Best is trial 54 with value: 0.7031428571428572.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:52:58,610]\u001b[0m Trial 55 finished with value: 0.6975238095238098 and parameters: {'lambda': 0.004980224075064425, 'alpha': 0.03510787695470061, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1551, 'n_estimators': 995}. Best is trial 54 with value: 0.7031428571428572.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:53:00,935]\u001b[0m Trial 56 finished with value: 0.7102857142857144 and parameters: {'lambda': 0.0013221304985698086, 'alpha': 0.02305461805888264, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.15009999999999998, 'n_estimators': 884}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:53:03,490]\u001b[0m Trial 57 finished with value: 0.6793333333333333 and parameters: {'lambda': 0.0012446922418356734, 'alpha': 0.022077479688830965, 'colsample_bytree': 0.4, 'subsample': 1.0, 'learning_rate': 0.15009999999999998, 'n_estimators': 962}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:53:05,883]\u001b[0m Trial 58 finished with value: 0.6791428571428573 and parameters: {'lambda': 0.006622004577595627, 'alpha': 0.1431496104944084, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1651, 'n_estimators': 885}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:53:08,000]\u001b[0m Trial 59 finished with value: 0.6751428571428572 and parameters: {'lambda': 0.0014168536815106854, 'alpha': 0.02621704841078774, 'colsample_bytree': 0.5, 'subsample': 1.0, 'learning_rate': 0.15009999999999998, 'n_estimators': 803}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:53:10,527]\u001b[0m Trial 60 finished with value: 0.677904761904762 and parameters: {'lambda': 0.003453061915516729, 'alpha': 0.06591221885128822, 'colsample_bytree': 0.4, 'subsample': 1.0, 'learning_rate': 0.1701, 'n_estimators': 955}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:53:12,750]\u001b[0m Trial 61 finished with value: 0.6932380952380953 and parameters: {'lambda': 0.006241233717411143, 'alpha': 0.008377211507259008, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1601, 'n_estimators': 885}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:53:15,123]\u001b[0m Trial 62 finished with value: 0.7001904761904763 and parameters: {'lambda': 0.00199673319822298, 'alpha': 0.01284143905578283, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1601, 'n_estimators': 875}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:53:17,298]\u001b[0m Trial 63 finished with value: 0.6935238095238097 and parameters: {'lambda': 0.0018104838952691495, 'alpha': 0.014581416070468362, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.18009999999999998, 'n_estimators': 804}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:53:19,428]\u001b[0m Trial 64 finished with value: 0.7004761904761907 and parameters: {'lambda': 0.0024756254478756406, 'alpha': 0.052568127938742146, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.14509999999999998, 'n_estimators': 764}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:53:21,329]\u001b[0m Trial 65 finished with value: 0.6792380952380952 and parameters: {'lambda': 0.0028689485895962183, 'alpha': 0.04629073394230276, 'colsample_bytree': 0.9000000000000001, 'subsample': 0.9, 'learning_rate': 0.1351, 'n_estimators': 765}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:53:22,970]\u001b[0m Trial 66 finished with value: 0.6595238095238096 and parameters: {'lambda': 0.001014578900194637, 'alpha': 0.36043897298369987, 'colsample_bytree': 0.4, 'subsample': 1.0, 'learning_rate': 0.14509999999999998, 'n_estimators': 761}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:53:25,064]\u001b[0m Trial 67 finished with value: 0.6751428571428572 and parameters: {'lambda': 0.01737522874540943, 'alpha': 0.05780931059819181, 'colsample_bytree': 1.0, 'subsample': 0.9, 'learning_rate': 0.15009999999999998, 'n_estimators': 815}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:53:27,445]\u001b[0m Trial 68 finished with value: 0.6863809523809524 and parameters: {'lambda': 0.0015222664201042758, 'alpha': 0.11020953113272494, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1301, 'n_estimators': 936}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:53:29,694]\u001b[0m Trial 69 finished with value: 0.6778095238095238 and parameters: {'lambda': 0.003426341240967464, 'alpha': 0.03256146187092011, 'colsample_bytree': 0.4, 'subsample': 1.0, 'learning_rate': 0.1751, 'n_estimators': 866}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:53:31,832]\u001b[0m Trial 70 finished with value: 0.6820952380952382 and parameters: {'lambda': 0.005384501751833538, 'alpha': 0.022922639033061288, 'colsample_bytree': 0.4, 'subsample': 0.9, 'learning_rate': 0.14509999999999998, 'n_estimators': 905}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:53:34,361]\u001b[0m Trial 71 finished with value: 0.6932380952380954 and parameters: {'lambda': 0.0038325905699101567, 'alpha': 0.02051598231771764, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1651, 'n_estimators': 965}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:53:36,703]\u001b[0m Trial 72 finished with value: 0.7002857142857144 and parameters: {'lambda': 0.0024577069483295065, 'alpha': 0.007109582478627625, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1551, 'n_estimators': 896}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:53:38,713]\u001b[0m Trial 73 finished with value: 0.6973333333333335 and parameters: {'lambda': 0.0024282570038449054, 'alpha': 0.08294927790166803, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1551, 'n_estimators': 752}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:53:40,765]\u001b[0m Trial 74 finished with value: 0.6973333333333335 and parameters: {'lambda': 0.008296731719590096, 'alpha': 0.0073542298983549645, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1701, 'n_estimators': 787}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:53:42,888]\u001b[0m Trial 75 finished with value: 0.678952380952381 and parameters: {'lambda': 0.013459544298683093, 'alpha': 0.01606555197132388, 'colsample_bytree': 0.3, 'subsample': 0.5, 'learning_rate': 0.1251, 'n_estimators': 938}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:53:45,243]\u001b[0m Trial 76 finished with value: 0.6835238095238096 and parameters: {'lambda': 0.002432929541319386, 'alpha': 0.03867844669314933, 'colsample_bytree': 0.4, 'subsample': 1.0, 'learning_rate': 0.1551, 'n_estimators': 905}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:53:46,786]\u001b[0m Trial 77 finished with value: 0.6908571428571428 and parameters: {'lambda': 0.0014775448337385007, 'alpha': 0.008379965030471833, 'colsample_bytree': 0.3, 'subsample': 0.9, 'learning_rate': 0.1901, 'n_estimators': 647}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:53:48,713]\u001b[0m Trial 78 finished with value: 0.6807619047619049 and parameters: {'lambda': 0.006701372418938602, 'alpha': 0.004045620666985065, 'colsample_bytree': 0.4, 'subsample': 0.9, 'learning_rate': 0.14509999999999998, 'n_estimators': 820}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:53:50,849]\u001b[0m Trial 79 finished with value: 0.6876190476190479 and parameters: {'lambda': 0.003127746609011728, 'alpha': 0.025715495289677936, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.18009999999999998, 'n_estimators': 858}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:53:52,896]\u001b[0m Trial 80 finished with value: 0.6960952380952383 and parameters: {'lambda': 0.0011720575701298413, 'alpha': 0.016807260672059614, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1651, 'n_estimators': 797}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:53:55,044]\u001b[0m Trial 81 finished with value: 0.7044761904761907 and parameters: {'lambda': 0.0020203854446397642, 'alpha': 0.013357193776383159, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1601, 'n_estimators': 877}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:53:57,400]\u001b[0m Trial 82 finished with value: 0.698857142857143 and parameters: {'lambda': 0.005019268124724571, 'alpha': 0.009246692653937827, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1701, 'n_estimators': 903}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:53:59,710]\u001b[0m Trial 83 finished with value: 0.6962857142857143 and parameters: {'lambda': 0.002471192899337615, 'alpha': 0.014195787739054177, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1351, 'n_estimators': 863}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:54:01,031]\u001b[0m Trial 84 finished with value: 0.6265714285714286 and parameters: {'lambda': 0.0020518609785551635, 'alpha': 2.3474089145676693, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1551, 'n_estimators': 709}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:54:03,502]\u001b[0m Trial 85 finished with value: 0.6877142857142857 and parameters: {'lambda': 0.0016715687766010845, 'alpha': 0.007252154293450322, 'colsample_bytree': 0.4, 'subsample': 1.0, 'learning_rate': 0.1401, 'n_estimators': 973}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:54:05,680]\u001b[0m Trial 86 finished with value: 0.6807619047619048 and parameters: {'lambda': 0.004042017054172613, 'alpha': 0.019549267963618795, 'colsample_bytree': 0.3, 'subsample': 0.9, 'learning_rate': 0.15009999999999998, 'n_estimators': 938}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:54:07,852]\u001b[0m Trial 87 finished with value: 0.7031428571428572 and parameters: {'lambda': 0.023473634497896897, 'alpha': 0.005636601005747783, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1601, 'n_estimators': 848}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:54:10,073]\u001b[0m Trial 88 finished with value: 0.6862857142857144 and parameters: {'lambda': 0.03357853084274716, 'alpha': 0.004764047018815245, 'colsample_bytree': 0.4, 'subsample': 1.0, 'learning_rate': 0.1751, 'n_estimators': 843}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:54:12,340]\u001b[0m Trial 89 finished with value: 0.7031428571428572 and parameters: {'lambda': 0.00851847070190272, 'alpha': 0.010913691529586302, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1651, 'n_estimators': 816}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:54:14,158]\u001b[0m Trial 90 finished with value: 0.6907619047619049 and parameters: {'lambda': 0.026298523095546197, 'alpha': 0.002337637077580732, 'colsample_bytree': 0.3, 'subsample': 0.9, 'learning_rate': 0.18009999999999998, 'n_estimators': 767}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:54:16,189]\u001b[0m Trial 91 finished with value: 0.6889523809523811 and parameters: {'lambda': 0.007679646889123362, 'alpha': 0.003095610150491639, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1651, 'n_estimators': 817}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:54:18,346]\u001b[0m Trial 92 finished with value: 0.698857142857143 and parameters: {'lambda': 0.019627147648464925, 'alpha': 0.010308846754474606, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.14509999999999998, 'n_estimators': 789}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:54:20,399]\u001b[0m Trial 93 finished with value: 0.6989523809523811 and parameters: {'lambda': 0.012104952676441646, 'alpha': 0.03039331547668624, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1601, 'n_estimators': 742}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:54:21,764]\u001b[0m Trial 94 finished with value: 0.7060000000000001 and parameters: {'lambda': 0.005758744911796952, 'alpha': 0.00592458636159379, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1551, 'n_estimators': 489}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:54:23,361]\u001b[0m Trial 95 finished with value: 0.7003809523809525 and parameters: {'lambda': 0.05944768953566761, 'alpha': 0.00537721962184014, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.15009999999999998, 'n_estimators': 559}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:54:24,709]\u001b[0m Trial 96 finished with value: 0.6947619047619048 and parameters: {'lambda': 0.04615810486191638, 'alpha': 0.005041871944751029, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.15009999999999998, 'n_estimators': 482}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:54:25,993]\u001b[0m Trial 97 finished with value: 0.6821904761904762 and parameters: {'lambda': 0.009891164357898056, 'alpha': 0.0036417886687594215, 'colsample_bytree': 0.4, 'subsample': 1.0, 'learning_rate': 0.1701, 'n_estimators': 462}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:54:28,093]\u001b[0m Trial 98 finished with value: 0.6793333333333336 and parameters: {'lambda': 0.07346609827384647, 'alpha': 0.0017648041937854684, 'colsample_bytree': 0.3, 'subsample': 0.9, 'learning_rate': 0.1351, 'n_estimators': 557}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_30036\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-01-12 01:54:32,881]\u001b[0m Trial 99 finished with value: 0.6862857142857142 and parameters: {'lambda': 0.005843712147701568, 'alpha': 0.010227730927162207, 'colsample_bytree': 0.4, 'subsample': 1.0, 'learning_rate': 0.1651, 'n_estimators': 681}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Tuning hyperparameters\n",
    "#Step 1. Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
    "        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3,1.0,step=0.1),\n",
    "        'subsample': trial.suggest_float('subsample', 0.4, 1.0, step=0.1),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.2, step=0.005),\n",
    "        'n_estimators': trial.suggest_int(\"n_estimators\",50,1000,1)\n",
    "        #'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n",
    "    }\n",
    "    model = xgb.XGBClassifier(**param,random_state=1,n_jobs=8)\n",
    "\n",
    " \n",
    "# Step 2: Scoring method:\n",
    "    score = model_selection.cross_val_score(model, X, y, n_jobs=8, scoring=\"accuracy\",cv=Cv_optuna)\n",
    "    accuracy = score.mean()\n",
    "    return accuracy\n",
    "\n",
    "# Step 3: Running it\n",
    "study = optuna.create_study(direction=\"maximize\",sampler=optuna.samplers.TPESampler(seed=0))\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "61fd4c6f-d6af-4f71-8f40-73e5cb7f1a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are : \n",
      "{'lambda': 0.0013221304985698086, 'alpha': 0.02305461805888264, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.15009999999999998, 'n_estimators': 884}\n"
     ]
    }
   ],
   "source": [
    "# Getting the best parameters:\n",
    "print(f\"The best parameters are : \\n{study.best_params}\")\n",
    "# Setting the best model\n",
    "clf=xgb.XGBClassifier(alpha = study.best_params['alpha']\n",
    "              ,colsample_bytree = study.best_params['colsample_bytree']\n",
    "              ,subsample = study.best_params['subsample']\n",
    "              ,n_estimators = study.best_params['n_estimators']\n",
    "              ,learning_rate= study.best_params['learning_rate'], n_jobs=8\n",
    "              ,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4b5d7bd3-660b-42d3-9546-2633863cc8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.682000</td>\n",
       "      <td>0.015467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.978164</td>\n",
       "      <td>0.001539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.710276</td>\n",
       "      <td>0.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.693874</td>\n",
       "      <td>0.013624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.748214</td>\n",
       "      <td>0.023279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.752117</td>\n",
       "      <td>0.018448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean        Se\n",
       "Accuracy_test   0.682000  0.015467\n",
       "Accuracy_train  0.978164  0.001539\n",
       "F1 Score        0.710276  0.015000\n",
       "Precision       0.693874  0.013624\n",
       "Recall          0.748214  0.023279\n",
       "Roc_auc         0.752117  0.018448"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model2=Model_results(clf,X,y,Cv_model)\n",
    "Model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a8261ab4-67a6-445c-8d21-feddad6b3796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Method</th>\n",
       "      <th colspan=\"4\" halign=\"left\">XGB</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Model 1</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Model 2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Values</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.662476</td>\n",
       "      <td>0.016918</td>\n",
       "      <td>0.682000</td>\n",
       "      <td>0.015467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.978164</td>\n",
       "      <td>0.001539</td>\n",
       "      <td>0.978164</td>\n",
       "      <td>0.001539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.693483</td>\n",
       "      <td>0.017356</td>\n",
       "      <td>0.710276</td>\n",
       "      <td>0.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.671509</td>\n",
       "      <td>0.015551</td>\n",
       "      <td>0.693874</td>\n",
       "      <td>0.013624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.737143</td>\n",
       "      <td>0.025112</td>\n",
       "      <td>0.748214</td>\n",
       "      <td>0.023279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.749660</td>\n",
       "      <td>0.017686</td>\n",
       "      <td>0.752117</td>\n",
       "      <td>0.018448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Method               XGB                              \n",
       "Model            Model 1             Model 2          \n",
       "Values              Mean        Se      Mean        Se\n",
       "Accuracy_test   0.662476  0.016918  0.682000  0.015467\n",
       "Accuracy_train  0.978164  0.001539  0.978164  0.001539\n",
       "F1 Score        0.693483  0.017356  0.710276  0.015000\n",
       "Precision       0.671509  0.015551  0.693874  0.013624\n",
       "Recall          0.737143  0.025112  0.748214  0.023279\n",
       "Roc_auc         0.749660  0.017686  0.752117  0.018448"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saving the data of model performance\n",
    "Model_data=pd.concat([Model1,Model2],axis=1)\n",
    "Model_data.columns = [['XGB']*4,['Model 1','Model 1', 'Model 2','Model 2'], ['Mean', 'Se', 'Mean', 'Se']]  \n",
    "Model_data.columns.names=['Method','Model','Values']\n",
    "Model_data.to_csv(\"./Results/XGB_model_mlrem_data.csv\",sep=',')\n",
    "Model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec50adff-12d7-4284-a740-a33a46351d03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydrogel",
   "language": "python",
   "name": "hydrogel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
